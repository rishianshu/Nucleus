OpenAI Codex v0.71.0 (research preview)
--------
workdir: /Users/rishikeshkumar/Development/Nucleus
model: gpt-5.1-codex-max
provider: openai
approval: never
sandbox: workspace-write [workdir, /tmp, $TMPDIR]
reasoning effort: xhigh
reasoning summaries: auto
session id: 019b4639-98a9-7971-b897-5cc7c9d6455c
--------
user
You are AGENT_CODEX as defined in docs/meta/AGENT_CODEX.md.

Feature slug: code-vector-profile-and-indexing-v1

Follow AGENT_CODEX.md exactly:

1. Boot:
   - Use this slug.
   - Verify intents/code-vector-profile-and-indexing-v1/* exists.
   - Ensure runs/code-vector-profile-and-indexing-v1/* exists or create missing run files.
   - Append start/resume heartbeat to LOG.md.
   - Set sync/STATE.md Focus Feature status accordingly.

2. Then follow runs/code-vector-profile-and-indexing-v1/RUNCARD.md for:
   - RESUME protocol
   - Loop
   - Guardrails
   - Tasks
   - Done criteria

3. Treat:
   - INTENT.md, SPEC.md, ACCEPTANCE.md as the definition of WHAT to implement.
   - PLAN.md, LOG.md, TODO.md, QUESTIONS.md, DECISIONS.md as current run state.
   - TODO.md may contain small and large items:
       • small items (≤~15 min, no contract change) → execute
       • large items → log proposal in QUESTIONS.md and STOP

4. Do not ask me questions unless AGENT_CODEX.md or RUNCARD.md are missing or inconsistent.

Begin immediately. Remain in execution mode: only respond when you have recorded work (e.g., heartbeat updates, summaries of completed steps, blockers, or final results). Do not pause to re-confirm instructions unless a required artifact is missing or conflicting. Keep coding straight through until the slug is finished or truly blocked.
mcp startup: no servers

thinking
**Preparing environment and verifying setup**

Planning to verify essential paths and files exist, create missing run files, log start status, and confirm feature state per agent guidelines before proceeding.
exec
/bin/zsh -lc ls in /Users/rishikeshkumar/Development/Nucleus succeeded in 103ms:
AGENT_INSTRUCTIONS.md
Makefile
README.md
apps
codex_output.log
docs
epics
github.com
infra
intents
mypy.ini
node_modules
nohup.out
package.json
packages
platform
playwright.config.ts
pnpm-lock.yaml
pnpm-workspace.yaml
prisma

thinking
**Locating AGENT_CODEX instructions**

I'm planning to find AGENT_CODEX instructions, likely in docs/meta/AGENT_CODEX.md, and considering how to view the file efficiently next.
exec
/bin/zsh -lc "sed -n '1,200p' docs/meta/AGENT_CODEX.md" in /Users/rishikeshkumar/Development/Nucleus succeeded in 109ms:
# AGENT_CODEX — Execution Agent Contract (v1)

## Purpose
This contract instructs the Developer Agent (Codex) how to pick work, read specs/state, implement, test, log, stop and resume.

## Directories
- intents/<slug>/ : INTENT.md, SPEC.md, ACCEPTANCE.md, ADR-*.md (ChatGPT-owned)
- runs/<slug>/    : PLAN.md, LOG.md, QUESTIONS.md, DECISIONS.md, TODO.md (Codex-owned)
- sync/STATE.md   : shared roll-up (Codex updates; ChatGPT reads)
- stories/<slug>/ : STORY.md timeline (both append)
- docs/meta/*     : schemas & governance (read-only for Codex)

## Boot
1) If invoked with slug, use it. Else read sync/STATE.md Focus Feature.
2) Verify intents/<slug>/* exists; else set blocked + QUESTIONS.md.
3) Ensure runs/<slug>/* files exist; append start heartbeat. Set STATE to in-progress.
4) Once the run begins, stay in execution mode; only surface progress via LOG heartbeats, blocker notes, or final summaries. Do not pause for validation unless a required artifact is missing/conflicting


## Loop
exec
/bin/zsh -lc "sed -n '200,400p' docs/meta/AGENT_CODEX.md" in /Users/rishikeshkumar/Development/Nucleus succeeded in 120ms:


thinking
**Verifying intents directory**

I'm checking whether the specific intents path exists by listing the directory contents for "intents/code-vector-profile-and-indexing-v1/*" before proceeding to read run card instructions.
exec
/bin/zsh -lc 'ls intents/code-vector-profile-and-indexing-v1' in /Users/rishikeshkumar/Development/Nucleus succeeded in 114ms:
ACCEPTANCE.md
INTENT.md
SPEC.md

thinking
**Verifying runs path existence**

I'm confirming that the runs directory and its slug subdirectory exist as part of boot step 3 setup.
exec
/bin/zsh -lc 'ls runs/code-vector-profile-and-indexing-v1' in /Users/rishikeshkumar/Development/Nucleus succeeded in 92ms:
DECISIONS.md
LOG.md
PLAN.md
QUESTIONS.md
RUNCARD.md
START_PROMPT.md
TODO.md

thinking
**Preparing to append heartbeat and check RUNCARD instructions**

I confirmed run files exist and plan to append start/resume heartbeat to LOG.md, then set sync/STATE Focus Feature status accordingly. Next, I'll read the RUNCARD with sed to confirm how to handle Boot versus Resume protocols before proceeding.
exec
/bin/zsh -lc "sed -n '1,200p' runs/code-vector-profile-and-indexing-v1/RUNCARD.md" in /Users/rishikeshkumar/Development/Nucleus succeeded in 121ms:
# Run Card — code-vector-profile-and-indexing-v1

ROLE: Developer Agent (follow docs/meta/AGENT_CODEX.md)

SLUG: code-vector-profile-and-indexing-v1

SCOPE: Implement only what’s required to satisfy `intents/code-vector-profile-and-indexing-v1/ACCEPTANCE.md`. No extra features.

INPUTS:
- intents/code-vector-profile-and-indexing-v1/INTENT.md
- intents/code-vector-profile-and-indexing-v1/SPEC.md
- intents/code-vector-profile-and-indexing-v1/ACCEPTANCE.md
- docs/meta/*
- runs/code-vector-profile-and-indexing-v1/* (PLAN.md, LOG.md, QUESTIONS.md, DECISIONS.md, TODO.md)

OUTPUTS:
- runs/code-vector-profile-and-indexing-v1/PLAN.md
- runs/code-vector-profile-and-indexing-v1/LOG.md (heartbeat every 40–45 minutes)
- runs/code-vector-profile-and-indexing-v1/QUESTIONS.md
- runs/code-vector-profile-and-indexing-v1/DECISIONS.md

thinking
**Reviewing RESUME protocol instructions**

I'm focusing on understanding the RESUME protocol by reading the PLAN and INTENT files to clarify the required sync/state steps and RUNCARD task handling.
exec
/bin/zsh -lc "sed -n '1,200p' intents/code-vector-profile-and-indexing-v1/INTENT.md" in /Users/rishikeshkumar/Development/Nucleus succeeded in 91ms:
title: Code Vector Profile and Indexing v1
slug: code-vector-profile-and-indexing-v1
type: feature
context: >
  UCL (Go) now provides GitHub code ingestion that emits index-ready records
  (raw.code.file_chunk) and MinIO provides staging + sink artifacts. Brain/Indexing
  needs a reliable, source-independent indexing contract so “docs from Confluence”
  and “docs from OneDrive” and “code from GitHub” can be searched uniformly.
  We use Postgres + pgvector as the embedding store.

why_now: >
  GitHub ingestion is only valuable if Workspace/Brain can retrieve code. We need
  a code vector profile, canonical metadata keys, and an index-run pipeline that
  can ingest raw.code.file_chunk artifacts into pgvector deterministically.

scope_in:
  - Define a canonical vector metadata schema (source-independent keys) and apply it to code chunks:
    - tenantId (implicit/required)
    - projectKey (normalized; for GitHub = owner/repo)
    - profileKind ("code")
exec
/bin/zsh -lc "sed -n '1,200p' intents/code-vector-profile-and-indexing-v1/SPEC.md" in /Users/rishikeshkumar/Development/Nucleus succeeded in 123ms:
# SPEC — Code Vector Profile and Indexing v1

## Problem

We have code chunks (raw.code.file_chunk) being produced and persisted (MinIO sink),
but Brain cannot search code until those chunks are embedded and stored in pgvector
with a consistent, source-independent metadata schema.

We must:
- define canonical vector metadata keys,
- define a “code” index profile,
- implement an index-run pipeline reading MinIO sink artifacts,
- store embeddings in pgvector with idempotent upserts.

## Interfaces / Contracts

### Canonical Vector Metadata Schema

Every indexed row MUST include:

exec
/bin/zsh -lc "sed -n '1,200p' intents/code-vector-profile-and-indexing-v1/ACCEPTANCE.md" in /Users/rishikeshkumar/Development/Nucleus succeeded in 143ms:
# Acceptance Criteria

1) A code vector profile exists and enforces canonical metadata keys  
   - Type: unit  
   - Evidence:
     - Add tests that map a raw.code.file_chunk envelope to:
       - profileKind="code"
       - projectKey normalized to "{owner}/{repo}"
       - entityKind="code.file_chunk"
       - sourceFamily="github"
     - Verify missing tenantId/projectKey/profileKind rejects the record.

2) Index-run ingests raw.code.file_chunk artifacts from MinIO sink and upserts into pgvector idempotently  
   - Type: integration  
   - Evidence:
     - Seed MinIO sink dataset with JSONL.GZ containing chunks for 2 files * 3 chunks each.
     - Run StartIndexRun(profileId="code.github.v1", sourceSelector=dataset prefix).
     - Assert:
       - vector_documents row count == 6 after first run
       - running the same index run again does not increase count (upsert/dedupe)

thinking
**Clarifying resume sequence**

The resume protocol requires reading the PLAN, last 40 lines of LOG, and TODO before appending the start/resume heartbeat; next, I'll read these files in that order to understand prior state for accurate heartbeat logging.
exec
/bin/zsh -lc 'tail -n 120 runs/code-vector-profile-and-indexing-v1/LOG.md' in /Users/rishikeshkumar/Development/Nucleus succeeded in 127ms:
2025-12-13T19:53:14Z | start | done: booted code-vector-profile-and-indexing-v1 run, confirmed intent/spec/acceptance files exist | next: read INTENT/SPEC/ACCEPTANCE plus PLAN/TODO/LOG tail for resume context | risks: none yet
2025-12-13T20:33:59Z | heartbeat | done: added vector_documents schema/store and seeded code.github.v1 profile; built code index runner with ingestionRunId filtering, deterministic embeddings, and MinIO JSONL.GZ normalization; added AC1–AC4 tests and reran pnpm --filter @apps/metadata-api test:brain (fails: METADATA_DATABASE_URL missing) | next: apply the new migration, configure METADATA_DATABASE_URL/pgvector Postgres, rerun brain tests, and validate index-run output against acceptance | risks: Postgres/pgvector env absent and migrations not yet applied; test suite blocked until METADATA_DATABASE_URL is set
2025-12-14T05:07:40Z | heartbeat | done: consolidated code/doc/work onto vector_index_entries (doc_id + metadata), removed vector_documents; added in-memory profile/vector stores for offline tests; fixed code index runner to use upsertEntries and reran pnpm --filter @apps/metadata-api test:brain (all green with deterministic embeddings) | next: apply migration 20251213195500_code_vector_documents to pgvector DB, hook production embedding provider, and run db-backed validation when infra is available | risks: prod DB not yet migrated; embeddings still use fake provider in tests only
2025-12-14T05:27:43Z | heartbeat | done: ran prisma migrate deploy with METADATA_DATABASE_URL (schema metadata @ localhost:5434) and applied 20251213195500_code_vector_documents | next: validate code indexer against real MinIO + production embeddings, then update TODO/state once prod DBs are migrated | risks: none locally; prod envs still need migration + real embedding provider
2025-12-14T06:17:45Z | heartbeat | done: ingested public repo (octocat/Hello-World) via real GitHub connector into MinIO localhost:9000 (ucl-staging) and ran CodeIndexRunner against sink raw.code.file_chunk using deterministic embeddings; vectors landed in metadata.vector_index_entries (docId/code profile) | next: rerun indexing with production embedding provider when available; keep MinIO artifacts for reuse | risks: current vectors use deterministic embeddings, not production model
2025-12-14T07:02:10Z | heartbeat | done: added materialized_artifacts registry + Prisma registry client; CodeIndexRunner now records run status (IN_PROGRESS/SUCCEEDED/FAILED) into registry (default no-op), fixed pg upsert placeholder, migrations applied and test:brain still green | next: wire ingestion sink completion to create registry entries and run indexer with production embeddings | risks: registry not yet fed by ingestion pipeline; embeddings still deterministic in local runs
2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.

thinking
**Planning heartbeat resume**

I'm preparing to append a start/resume heartbeat with the current timestamp, following the resume protocol by reading PLAN.md next after having reviewed the LOG tail.
exec
/bin/zsh -lc "sed -n '1,200p' runs/code-vector-profile-and-indexing-v1/PLAN.md" in /Users/rishikeshkumar/Development/Nucleus succeeded in 91ms:
# Plan
- Align code, doc, and work vectors on the shared `vector_index_entries` schema (docId + source metadata) and seed `code.github.v1`.
- Provide deterministic/in-memory embedding + vector stores for offline tests; keep Prisma store using docId/chunkId uniqueness.
- Implement code index-run to read MinIO JSONL.GZ datasets (ingestionRunId filter), normalize, embed, and upsert via the vector store.
- Cover AC1–AC4 with tests (normalization, idempotent indexing/search filters, hardening) and run `pnpm --filter @apps/metadata-api test:brain`; validate against pgvector/MinIO once infra is available.
exec
/bin/zsh -lc "sed -n '1,200p' runs/code-vector-profile-and-indexing-v1/TODO.md" in /Users/rishikeshkumar/Development/Nucleus succeeded in 111ms:
- [x] Implement Endpoint-Driven Vector Normalization (VectorProfileProvider, GitHub connector, Staging integration).
- [x] Verify Staging (vectorPayload IS present in MinIO).
- [ ] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
- [ ] Rerun code indexing with the production embedding provider once access is available.
- [ ] Feed materialized_artifacts registry from sink completion events.
exec
/bin/zsh -lc "sed -n '1,200p' runs/code-vector-profile-and-indexing-v1/QUESTIONS.md" in /Users/rishikeshkumar/Development/Nucleus succeeded in 137ms:

exec
/bin/zsh -lc "sed -n '1,200p' runs/code-vector-profile-and-indexing-v1/DECISIONS.md" in /Users/rishikeshkumar/Development/Nucleus succeeded in 113ms:
- Extend the existing `vector_index_entries` table to carry doc_id, entity_kind, source_family, source_url/external_id, observed_at, content_text, raw_payload, and source_pointer (doc_id + profile_id unique), keeping existing node_id/profile_id/chunk_id uniqueness so graph profiles remain unchanged.
- Canonical code profile `code.github.v1` normalizes projectKey to lower-case `{owner}/{repo}`, entityKind=`code.file_chunk`, profileKind=`code`, sourceFamily=`github`, docId=`code:github:{tenantId}:{projectKey}:{path}:{sha}:{chunkIndex}`. Content comes from `payload.text` truncated to 20k chars; attributes capture path/sha/chunkIndex/language/truncated flag. Missing tenantId/projectKey/profileKind fail closed with E_MISSING_CANONICAL_KEYS. Profile is also seeded into `vector_index_profiles` with text_source field hints.
- Deterministic embeddings use a fake provider that hashes text to a one-hot 1536-dim vector (no external calls) for tests and default offline runs.
- Index runs read MinIO-style JSONL.GZ envelopes from a dataset prefix (supports `minio://bucket/prefix`) via filesystem, streaming/decompressing lines; optional `ingestionRunId` filters to `run=<id>` segments (basePrefix defaults to `sink`). Errors classify as E_SOURCE_READ_FAILED, E_INVALID_RECORD, E_MISSING_CANONICAL_KEYS, E_EMBEDDING_FAILED, or E_VECTOR_DB_FAILED. Oversized text is truncated; malformed/missing-key records are skipped and surfaced as errors, with run status set to SUCCEEDED_WITH_ERRORS when any non-fatal errors occur.

thinking
**Planning to update sync state**

I need to read the current sync state to update the Focus feature status during boot.
exec
/bin/zsh -lc "sed -n '1,200p' sync/STATE.md" in /Users/rishikeshkumar/Development/Nucleus succeeded in 219ms:
# STATE SYNC (auto-updated)

## Focus Feature
materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)

## Last Run
- slug: materialized-registry-and-index-trigger-v1
- status: success
- duration: ~0.9h
- tests: pnpm --filter @apps/metadata-api test:brain (green)
- commits: none
- decisions: 2
- next_step: apply the materialized_artifacts migration/client to environments and extend profile mappings as new source families adopt the registry

## Global Queue
TODAY:
- 
NEXT:
- 
LATER:

thinking
**Appending start/resume heartbeat and updating state**

I'm preparing to append a start/resume heartbeat entry with the current UTC timestamp to the LOG.md and update the sync/STATE Focus Feature status to in-progress as per boot instructions before continuing main operations.
exec
/bin/zsh -lc 'date -u +"%Y-%m-%dT%H:%M:%SZ"' in /Users/rishikeshkumar/Development/Nucleus succeeded in 121ms:
2025-12-22T13:23:29Z

thinking
**Evaluating log timestamp consistency**

The last log entry timestamp is later than the current system time, risking chronological inconsistency; I'll consider using a timestamp just after the last entry to maintain monotonic order without faking time.

thinking
**Deciding timestamp and log append method**

Choosing to use current UTC time from `date -u` for consistency despite possible chronological quirks, and planning to append the new log line with a concise summary using shell echo or printf to the log file.
exec
/bin/zsh -lc "printf '%s | resume | done: resumed code-vector-profile-and-indexing-v1 run; reviewed INTENT/SPEC/ACCEPTANCE, PLAN, TODO, LOG tail | next: investigate IndexArtifact pipeline/vector_entries empty issue; inspect code + logs and run targeted brain tests | risks: db persistence path or long-running checkpoint failures could block indexing\\n' \"2025-12-22T13:23:29Z\" >> runs/code-vector-profile-and-indexing-v1/LOG.md" in /Users/rishikeshkumar/Development/Nucleus succeeded in 99ms:


thinking
**Updating Focus Feature to in-progress**

I'm changing the Focus Feature in sync/STATE to the current slug with status "in-progress" and a timestamp to reflect run start, while leaving Last Run unchanged for now until post-run updates.

thinking
**Planning state update on run start**

I'll update the Focus Feature line in the sync/STATE file to the new slug with an in-progress status and current UTC timestamp, leaving Last Run unchanged until post-run. I'll apply this patch early using ASCII format, ensuring consistent timestamp formatting without seconds.
file update
M /Users/rishikeshkumar/Development/Nucleus/sync/STATE.md
@@ -3,3 +3,3 @@
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
apply_patch(auto_approved=true) exited 0 in 65ms:
Success. Updated the following files:
M sync/STATE.md
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1


thinking
**Locating IndexArtifact implementation**

Preparing to search the codebase for "IndexArtifact" to find where it's defined, focusing on likely packages like indexing or metadata to understand the issue with vector_entries and checkpoint saving.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'rg "IndexArtifact"' in /Users/rishikeshkumar/Development/Nucleus succeeded in 356ms:
codex_output.log:2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
codex_output.log:- [ ] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
codex_output.log:/bin/zsh -lc "printf '%s | resume | done: resumed code-vector-profile-and-indexing-v1 run; reviewed INTENT/SPEC/ACCEPTANCE, PLAN, TODO, LOG tail | next: investigate IndexArtifact pipeline/vector_entries empty issue; inspect code + logs and run targeted brain tests | risks: db persistence path or long-running checkpoint failures could block indexing\\n' \"2025-12-22T13:23:29Z\" >> runs/code-vector-profile-and-indexing-v1/LOG.md" in /Users/rishikeshkumar/Development/Nucleus succeeded in 99ms:
codex_output.log:**Locating IndexArtifact implementation**
codex_output.log:Preparing to search the codebase for "IndexArtifact" to find where it's defined, focusing on likely packages like indexing or metadata to understand the issue with vector_entries and checkpoint saving.
codex_output.log:/bin/zsh -lc 'rg "IndexArtifact"' in /Users/rishikeshkumar/Development/Nucleus
scripts/start-brain-worker.sh:# Starts the brain worker (IndexArtifact / ExtractSignals / ExtractInsights / BuildClusters).
runs/code-vector-profile-and-indexing-v1/LOG.md:2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
runs/code-vector-profile-and-indexing-v1/LOG.md:2025-12-22T13:23:29Z | resume | done: resumed code-vector-profile-and-indexing-v1 run; reviewed INTENT/SPEC/ACCEPTANCE, PLAN, TODO, LOG tail | next: investigate IndexArtifact pipeline/vector_entries empty issue; inspect code + logs and run targeted brain tests | risks: db persistence path or long-running checkpoint failures could block indexing
runs/code-vector-profile-and-indexing-v1/TODO.md:- [ ] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
intents/force-minio-staging-v1/ACCEPTANCE.md:    - Downstream `IndexArtifact` activity succeeds (because it finds the object in MinIO).
README.md:| brain worker | Temporal activities: IndexArtifact, ExtractSignals, ExtractInsights, BuildClusters | `bash scripts/start-brain-worker.sh` | stop via `scripts/stop-go-stack.sh` | `DEBUG=1 ...` (port 40002) | `/tmp/nucleus/brain_worker.log` |
README.md:  - IndexArtifact (vector indexing via store-core vector gRPC)
apps/metadata-api/README.md:- Brain worker on queue `brain-go` runs: IndexArtifact, ExtractSignals, ExtractInsights, BuildClusters (requires store-core gRPC at 9099).
apps/metadata-api/src/temporal/workflows.ts:  IndexArtifact(input: {
apps/metadata-api/src/temporal/workflows.ts:    const indexResult = await goBrainActivities.IndexArtifact({
platform/brain-core/README.md:- Temporal activities: IndexArtifact, ExtractSignals, ExtractInsights, BuildClusters. Consumes vector/signal/logstore gRPC (store-core).
platform/brain-core/cmd/brain-worker/main.go:	w.RegisterActivity(acts.IndexArtifact)
platform/brain-core/cmd/brain-worker/main.go:	log.Printf("Registered brain activities: IndexArtifact, ExtractSignals, ExtractInsights, BuildClusters")
platform/brain-core/internal/activities/signal_engine.go:func (e *signalEngine) eval(rec map[string]any, req IndexArtifactRequest, defID string) []*signalpb.Instance {

thinking
**Planning investigation steps**

I'm preparing to create a multi-step plan to debug the empty vector entries and checkpoint save failures by inspecting brain-core internals, starting with relevant type definitions to understand context.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,200p' platform/brain-core/internal/activities/types.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 361ms:
// Package activities provides Temporal activity implementations for UCL connectors.
package activities

// CollectionJobRequest matches Python ingestion_models.requests.CollectionJobRequest
type CollectionJobRequest struct {
	RunID         string         `json:"runId"`
	EndpointID    string         `json:"endpointId"`
	SourceID      string         `json:"sourceId"`
	EndpointName  string         `json:"endpointName"`
	ConnectionURL string         `json:"connectionUrl"`
	Schemas       []string       `json:"schemas,omitempty"`
	ProjectID     string         `json:"projectId,omitempty"`
	Labels        []string       `json:"labels,omitempty"`
	Config        map[string]any `json:"config,omitempty"`
}

// CollectionResult matches Python CollectionJobResult
type CollectionResult struct {
	Records     []CatalogRecord `json:"records,omitempty"`
	RecordsPath string          `json:"recordsPath,omitempty"`

thinking
**Opening key activities file**

I'm opening the main activities implementation to understand how IndexArtifact is handled and get context on the new code.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,240p' platform/brain-core/internal/activities/activities.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 221ms:
// Package activities implements Temporal activities for UCL connectors.
package activities

import (
	"context"
	"crypto/sha1"
	"encoding/json"
	"fmt"
	"os"
	"strconv"
	"strings"
	"time"

	"go.temporal.io/sdk/activity"

	"github.com/nucleus/brain-core/internal/bridge"
	"github.com/nucleus/ucl-core/pkg/cdm"
	"github.com/nucleus/ucl-core/pkg/endpoint"
	"github.com/nucleus/ucl-core/pkg/orchestration"
	"github.com/nucleus/ucl-core/pkg/staging"

thinking
**Locating target function by line number**

I'm searching for the definition of the target function near the file's end using line-numbered search to isolate the relevant code section efficiently.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'rg -n "IndexArtifact" platform/brain-core/internal/activities/activities.go' in /Users/rishikeshkumar/Development/Nucleus succeeded in 116ms:
864:// IndexArtifact streams records from a sink endpoint used as a source and returns counters + checkpoint.
865:func (a *Activities) IndexArtifact(ctx context.Context, req IndexArtifactRequest) (*IndexArtifactResult, error) {
1167:	return &IndexArtifactResult{

thinking
**Continuing code review**

I'm extending the code view to start around line 840 to gather more context for analysis.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '820,1185p' platform/brain-core/internal/activities/activities.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 100ms:
	cdmModelID string,
	candidate endpoint.SourceEndpoint,
) *endpoint.Schema {
	// Prefer CDM-defined schema when provided.
	if schema := schemaFromCDM(cdmModelID); schema != nil {
		return schema
	}

	if candidate != nil {
		if schema, err := candidate.GetSchema(ctx, unitID); err == nil && schema != nil && len(schema.Fields) > 0 {
			return schema
		}
	}

	src, err := bridge.GetSourceEndpoint(templateID, params)
	if err != nil {
		return nil
	}
	defer src.Close()


thinking
**Searching for vector store functions**

I'm searching for key functions related to vectorstore like getVectorStore and getEmbeddingProvider to understand vector entry handling and upsert behavior.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'rg -n "getVectorStore" -g"*.go" platform/brain-core/internal/activities' in /Users/rishikeshkumar/Development/Nucleus succeeded in 130ms:
platform/brain-core/internal/activities/activities.go:1114:		client, err := getVectorStore()
platform/brain-core/internal/activities/vector_store_client.go:31:func getVectorStore() (*vectorClient, error) {
platform/brain-core/internal/activities/clustering.go:87:	client, err := getVectorStore()

thinking
**Opening vector store client file**
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,200p' platform/brain-core/internal/activities/vector_store_client.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 245ms:
package activities

import (
	"context"
	"fmt"
	"sync"
	"time"

	vectorpb "github.com/nucleus/store-core/gen/go/vectorpb"
	"github.com/nucleus/ucl-core/pkg/vectorstore"
	"go.temporal.io/sdk/activity"
	"google.golang.org/grpc"
	"google.golang.org/protobuf/types/known/structpb"
)

// vectorClient prefers the store-core gRPC service for writes and falls back to
// a local pgvector store for read-heavy operations (ListEntries) until the
// VectorService exposes a list API.
type vectorClient struct {
	grpcAddr string

thinking
**Analyzing checkpoint save failure**

Logs show "checkpoint-save-failed context deadline exceeded," indicating a timeout saving checkpoints, likely in a different data store than the vector store. Upsert to the vector store (via gRPC) may still be working, but the empty vector_entries table suggests otherwise. Next, I'll check definitions of checkpoint save functions and inspect environment details to clarify the failure point.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'rg -n "saveCheckpointKV" -g"*.go" platform/brain-core/internal/activities' in /Users/rishikeshkumar/Development/Nucleus succeeded in 396ms:
platform/brain-core/internal/activities/kv_client.go:65:func saveCheckpointKV(ctx context.Context, tenantID, projectID, key string, checkpoint map[string]any) error {
platform/brain-core/internal/activities/insight_kv.go:39:	_ = saveCheckpointKV(ctx, tenantID, projectID, makeInsightKey(skillID, entityRef), map[string]any{
platform/brain-core/internal/activities/activities.go:1146:	if err := saveCheckpointKV(ctx, tenantID, projectID, key, newCheckpoint); err != nil {
platform/brain-core/internal/activities/clustering.go:429:	saveCheckpointKV(ctx, tenant, project, fmt.Sprintf("cluster:%s", req.DatasetSlug), map[string]any{

thinking
**Opening key source file**
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,200p' platform/brain-core/internal/activities/kv_client.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 89ms:
package activities

import (
	"context"
	"encoding/json"
	"fmt"
	"os"
	"sync"
	"time"

	"google.golang.org/grpc"

	kvpb "github.com/nucleus/ucl-core/gen/go/proto/github.com/nucleus/ucl-core/pkg/kvpb"
)

var (
	kvOnce   sync.Once
	kvClient kvpb.KVServiceClient
	kvErr    error
)

thinking
**Analyzing vector store upsert and checkpoint failure**

The "context deadline exceeded" error likely means the KV checkpoint save failed due to an unreachable service, causing checkpoint save errors after a successful Upsert. Since IndexArtifact completes yet vector_entries is empty, the Upsert may be writing to an unexpected or ephemeral store, potentially due to store-core gRPC service misconfiguration or absence. Next steps involve inspecting the store-core vector service implementation, verifying gRPC target addresses, and environment defaults to confirm where vectors are written and why checkpoint saving fails.

thinking
**Investigating storage of vector entries**

I'm checking if vector entries are stored in the metadata Postgres database via the store-core service by searching the repo for "vector_entries" to identify the relevant table and confirm where data is stored.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'rg "vector_entries"' in /Users/rishikeshkumar/Development/Nucleus succeeded in 289ms:
platform/ucl-core/pkg/vectorstore/vectorstore.go:// CREATE TABLE IF NOT EXISTS vector_entries (
platform/ucl-core/pkg/vectorstore/vectorstore.go:// CREATE INDEX IF NOT EXISTS vector_entries_profile_idx ON vector_entries (tenant_id, project_id, profile_id);
platform/ucl-core/pkg/vectorstore/vectorstore.go:// CREATE INDEX IF NOT EXISTS vector_entries_artifact_idx ON vector_entries (tenant_id, artifact_id, run_id);
platform/ucl-core/pkg/vectorstore/vectorstore.go:// CREATE INDEX IF NOT EXISTS vector_entries_meta_idx ON vector_entries USING gin (metadata);
platform/ucl-core/pkg/vectorstore/vectorstore.go:// CREATE INDEX IF NOT EXISTS vector_entries_embedding_idx ON vector_entries USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);
runs/code-vector-profile-and-indexing-v1/LOG.md:2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
runs/code-vector-profile-and-indexing-v1/LOG.md:2025-12-22T13:23:29Z | resume | done: resumed code-vector-profile-and-indexing-v1 run; reviewed INTENT/SPEC/ACCEPTANCE, PLAN, TODO, LOG tail | next: investigate IndexArtifact pipeline/vector_entries empty issue; inspect code + logs and run targeted brain tests | risks: db persistence path or long-running checkpoint failures could block indexing
runs/code-vector-profile-and-indexing-v1/TODO.md:- [ ] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
codex_output.log:2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
codex_output.log:- [ ] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
codex_output.log:/bin/zsh -lc "printf '%s | resume | done: resumed code-vector-profile-and-indexing-v1 run; reviewed INTENT/SPEC/ACCEPTANCE, PLAN, TODO, LOG tail | next: investigate IndexArtifact pipeline/vector_entries empty issue; inspect code + logs and run targeted brain tests | risks: db persistence path or long-running checkpoint failures could block indexing\\n' \"2025-12-22T13:23:29Z\" >> runs/code-vector-profile-and-indexing-v1/LOG.md" in /Users/rishikeshkumar/Development/Nucleus succeeded in 99ms:
codex_output.log:Preparing to search the codebase for "IndexArtifact" to find where it's defined, focusing on likely packages like indexing or metadata to understand the issue with vector_entries and checkpoint saving.
codex_output.log:codex_output.log:2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
codex_output.log:codex_output.log:- [ ] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
codex_output.log:codex_output.log:/bin/zsh -lc "printf '%s | resume | done: resumed code-vector-profile-and-indexing-v1 run; reviewed INTENT/SPEC/ACCEPTANCE, PLAN, TODO, LOG tail | next: investigate IndexArtifact pipeline/vector_entries empty issue; inspect code + logs and run targeted brain tests | risks: db persistence path or long-running checkpoint failures could block indexing\\n' \"2025-12-22T13:23:29Z\" >> runs/code-vector-profile-and-indexing-v1/LOG.md" in /Users/rishikeshkumar/Development/Nucleus succeeded in 99ms:
codex_output.log:codex_output.log:Preparing to search the codebase for "IndexArtifact" to find where it's defined, focusing on likely packages like indexing or metadata to understand the issue with vector_entries and checkpoint saving.
codex_output.log:runs/code-vector-profile-and-indexing-v1/LOG.md:2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
codex_output.log:runs/code-vector-profile-and-indexing-v1/LOG.md:2025-12-22T13:23:29Z | resume | done: resumed code-vector-profile-and-indexing-v1 run; reviewed INTENT/SPEC/ACCEPTANCE, PLAN, TODO, LOG tail | next: investigate IndexArtifact pipeline/vector_entries empty issue; inspect code + logs and run targeted brain tests | risks: db persistence path or long-running checkpoint failures could block indexing
codex_output.log:runs/code-vector-profile-and-indexing-v1/TODO.md:- [ ] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
codex_output.log:Logs show "checkpoint-save-failed context deadline exceeded," indicating a timeout saving checkpoints, likely in a different data store than the vector store. Upsert to the vector store (via gRPC) may still be working, but the empty vector_entries table suggests otherwise. Next, I'll check definitions of checkpoint save functions and inspect environment details to clarify the failure point.

thinking
**Identifying relevant vectorstore files**

The search showed matching results in both platform/ucl-core and platform/store-core, so I'll focus next on inspecting the core vectorstore implementation files to understand their interfaces and logic, starting with vectorstore.go in ucl-core.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,200p' platform/ucl-core/pkg/vectorstore/vectorstore.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 104ms:
package vectorstore

import "time"

// NOTE: This package defines the canonical vector entry contract and a store interface.
// The actual implementation (pgvector or otherwise) will live in a separate module.
// Table DDL reference (pgvector):
//
// CREATE TABLE IF NOT EXISTS vector_entries (
//   tenant_id        text NOT NULL,
//   project_id       text NOT NULL,
//   profile_id       text NOT NULL,
//   node_id          text NOT NULL,
//   source_family    text,
//   artifact_id      text,
//   run_id           text,
//   sink_endpoint_id text,
//   dataset_slug     text,
//   entity_kind      text,
//   labels           text[],

thinking
**Inspecting vector store implementation**
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,240p' platform/store-core/pkg/vectorstore/pgvector_store.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 119ms:
package vectorstore

import (
	"database/sql"
	"encoding/json"
	"errors"
	"fmt"
	"strconv"
	"strings"
	"time"

	"github.com/lib/pq"
)

// PgVectorStore implements Store backed by Postgres + pgvector.
type PgVectorStore struct {
	db        *sql.DB
	dimension int
}


thinking
**Identifying vector service gRPC usage**

I'm checking if the vector service gRPC uses the store by searching for `UpsertEntries` in the store-core internal files, specifically looking into `platform/store-core/internal/vector/server.go`.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'rg -n "UpsertEntries" platform/store-core' in /Users/rishikeshkumar/Development/Nucleus succeeded in 120ms:
platform/store-core/pkg/vectorstore/service.go:22:func (s *GRPCServer) UpsertEntries(ctx context.Context, req *vectorpb.UpsertEntriesRequest) (*vectorpb.UpsertEntriesResponse, error) {
platform/store-core/pkg/vectorstore/service.go:24:		return &vectorpb.UpsertEntriesResponse{Upserted: 0}, nil
platform/store-core/pkg/vectorstore/service.go:30:	if err := s.store.UpsertEntries(entries); err != nil {
platform/store-core/pkg/vectorstore/service.go:33:	return &vectorpb.UpsertEntriesResponse{Upserted: int32(len(entries))}, nil
platform/store-core/pkg/vectorstore/vectorstore.go:47:	UpsertEntries(entries []Entry) error
platform/store-core/pkg/vectorstore/pgvector_store.go:87:// UpsertEntries inserts or updates entries with embeddings.
platform/store-core/pkg/vectorstore/pgvector_store.go:88:func (s *PgVectorStore) UpsertEntries(entries []Entry) error {
platform/store-core/gen/go/vectorpb/vector_grpc.pb.go:22:	VectorService_UpsertEntries_FullMethodName    = "/vector.VectorService/UpsertEntries"
platform/store-core/gen/go/vectorpb/vector_grpc.pb.go:32:	UpsertEntries(ctx context.Context, in *UpsertEntriesRequest, opts ...grpc.CallOption) (*UpsertEntriesResponse, error)
platform/store-core/gen/go/vectorpb/vector_grpc.pb.go:46:func (c *vectorServiceClient) UpsertEntries(ctx context.Context, in *UpsertEntriesRequest, opts ...grpc.CallOption) (*UpsertEntriesResponse, error) {
platform/store-core/gen/go/vectorpb/vector_grpc.pb.go:48:	out := new(UpsertEntriesResponse)
platform/store-core/gen/go/vectorpb/vector_grpc.pb.go:49:	err := c.cc.Invoke(ctx, VectorService_UpsertEntries_FullMethodName, in, out, cOpts...)
platform/store-core/gen/go/vectorpb/vector_grpc.pb.go:90:	UpsertEntries(context.Context, *UpsertEntriesRequest) (*UpsertEntriesResponse, error)
platform/store-core/gen/go/vectorpb/vector_grpc.pb.go:104:func (UnimplementedVectorServiceServer) UpsertEntries(context.Context, *UpsertEntriesRequest) (*UpsertEntriesResponse, error) {
platform/store-core/gen/go/vectorpb/vector_grpc.pb.go:105:	return nil, status.Error(codes.Unimplemented, "method UpsertEntries not implemented")
platform/store-core/gen/go/vectorpb/vector_grpc.pb.go:137:func _VectorService_UpsertEntries_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
platform/store-core/gen/go/vectorpb/vector_grpc.pb.go:138:	in := new(UpsertEntriesRequest)
platform/store-core/gen/go/vectorpb/vector_grpc.pb.go:143:		return srv.(VectorServiceServer).UpsertEntries(ctx, in)
platform/store-core/gen/go/vectorpb/vector_grpc.pb.go:147:		FullMethod: VectorService_UpsertEntries_FullMethodName,
platform/store-core/gen/go/vectorpb/vector_grpc.pb.go:150:		return srv.(VectorServiceServer).UpsertEntries(ctx, req.(*UpsertEntriesRequest))

thinking
**Planning service code review**

I'm checking the service implementation in `platform/store-core/pkg/vectorstore/service.go` to understand its structure and functionality for the next step.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,200p' platform/store-core/pkg/vectorstore/service.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 108ms:
package vectorstore

import (
	"context"

	vectorpb "github.com/nucleus/store-core/gen/go/vectorpb"
	"google.golang.org/grpc/codes"
	"google.golang.org/grpc/status"
	"google.golang.org/protobuf/types/known/structpb"
)

// GRPCServer implements vectorpb.VectorServiceServer backed by a Store.
type GRPCServer struct {
	vectorpb.UnimplementedVectorServiceServer
	store Store
}

func NewGRPCServer(store Store) *GRPCServer {
	return &GRPCServer{store: store}
}

thinking
**Locating vector service entry**

I'm checking the platform/store-core/cmd directory to find the main vector server file and learn about its configuration and environment setup.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'ls platform/store-core/cmd' in /Users/rishikeshkumar/Development/Nucleus succeeded in 655ms:
store-server

thinking
**Inspecting main server file**

I'm checking the main server file to understand the core startup and command logic essential for the implementation.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,200p' platform/store-core/cmd/store-server/main.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 503ms:
package main

import (
	"context"
	"fmt"
	"log"
	"net"
	"os"
	"strconv"
	"strings"

	"google.golang.org/grpc"
	"google.golang.org/grpc/codes"
	"google.golang.org/grpc/health"
	healthpb "google.golang.org/grpc/health/grpc_health_v1"
	"google.golang.org/grpc/status"

	"github.com/nucleus/store-core/gen/go/kvpb"
	"github.com/nucleus/store-core/gen/go/logpb"
	"github.com/nucleus/store-core/gen/go/signalpb"

thinking
**Analyzing vector service registration risk**

The vector store service might be missing from the store-core server if VECTOR_DATABASE_URL isn't set, causing gRPC Upsert calls to fail silently and leaving vector entries empty despite successful IndexArtifact runs. Next, I'll investigate environment configs and startup scripts like `start-brain-worker.sh` to confirm how the vector store service is initialized and which DB it targets.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,200p' scripts/start-brain-worker.sh" in /Users/rishikeshkumar/Development/Nucleus succeeded in 175ms:
#!/usr/bin/env bash
set -euo pipefail

# Starts the brain worker (IndexArtifact / ExtractSignals / ExtractInsights / BuildClusters).
# DEBUG=1 runs under Delve on port 40002.

ROOT_DIR="$(git rev-parse --show-toplevel)"
LOG_DIR="${NUCLEUS_LOG_DIR:-/tmp/nucleus}"
PID_DIR="$ROOT_DIR/.nucleus/pids"
mkdir -p "$LOG_DIR" "$PID_DIR"

if [[ -f "$ROOT_DIR/.env" ]]; then
  # shellcheck disable=SC1090
  source "$ROOT_DIR/.env"
fi
# Ensure Go bin (for dlv) is on PATH
PATH="$PATH:$(go env GOPATH 2>/dev/null)/bin"
export PATH

LOG_FILE="$LOG_DIR/brain_worker.log"

thinking
**Verifying store-core server setup**

I'm checking the scripts related to `store-server` to confirm if the vector database and other services are started separately from the vector grpc setup on localhost:9099.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'ls scripts | grep store' in /Users/rishikeshkumar/Development/Nucleus succeeded in 122ms:
start-store-core.sh
stop-store-core.sh

thinking
**Reviewing env setup in start-store-core script**
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,200p' scripts/start-store-core.sh" in /Users/rishikeshkumar/Development/Nucleus succeeded in 110ms:
#!/usr/bin/env bash
set -euo pipefail

# Starts the store-core gRPC server (kv, vector, signal, logstore).
# Uses environment from .env and supports DEBUG=1 to run under Delve on port 40001.

ROOT_DIR="$(git rev-parse --show-toplevel)"
LOG_DIR="${NUCLEUS_LOG_DIR:-/tmp/nucleus}"
PID_DIR="$ROOT_DIR/.nucleus/pids"
mkdir -p "$LOG_DIR" "$PID_DIR"

# Load env
if [[ -f "$ROOT_DIR/.env" ]]; then
  # shellcheck disable=SC1090
  source "$ROOT_DIR/.env"
fi
# Ensure Go bin (for dlv) is on PATH
PATH="$PATH:$(go env GOPATH 2>/dev/null)/bin"
export PATH


thinking
**Analyzing environment dependency and embedding provider**

I’ve identified that store-core must be running with environment variables set for the brain worker to upsert successfully, and the empty vector_entries issue likely relates to missing store-core or DB in staging. Next, I’ll inspect the embedding provider setup and the normalizeVectorRecord function to see if recent pipeline changes affect data writing.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'rg -n "getEmbeddingProvider" platform/brain-core/internal/activities' in /Users/rishikeshkumar/Development/Nucleus succeeded in 120ms:
platform/brain-core/internal/activities/activities.go:1097:		embedder, err := getEmbeddingProvider()
platform/brain-core/internal/activities/embedding.go:43:func getEmbeddingProvider() (EmbeddingProvider, error) {
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,200p' platform/brain-core/internal/activities/embedding.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 101ms:
package activities

import (
	"bytes"
	"encoding/json"
	"errors"
	"hash/fnv"
	"net/http"
	"os"
	"strconv"
	"strings"
	"sync"
	"time"
)

// EmbeddingProvider defines the minimal embed API.
type EmbeddingProvider interface {
	EmbedText(model string, texts []string) ([][]float32, error)
}


thinking
**Analyzing embedding and vector store interactions**

The embedding provider defaults to a zero vector but UpsertEntries still inserts entries; Upsert errors if the vector store is unreachable via gRPC, which connects asynchronously so Dial itself doesn't error immediately. The vector store DB may differ from the metadata DB checked, possibly explaining empty vector_entries despite Upsert success. Next, I’ll verify how index results are handled in the workflow to clarify what “completes” means in this context.

thinking
**Analyzing vector DB timeout and indexing issues**

Observed that store-core defaults VECTOR_DATABASE_URL to METADATA_DATABASE_URL, so both may share the same DB, but staging might use distinct DBs causing vector_entries table emptiness. Noted the 'checkpoint-save-failed context deadline exceeded' error indicating KV gRPC save timeout in store-core, possibly causing upsert failures. Need to explore metadata-api workflows and index-run code to understand full data flow and fix indexing pipeline for test acceptance.

thinking
**Identifying environment mismatch causing checkpoint save failure**

The checkpoint save fails due to a context deadline exceeded error likely because the KV gRPC client tries connecting to the wrong default port (50051) while the store-core server listens on 9099; this suggests a misconfiguration in environment variables pointing the KV client to the wrong gRPC address. Next, I'll confirm environment vars and consider adjusting client fallback or setup to fix the empty vector_entries bug.

thinking
**Identifying default port mismatch causing checkpoint save failure**

The checkpoint save tries connecting to the kv service on outdated default port 50051, but store-core uses 9099, causing timeouts; aligning defaults or setting env vars to 9099 should fix this. The vector entries remain empty possibly due to misconfiguration or store-core not running properly, warranting further log inspection.

thinking
**Analyzing vector normalization as vector_entries cause**

I'm investigating if empty vector_entries result from entries failing normalization due to missing or malformed vectorPayload keys; next, I'll examine the normalizeVectorRecord function to understand its expected input and output.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'rg -n "normalizeVectorRecord" platform/brain-core/internal/activities' in /Users/rishikeshkumar/Development/Nucleus succeeded in 120ms:
platform/brain-core/internal/activities/activities.go:1065:			entry, content, ok = normalizeVectorRecord(rec, profileID, tenantID, projectID, req.DatasetSlug, req.SinkEndpointID)
platform/brain-core/internal/activities/normalize.go:11:// normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
platform/brain-core/internal/activities/normalize.go:13:func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,200p' platform/brain-core/internal/activities/normalize.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 100ms:
package activities

import (
	"fmt"
	"strings"

	"github.com/nucleus/ucl-core/pkg/vectorprofile"
	"github.com/nucleus/ucl-core/pkg/vectorstore"
)

// normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
// Returns (entry, content, ok).
func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
	n := vectorprofile.Resolve(profileID)
	entry, content, ok := n.Normalize(rec)
	if !ok {
		return vectorstore.Entry{}, "", false
	}
	// Imprint normalized fields that normalizers might not set.
	out := vectorstore.Entry{

thinking
**Exploring vector profile definitions**

I'm checking the vectorprofile directory to understand how normalization uses vector profiles and to investigate the new profile like "code.github.v1".
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'ls platform/ucl-core/pkg/vectorprofile' in /Users/rishikeshkumar/Development/Nucleus succeeded in 94ms:
profile.go
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,240p' platform/ucl-core/pkg/vectorprofile/profile.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 95ms:
package vectorprofile

import (
	"encoding/json"
	"fmt"
	"sync"

	"github.com/nucleus/ucl-core/pkg/vectorstore"
)

// Normalizer transforms a raw record (map) into a vector entry and content text.
type Normalizer interface {
	Normalize(record map[string]any) (vectorstore.Entry, string, bool)
}

// Simple registry for normalizers keyed by profileId.
var (
	registryMu sync.RWMutex
	registry   = map[string]Normalizer{}
)

thinking
**Locating profile registrations**

Discovering that profile definitions or normalizers might be outside the current directory, I'm searching the entire repo for `Register(` to find where profiles or related entities are registered.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'rg -n "vectorprofile.Register"' in /Users/rishikeshkumar/Development/Nucleus succeeded in 200ms:
codex_output.log:1280:/bin/zsh -lc 'rg -n "vectorprofile.Register"' in /Users/rishikeshkumar/Development/Nucleus
platform/ucl-core/internal/connector/confluence/vector_profiles.go:12:	vectorprofile.Register("source.confluence.pages.v1", &pageNormalizer{})
platform/ucl-core/internal/connector/github/vector_profiles.go:12:	vectorprofile.Register("source.github.code.v1", &codeNormalizer{})
platform/ucl-core/internal/connector/github/vector_profiles.go:13:	vectorprofile.Register("source.github.issues.v1", &issueNormalizer{})
platform/ucl-core/internal/connector/jira/vector_profiles.go:12:	vectorprofile.Register("source.jira.issues.v1", &issueNormalizer{})
platform/ucl-core/internal/core/cdm/vector_profiles.go:12:	vectorprofile.Register("cdm.work.item.v1", &workItemNormalizer{})
platform/ucl-core/internal/core/cdm/vector_profiles.go:13:	vectorprofile.Register("cdm.doc.item.v1", &docItemNormalizer{})
platform/ucl-core/internal/connector/onedrive/vector_profiles.go:12:	vectorprofile.Register("source.onedrive.docs.v1", &docNormalizer{})

thinking
**Checking GitHub vector profiles**

I'm going to inspect the GitHub vector profiles, possibly under a new profile like "code.github.v1," found in the connector code at platform/ucl-core/internal/connector/github/vector_profiles.go.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,240p' platform/ucl-core/internal/connector/github/vector_profiles.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 92ms:
package github

import (
	"fmt"
	"strings"

	"github.com/nucleus/ucl-core/pkg/vectorprofile"
	"github.com/nucleus/ucl-core/pkg/vectorstore"
)

func init() {
	vectorprofile.Register("source.github.code.v1", &codeNormalizer{})
	vectorprofile.Register("source.github.issues.v1", &issueNormalizer{})
}

// codeNormalizer expects payload fields: repo, path, sha, chunkIndex, text.
type codeNormalizer struct{}

func (n *codeNormalizer) Normalize(rec map[string]any) (vectorstore.Entry, string, bool) {
	payload, _ := rec["payload"].(map[string]any)

thinking
**Checking for code profile variants**

I'm searching for occurrences of "code.github.v1" to verify if it's defined differently or updated elsewhere compared to "source.github.code.v1".
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'rg "code.github.v1"' in /Users/rishikeshkumar/Development/Nucleus succeeded in 138ms:
codex_output.log:     - Run StartIndexRun(profileId="code.github.v1", sourceSelector=dataset prefix).
codex_output.log:2025-12-13T20:33:59Z | heartbeat | done: added vector_documents schema/store and seeded code.github.v1 profile; built code index runner with ingestionRunId filtering, deterministic embeddings, and MinIO JSONL.GZ normalization; added AC1–AC4 tests and reran pnpm --filter @apps/metadata-api test:brain (fails: METADATA_DATABASE_URL missing) | next: apply the new migration, configure METADATA_DATABASE_URL/pgvector Postgres, rerun brain tests, and validate index-run output against acceptance | risks: Postgres/pgvector env absent and migrations not yet applied; test suite blocked until METADATA_DATABASE_URL is set
codex_output.log:- Align code, doc, and work vectors on the shared `vector_index_entries` schema (docId + source metadata) and seed `code.github.v1`.
codex_output.log:- Canonical code profile `code.github.v1` normalizes projectKey to lower-case `{owner}/{repo}`, entityKind=`code.file_chunk`, profileKind=`code`, sourceFamily=`github`, docId=`code:github:{tenantId}:{projectKey}:{path}:{sha}:{chunkIndex}`. Content comes from `payload.text` truncated to 20k chars; attributes capture path/sha/chunkIndex/language/truncated flag. Missing tenantId/projectKey/profileKind fail closed with E_MISSING_CANONICAL_KEYS. Profile is also seeded into `vector_index_profiles` with text_source field hints.
codex_output.log:I'm checking the vectorprofile directory to understand how normalization uses vector profiles and to investigate the new profile like "code.github.v1".
codex_output.log:I'm going to inspect the GitHub vector profiles, possibly under a new profile like "code.github.v1," found in the connector code at platform/ucl-core/internal/connector/github/vector_profiles.go.
codex_output.log:I'm searching for occurrences of "code.github.v1" to verify if it's defined differently or updated elsewhere compared to "source.github.code.v1".
codex_output.log:/bin/zsh -lc 'rg "code.github.v1"' in /Users/rishikeshkumar/Development/Nucleus
runs/materialized-registry-and-index-trigger-v1/DECISIONS.md:* Indexing from a registry ID resolves the profile from artifact family/kind (defaults to `code.github.v1` for code/github artifacts); other families will need explicit profile mappings when added.
runs/code-vector-profile-and-indexing-v1/DECISIONS.md:- Canonical code profile `code.github.v1` normalizes projectKey to lower-case `{owner}/{repo}`, entityKind=`code.file_chunk`, profileKind=`code`, sourceFamily=`github`, docId=`code:github:{tenantId}:{projectKey}:{path}:{sha}:{chunkIndex}`. Content comes from `payload.text` truncated to 20k chars; attributes capture path/sha/chunkIndex/language/truncated flag. Missing tenantId/projectKey/profileKind fail closed with E_MISSING_CANONICAL_KEYS. Profile is also seeded into `vector_index_profiles` with text_source field hints.
runs/code-vector-profile-and-indexing-v1/PLAN.md:- Align code, doc, and work vectors on the shared `vector_index_entries` schema (docId + source metadata) and seed `code.github.v1`.
runs/code-vector-profile-and-indexing-v1/LOG.md:2025-12-13T20:33:59Z | heartbeat | done: added vector_documents schema/store and seeded code.github.v1 profile; built code index runner with ingestionRunId filtering, deterministic embeddings, and MinIO JSONL.GZ normalization; added AC1–AC4 tests and reran pnpm --filter @apps/metadata-api test:brain (fails: METADATA_DATABASE_URL missing) | next: apply the new migration, configure METADATA_DATABASE_URL/pgvector Postgres, rerun brain tests, and validate index-run output against acceptance | risks: Postgres/pgvector env absent and migrations not yet applied; test suite blocked until METADATA_DATABASE_URL is set
runs/code-vector-profile-and-indexing-v1/RUNCARD.md:2) Add code vector profile "code.github.v1" mapping raw.code.file_chunk → vector_index_entries rows with stable docId + source metadata.
stories/code-vector-profile-and-indexing-v1/STORY.md:- 2025-12-13: Extended pgvector-backed `vector_index_entries` with docId/source metadata and seeded `code.github.v1`, implemented the code index runner/normalizer with deterministic embeddings, MinIO JSONL.GZ reading, and ingestionRunId filtering, and wrote AC1–AC4 tests; brain test suite currently blocked locally by missing METADATA_DATABASE_URL/Postgres.
intents/code-vector-profile-and-indexing-v1/ACCEPTANCE.md:     - Run StartIndexRun(profileId="code.github.v1", sourceSelector=dataset prefix).
intents/code-vector-profile-and-indexing-v1/SPEC.md:ProfileId: "code.github.v1"

thinking
**Analyzing vector indexing and storage systems**

I'm comparing two vector entry tables—`vector_entries` (store-core) and `vector_index_entries` (metadata)—noting possible migration and discrepancies in logs referencing index artifacts and empty tables, indicating multiple vector index systems may coexist or have transitioned, so next I'll inspect relevant code and database schema files to clarify their usage and data flow.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'rg "vector_index_entries"' in /Users/rishikeshkumar/Development/Nucleus succeeded in 164ms:
codex_output.log:2025-12-14T05:07:40Z | heartbeat | done: consolidated code/doc/work onto vector_index_entries (doc_id + metadata), removed vector_documents; added in-memory profile/vector stores for offline tests; fixed code index runner to use upsertEntries and reran pnpm --filter @apps/metadata-api test:brain (all green with deterministic embeddings) | next: apply migration 20251213195500_code_vector_documents to pgvector DB, hook production embedding provider, and run db-backed validation when infra is available | risks: prod DB not yet migrated; embeddings still use fake provider in tests only
codex_output.log:2025-12-14T06:17:45Z | heartbeat | done: ingested public repo (octocat/Hello-World) via real GitHub connector into MinIO localhost:9000 (ucl-staging) and ran CodeIndexRunner against sink raw.code.file_chunk using deterministic embeddings; vectors landed in metadata.vector_index_entries (docId/code profile) | next: rerun indexing with production embedding provider when available; keep MinIO artifacts for reuse | risks: current vectors use deterministic embeddings, not production model
codex_output.log:- Align code, doc, and work vectors on the shared `vector_index_entries` schema (docId + source metadata) and seed `code.github.v1`.
codex_output.log:- Extend the existing `vector_index_entries` table to carry doc_id, entity_kind, source_family, source_url/external_id, observed_at, content_text, raw_payload, and source_pointer (doc_id + profile_id unique), keeping existing node_id/profile_id/chunk_id uniqueness so graph profiles remain unchanged.
codex_output.log:codex_output.log:- Align code, doc, and work vectors on the shared `vector_index_entries` schema (docId + source metadata) and seed `code.github.v1`.
codex_output.log:runs/code-vector-profile-and-indexing-v1/PLAN.md:- Align code, doc, and work vectors on the shared `vector_index_entries` schema (docId + source metadata) and seed `code.github.v1`.
codex_output.log:runs/code-vector-profile-and-indexing-v1/RUNCARD.md:2) Add code vector profile "code.github.v1" mapping raw.code.file_chunk → vector_index_entries rows with stable docId + source metadata.
codex_output.log:stories/code-vector-profile-and-indexing-v1/STORY.md:- 2025-12-13: Extended pgvector-backed `vector_index_entries` with docId/source metadata and seeded `code.github.v1`, implemented the code index runner/normalizer with deterministic embeddings, MinIO JSONL.GZ reading, and ingestionRunId filtering, and wrote AC1–AC4 tests; brain test suite currently blocked locally by missing METADATA_DATABASE_URL/Postgres.
codex_output.log:I'm comparing two vector entry tables—`vector_entries` (store-core) and `vector_index_entries` (metadata)—noting possible migration and discrepancies in logs referencing index artifacts and empty tables, indicating multiple vector index systems may coexist or have transitioned, so next I'll inspect relevant code and database schema files to clarify their usage and data flow.
codex_output.log:/bin/zsh -lc 'rg "vector_index_entries"' in /Users/rishikeshkumar/Development/Nucleus
sync/STATE.md:- 2025-12-14T06:17Z run in-progress (code-vector-profile-and-indexing-v1, real GitHub→MinIO ingestion and CodeIndexRunner wrote vectors to metadata.vector_index_entries using deterministic embeddings)
sync/STATE.md:- 2025-12-14T05:07Z run in-progress (code-vector-profile-and-indexing-v1, unified vector_index_entries docId schema + in-memory stores; test:brain green with deterministic embeddings)
stories/code-vector-profile-and-indexing-v1/STORY.md:- 2025-12-13: Extended pgvector-backed `vector_index_entries` with docId/source metadata and seeded `code.github.v1`, implemented the code index runner/normalizer with deterministic embeddings, MinIO JSONL.GZ reading, and ingestionRunId filtering, and wrote AC1–AC4 tests; brain test suite currently blocked locally by missing METADATA_DATABASE_URL/Postgres.
stories/code-vector-profile-and-indexing-v1/STORY.md:- 2025-12-14: Pivoted to a single vector store (removed vector_documents), added in-memory profile/vector stores for offline runs, fixed the code index runner API, reran `pnpm --filter @apps/metadata-api test:brain` successfully with deterministic embeddings, applied migration `20251213195500_code_vector_documents` to the local metadata pgvector instance, ran a real GitHub→MinIO ingestion (octocat/Hello-World) with CodeIndexRunner persisting vectors to `metadata.vector_index_entries` (deterministic embeddings), and added a `materialized_artifacts` registry + registry-aware CodeIndexRunner to track index runs (registry not yet fed by ingestion events).
README.md:  - Vectors: `SELECT COUNT(*) FROM metadata.vector_index_entries;`
intents/brain-search-graphrag-api-v1/ACCEPTANCE.md:     - Seed vector_index_entries for at least two nodeIds under one tenantId and
scripts/verify-workflow-lineage.sh:FROM vector_index_entries;
runs/code-vector-profile-and-indexing-v1/DECISIONS.md:- Extend the existing `vector_index_entries` table to carry doc_id, entity_kind, source_family, source_url/external_id, observed_at, content_text, raw_payload, and source_pointer (doc_id + profile_id unique), keeping existing node_id/profile_id/chunk_id uniqueness so graph profiles remain unchanged.
runs/code-vector-profile-and-indexing-v1/PLAN.md:- Align code, doc, and work vectors on the shared `vector_index_entries` schema (docId + source metadata) and seed `code.github.v1`.
runs/code-vector-profile-and-indexing-v1/LOG.md:2025-12-14T05:07:40Z | heartbeat | done: consolidated code/doc/work onto vector_index_entries (doc_id + metadata), removed vector_documents; added in-memory profile/vector stores for offline tests; fixed code index runner to use upsertEntries and reran pnpm --filter @apps/metadata-api test:brain (all green with deterministic embeddings) | next: apply migration 20251213195500_code_vector_documents to pgvector DB, hook production embedding provider, and run db-backed validation when infra is available | risks: prod DB not yet migrated; embeddings still use fake provider in tests only

thinking
**Clarifying vector systems and data flow**

There are two vector data systems—store-core’s vector_entries and metadata’s vector_index_entries—with some ambiguity on which the new pipeline targets. I’ll focus on inspecting metadata-api and vectorprofile code to understand their roles and check the referenced START_PROMPT.md for context on indexing and vector profile tasks.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,200p' runs/code-vector-profile-and-indexing-v1/START_PROMPT.md" in /Users/rishikeshkumar/Development/Nucleus succeeded in 92ms:
You are AGENT_CODEX as defined in docs/meta/AGENT_CODEX.md.

Feature slug: code-vector-profile-and-indexing-v1

Follow AGENT_CODEX.md exactly:

1. Boot:
   - Use this slug.
   - Verify intents/code-vector-profile-and-indexing-v1/* exists.
   - Ensure runs/code-vector-profile-and-indexing-v1/* exists or create missing run files.
   - Append start/resume heartbeat to LOG.md.
   - Set sync/STATE.md Focus Feature status accordingly.

2. Then follow runs/code-vector-profile-and-indexing-v1/RUNCARD.md for:
   - RESUME protocol
   - Loop
   - Guardrails
   - Tasks
   - Done criteria


thinking
**Inspecting metadata API workflows**

I'm reviewing how IndexArtifact is used and stored in the metadata API workflows to clarify the architecture and guide fixing the TODO issue.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,200p' apps/metadata-api/src/temporal/workflows.ts" in /Users/rishikeshkumar/Development/Nucleus succeeded in 90ms:
import { proxyActivities, workflowInfo, log, ApplicationFailure, startChild } from "@temporalio/workflow";
import type { CatalogRecordInput, CollectionJobRequest, MetadataActivities } from "./activities.js";

export const WORKFLOW_NAMES = {
  collectionRun: "collectionRunWorkflow",
  listEndpointTemplates: "listEndpointTemplatesWorkflow",
  buildEndpointConfig: "buildEndpointConfigWorkflow",
  testEndpointConnection: "testEndpointConnectionWorkflow",
  previewDataset: "previewDatasetWorkflow",
  ingestionRun: "ingestionRunWorkflow",
  postIngestion: "postIngestionWorkflow",
} as const;

const {
  createCollectionRun,
  markRunStarted,
  markRunCompleted,
  markRunSkipped,
  markRunFailed,
  prepareCollectionJob,

thinking
**Identifying vector indexing integration points**

Noted that Node workflow uses goBrainActivities IndexArtifact with ingestion parameters, where errors or empty vector entries could cause issues. Planning to explore vector payload production in connector code, especially around VectorProfileProvider, to verify the end-to-end vector indexing pipeline is functioning.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'rg -n "VectorProfileProvider"' in /Users/rishikeshkumar/Development/Nucleus succeeded in 193ms:
runs/code-vector-profile-and-indexing-v1/LOG.md:7:2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
runs/code-vector-profile-and-indexing-v1/TODO.md:1:- [x] Implement Endpoint-Driven Vector Normalization (VectorProfileProvider, GitHub connector, Staging integration).
platform/ucl-core/pkg/endpoint/vector_profile.go:10:// VectorProfileProvider is re-exported from internal for external consumers.
platform/ucl-core/pkg/endpoint/vector_profile.go:11:type VectorProfileProvider = internal.VectorProfileProvider
platform/ucl-core/pkg/staging/staging.go:73:	VectorPayload map[string]any `json:"vectorPayload,omitempty"`      // pre-normalized vector-ready record (if endpoint supports VectorProfileProvider)
codex_output.log:236:2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
codex_output.log:251:- [x] Implement Endpoint-Driven Vector Normalization (VectorProfileProvider, GitHub connector, Staging integration).
codex_output.log:370:codex_output.log:2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
codex_output.log:377:runs/code-vector-profile-and-indexing-v1/LOG.md:2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
codex_output.log:704:runs/code-vector-profile-and-indexing-v1/LOG.md:2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
codex_output.log:707:codex_output.log:2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
codex_output.log:711:codex_output.log:codex_output.log:2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
codex_output.log:715:codex_output.log:runs/code-vector-profile-and-indexing-v1/LOG.md:2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
codex_output.log:1499:Noted that Node workflow uses goBrainActivities IndexArtifact with ingestion parameters, where errors or empty vector entries could cause issues. Planning to explore vector payload production in connector code, especially around VectorProfileProvider, to verify the end-to-end vector indexing pipeline is functioning.
codex_output.log:1516:/bin/zsh -lc 'rg -n "VectorProfileProvider"' in /Users/rishikeshkumar/Development/Nucleus
platform/ucl-core/internal/connector/github/github.go:1481:// VectorProfileProvider Implementation
platform/ucl-core/internal/endpoint/vector_profile.go:19:// VectorProfileProvider is an optional interface that endpoints can implement
platform/ucl-core/internal/endpoint/vector_profile.go:23:type VectorProfileProvider interface {
platform/brain-core/internal/activities/activities.go:338:	vectorProvider, supportsVectorProfile := ep.(endpoint.VectorProfileProvider)
platform/brain-core/internal/activities/activities.go:501:		// If endpoint supports VectorProfileProvider, produce vector-ready envelope

thinking
**Inspecting GitHub connector payload handling**

I'm focusing on the section around line 1481 in the GitHub connector to understand how vectorPayload is produced and handled. I'll review lines 1420–1525 in the relevant file to clarify this part.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1420,1545p' platform/ucl-core/internal/connector/github/github.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 91ms:
	var chunks []string
	for start := 0; start < len(runes); {
		end := start + chunkBytes
		if end > len(runes) {
			end = len(runes)
		}
		chunks = append(chunks, string(runes[start:end]))
		if end == len(runes) {
			break
		}
		start = end - overlap
		if start < 0 {
			start = 0
		}
	}
	return chunks
}

func detectLanguage(path string) string {
	lower := strings.ToLower(path)
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1545,1670p' platform/ucl-core/internal/connector/github/github.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 88ms:
		TenantID:     asString(rec["_tenantId"]),
		ProjectKey:   repo,
		SourceURL:    asString(rec["_sourceUrl"]),
		ExternalID:   asString(rec["_externalId"]),
		Metadata: map[string]any{
			"repo":       repo,
			"path":       path,
			"sha":        sha,
			"chunkIndex": chunkIdx,
			"language":   asString(rec["language"]),
		},
	}, true
}

func (g *GitHub) normalizeIssue(rec endpoint.Record) (endpoint.VectorIndexRecord, bool) {
	// Handle nested payload structure from staging
	var payload map[string]any
	if p, ok := rec["payload"].(map[string]any); ok {
		if nested, ok := p["payload"].(map[string]any); ok {
			payload = nested // nested structure: payload.payload.*

thinking
**Inspecting vector profile definitions**

I'm noting that vector profile IDs use an older naming convention and plan to check the VectorIndexRecord and vector_profile struct definitions inside the internal endpoint package to better understand their structure.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,200p' platform/ucl-core/internal/endpoint/vector_profile.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 92ms:
package endpoint

// VectorIndexRecord represents a normalized record ready for vector indexing.
// This is the output format expected by brain-worker for embedding and storage.
type VectorIndexRecord struct {
	NodeID       string         `json:"nodeId"`       // Stable unique identifier
	ProfileID    string         `json:"profileId"`    // Vector profile ID (e.g., source.github.code.v1)
	EntityKind   string         `json:"entityKind"`   // Entity type (e.g., code.file_chunk, work.item)
	Text         string         `json:"text"`         // Content to embed
	SourceFamily string         `json:"sourceFamily"` // Source system (e.g., github, jira)
	TenantID     string         `json:"tenantId"`     // Tenant identifier
	ProjectKey   string         `json:"projectKey"`   // Project identifier (e.g., owner/repo)
	SourceURL    string         `json:"sourceUrl"`    // URL to source item
	ExternalID   string         `json:"externalId"`   // External system ID
	Metadata     map[string]any `json:"metadata"`     // Additional metadata
	RawPayload   map[string]any `json:"rawPayload"`   // Original payload for reference
}

// VectorProfileProvider is an optional interface that endpoints can implement
// to provide vector indexing support. When an endpoint implements this interface,

thinking
**Inspecting vector indexing and staging flow**

I'm verifying how connectors produce profile IDs and whether they convert to canonical forms in the Node pipeline. I'll examine the staging code to confirm that vector payloads and IDs flow correctly through the StageRecord structure.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,200p' platform/ucl-core/pkg/staging/staging.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 96ms:
package staging

import (
	"context"
	"fmt"
	"strings"
	"sync"

	"github.com/google/uuid"
)

const (
	ProviderMemory      = "memory"
	ProviderObjectStore = "object"
	ProviderMinIO       = "object.minio"

	// DefaultLargeRunThresholdBytes determines when object-store staging is required.
	DefaultLargeRunThresholdBytes int64 = 2 * 1024 * 1024 // ~2MB
	// DefaultMemoryCapBytes is the max bytes allowed for the in-memory provider.
	DefaultMemoryCapBytes int64 = 2 * 1024 * 1024

thinking
**Locating vectorPayload generation logic**

I'm tracing where vectorPayload is produced, focusing on ingestion or staging pipeline code, especially around the staging object store file, to confirm how MinIO connector fills it when using VectorProfileProvider.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'rg -n "VectorPayload" platform/ucl-core' in /Users/rishikeshkumar/Development/Nucleus succeeded in 110ms:
platform/ucl-core/pkg/staging/staging.go:73:	VectorPayload map[string]any `json:"vectorPayload,omitempty"`      // pre-normalized vector-ready record (if endpoint supports VectorProfileProvider)

thinking
**Searching entire repo for key variable**

I'm planning to search the whole repository for "vectorPayload" to find where else it might be set outside the ucl-core modules.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'rg "vectorPayload"' in /Users/rishikeshkumar/Development/Nucleus succeeded in 338ms:
codex_output.log:2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
codex_output.log:- [x] Verify Staging (vectorPayload IS present in MinIO).
codex_output.log:codex_output.log:2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
codex_output.log:runs/code-vector-profile-and-indexing-v1/LOG.md:2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
codex_output.log:runs/code-vector-profile-and-indexing-v1/LOG.md:2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
codex_output.log:codex_output.log:2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
codex_output.log:codex_output.log:codex_output.log:2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
codex_output.log:codex_output.log:runs/code-vector-profile-and-indexing-v1/LOG.md:2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
codex_output.log:I'm investigating if empty vector_entries result from entries failing normalization due to missing or malformed vectorPayload keys; next, I'll examine the normalizeVectorRecord function to understand its expected input and output.
codex_output.log:runs/code-vector-profile-and-indexing-v1/LOG.md:7:2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
codex_output.log:platform/ucl-core/pkg/staging/staging.go:73:	VectorPayload map[string]any `json:"vectorPayload,omitempty"`      // pre-normalized vector-ready record (if endpoint supports VectorProfileProvider)
codex_output.log:codex_output.log:236:2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
codex_output.log:codex_output.log:370:codex_output.log:2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
codex_output.log:codex_output.log:377:runs/code-vector-profile-and-indexing-v1/LOG.md:2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
codex_output.log:codex_output.log:704:runs/code-vector-profile-and-indexing-v1/LOG.md:2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
codex_output.log:codex_output.log:707:codex_output.log:2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
codex_output.log:codex_output.log:711:codex_output.log:codex_output.log:2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
codex_output.log:codex_output.log:715:codex_output.log:runs/code-vector-profile-and-indexing-v1/LOG.md:2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
codex_output.log:I'm focusing on the section around line 1481 in the GitHub connector to understand how vectorPayload is produced and handled. I'll review lines 1420–1525 in the relevant file to clarify this part.
codex_output.log:**Locating vectorPayload generation logic**

thinking
**Investigating vectorPayload creation**

Planning to examine lines 300–520 in activities.go to understand how vectorPayload is produced and if ingestion relates to sink runner interactions.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '300,520p' platform/brain-core/internal/activities/activities.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 87ms:
	}
	if planSchema != nil {
		planMetadata["schema"] = planSchema
	}
	if req.CDMModelID != "" {
		planMetadata["cdmModelId"] = req.CDMModelID
	}

	return &PlanResult{
		Slices:       slices,
		Strategy:     string(strategy),
		PlanMetadata: planMetadata,
	}, nil
}

// =============================================================================
// ACTIVITY 4: RunIngestionUnit
// =============================================================================

// RunIngestionUnit executes ingestion for a unit/slice.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '520,820p' platform/brain-core/internal/activities/activities.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 89ms:
		envelopes = append(envelopes, staging.RecordEnvelope{
			RecordKind: "raw",
			EntityKind: req.UnitID,
			Source: staging.SourceRef{
				EndpointID:   req.EndpointID,
				SourceFamily: templateID,
				SourceID:     req.UnitID,
			},
			Payload:       norm,
			VectorPayload: vectorPayload,
			ObservedAt:    time.Now().UTC().Format(time.RFC3339),
		})

		if len(envelopes) >= chunkSize {
			if err := flush(); err != nil {
				return nil, fmt.Errorf("failed to stage records chunk: %w", err)
			}
		}
	}


thinking
**Tracing vectorPayload storage path**

I'm confirming that vectorPayload is set on envelopes but missing from the final stored entries; next, I'll verify if the MinIO staging provider retains vectorPayload when saving to the sink by locating and inspecting the MinIO staging implementation.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'ls platform/ucl-core/pkg/staging' in /Users/rishikeshkumar/Development/Nucleus succeeded in 88ms:
compat.go
helpers.go
memory_provider.go
object_provider.go
staging.go

thinking
**Examining object provider implementation**

I'm opening `object_provider.go` to verify if it uses MinIO or another solution, aiming to understand the object storage handling.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,240p' platform/ucl-core/pkg/staging/object_provider.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 88ms:
package staging

import (
	"bytes"
	"compress/gzip"
	"context"
	"fmt"
	"io"
	"os"
	"path/filepath"
	"sort"
	"strings"
	"sync"
)

// ObjectStoreProvider stores batches on disk under a deterministic prefix to mimic an object store.
type ObjectStoreProvider struct {
	root     string
	compress bool
	mu       sync.Mutex

thinking
**Inspecting staging stream function**

I'll locate and review the `streamFromStaging` function to understand how it reads records in staging mode and whether it utilizes the staging provider.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'rg -n "streamFromStaging" platform/brain-core/internal/activities' in /Users/rishikeshkumar/Development/Nucleus succeeded in 111ms:
platform/brain-core/internal/activities/insights.go:29:		iter, closeFn, err = streamFromStaging(ctx, req.StagingProviderID, req.StageRef, "", req.DatasetSlug, req.Checkpoint, 0)
platform/brain-core/internal/activities/signals.go:31:		iter, closeFn, err = streamFromStaging(ctx, req.StagingProviderID, req.StageRef, "", req.DatasetSlug, req.Checkpoint, 0)
platform/brain-core/internal/activities/stream.go:42:// streamFromStaging replays staged envelopes directly from a staging provider using the shared registry.
platform/brain-core/internal/activities/stream.go:44:func streamFromStaging(ctx context.Context, stagingProviderID string, stageRef string, sliceID string, datasetID string, checkpoint map[string]any, limit int64) (endpoint.Iterator[endpoint.Record], func(), error) {
platform/brain-core/internal/activities/activities.go:976:		iter, closeFn, err = streamFromStaging(ctx, req.StagingProviderID, req.StageRef, "", req.DatasetSlug, cp, 0)
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,200p' platform/brain-core/internal/activities/stream.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 89ms:
package activities

import (
	"context"
	"fmt"

	"github.com/nucleus/ucl-core/pkg/endpoint"
	"github.com/nucleus/ucl-core/pkg/orchestration"
	"github.com/nucleus/ucl-core/pkg/staging"
)

// streamDataset opens a SourceEndpoint and reads a dataset with optional checkpoint/limit. Caller must Close().
func streamDataset(ctx context.Context, sinkEndpointID string, cfg map[string]any, datasetSlug string, checkpoint map[string]any, limit int64) (endpoint.Iterator[endpoint.Record], func(), error) {
	ep, err := endpoint.Create(sinkEndpointID, cfg)
	if err != nil {
		return nil, func() {}, fmt.Errorf("create endpoint: %w", err)
	}
	source, ok := ep.(endpoint.SourceEndpoint)
	if !ok {
		ep.Close()
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '200,400p' platform/brain-core/internal/activities/stream.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 88ms:
			continue
		}

		it.records = recs
		it.recordIdx = 0
		it.baseOffset = startOffset
	}
}

func (it *stagingIterator) Value() endpoint.Record { return it.current }

func (it *stagingIterator) Err() error { return it.err }

func (it *stagingIterator) Close() error { return nil }

thinking
**Clarifying vectorPayload absence in staging reads**

The staging iterator’s Value does not include vectorPayload, so indexing reads on staging always get nil for it. However, payload holds the normalized record data, which fallback normalization uses with vectorprofile, explaining how vector data may still be derived despite vectorPayload being missing.

thinking
**Analyzing payload nesting for normalization**

The top-level `payload` in the normalized record (`norm`) contains metadata keys, while the original raw record is nested inside `norm["payload"]`. However, the code normalizer expects `payload["repo"]` at the top level, which may cause failures unless the nesting is flattened or adjusted before normalization.

thinking
**Identifying payload nesting issue blocking normalization**

The nested payload structure in normalized records prevents the normalizer from finding top-level repo fields, causing zero vectors to be generated. To fix this, stagingIterator’s Value map should explicitly include the envelope’s vectorPayload alongside the payload to ensure proper normalization and vector extraction.

thinking
**Checking payload preservation in staging helpers**

I'm verifying whether the reading and encoding helpers preserve the VectorPayload field in RecordEnvelope to ensure correct handling when reading from the provider.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,200p' platform/ucl-core/pkg/staging/helpers.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 102ms:
package staging

import (
	"bytes"
	"compress/gzip"
	"encoding/json"
	"fmt"
	"io"
)

// cloneEnvelopes makes a shallow copy of envelope slice to avoid mutation.
func cloneEnvelopes(in []RecordEnvelope) []RecordEnvelope {
	out := make([]RecordEnvelope, len(in))
	copy(out, in)
	return out
}

// envelopeSizeBytes approximates payload size using JSONL encoding.
func envelopeSizeBytes(records []RecordEnvelope) (int64, error) {
	buf := &bytes.Buffer{}

thinking
**Identifying payload forwarding gap**

The main issue is the staging iterator fails to forward the VectorPayload field to the output map, blocking IndexArtifact from accessing it. Next, I'll verify how payload normalization expects this data and explore ingestion pipeline stages, focusing on references to StageRef and batchRefs in MinIO-related code and metadata-api activities like startVectorIndexing.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'rg -n "startVectorIndexing" apps/metadata-api/src/temporal/activities.ts' in /Users/rishikeshkumar/Development/Nucleus succeeded in 99ms:
156:  startVectorIndexing(input: {
943:  async startVectorIndexing(args) {
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '900,1040p' apps/metadata-api/src/temporal/activities.ts" in /Users/rishikeshkumar/Development/Nucleus succeeded in 95ms:
      (args.bucket && args.basePrefix && args.datasetSlug
        ? `minio://${args.bucket}/${trimSlashes([args.basePrefix, tenantId, args.datasetSlug].join("/"))}`
        : null);
    if (!handleUri) {
      throw new Error("datasetPrefix or bucket/basePrefix/datasetSlug is required to build registry handle");
    }
    let datasetRecord: Record<string, unknown> | null = null;
    if (args.datasetId) {
      try {
        const store = await getMetadataStore();
        datasetRecord = await store.getRecord(CATALOG_DATASET_DOMAIN, args.datasetId);
      } catch (error) {
        // eslint-disable-next-line no-console
        console.warn("[registry] failed to fetch dataset metadata", error);
      }
    }
    const { canonicalMeta, sourceMeta } = buildMaterializedMetadata({
      artifactKind,
      sourceFamily: args.sourceFamily ?? null,
      datasetId: args.datasetId ?? null,

thinking
**Checking IndexArtifact usage in workflow**

I'm searching the workflow file for where IndexArtifact is invoked and what input data it receives, focusing on understanding the role of stageRef and dataset in that context.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'rg -n "IndexArtifact" apps/metadata-api/src/temporal/workflows.ts' in /Users/rishikeshkumar/Development/Nucleus succeeded in 90ms:
113:  IndexArtifact(input: {
342:    const indexResult = await goBrainActivities.IndexArtifact({
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '300,380p' apps/metadata-api/src/temporal/workflows.ts" in /Users/rishikeshkumar/Development/Nucleus succeeded in 100ms:
        datasetSlug: args.datasetSlug ?? artifactKind,
        sinkEndpointId: args.sinkEndpointId,
        sinkEndpointConfig: args.sinkEndpointConfig ?? null,
        tenantId: args.tenantId ?? null,
        sourceFamily: args.sourceFamily ?? null,
        bucket: args.bucket ?? null,
        basePrefix: args.basePrefix ?? null,
        stageRef: args.stageRef ?? null,
        batchRefs: args.batchRefs ?? null,
        stagingProviderId: args.stagingProviderId ?? null,
      });
    }
  } catch (error) {
    log.warn("vector-indexing-trigger-failed", {
      datasetPrefix: registryPrefix,
      artifactKind,
      error: error instanceof Error ? error.message : String(error),
    });
  }
}

thinking
**Identifying missing project key usage**

The code builds the index entry using a projectID from the request or environment rather than the projectKey in vectorPayload, which may cause incorrect metadata but not empty vector entries. Next, I'll adjust IndexArtifact to use vectorPayload's projectKey for accurate canonical project metadata.

thinking
**Identifying projectID misuse causing empty vector entries**

The code falls back to a default projectID "global" when none is passed, which causes vector entries to have incorrect project metadata and may lead to normalization failure and zero embeddings inserted. The index reports success inaccurately by setting records indexed to total read despite no actual inserts.

thinking
**Analyzing staging iterator payload handling**

The staging iterator currently returns normalized payloads, which breaks fallback normalizers expecting raw payload data for vector profiles. Adjusting the iterator to include the original raw payload (e.g., inside `payload["payload"]`) or adding a separate `raw` field could restore compatibility while preserving metadata. Next, I'll verify the envelope payload structure to decide how best to include both normalized and raw data without breaking other consumers.

thinking
**Adjusting payload handling in staging iterator**

I'm refining the staging iterator to unwrap nested payloads when present, preserving both raw and normalized data to avoid breaking downstream consumers like ExtractSignals, which may rely on normalized payload structures. Next, I’ll review ExtractSignals and related files to confirm expectations on payload format.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,160p' platform/brain-core/internal/activities/signals.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 86ms:
package activities

import (
	"context"
	"crypto/sha1"
	"fmt"
	"strings"
	"time"

	signalpb "github.com/nucleus/store-core/gen/go/signalpb"
	"github.com/nucleus/ucl-core/pkg/endpoint"
	"github.com/nucleus/ucl-core/pkg/kgpb"
	"go.temporal.io/sdk/activity"
	"google.golang.org/grpc"
)

// ExtractSignals reads records from the sink endpoint and extracts signal candidates.
// Uses store-core SignalService for persistence.
func (a *Activities) ExtractSignals(ctx context.Context, req IndexArtifactRequest) error {
	logger := activity.GetLogger(ctx)

thinking
**Inspecting signal engine data expectations**

I'm verifying how the signal engine's evaluation functions access record fields to ensure adding the original record to Value won't break existing assumptions. I'll check the first 200 lines of the signal engine code to confirm expected data structure.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,200p' platform/brain-core/internal/activities/signal_engine.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 79ms:
package activities

import (
	"fmt"
	"strings"
	"time"

	signalpb "github.com/nucleus/store-core/gen/go/signalpb"
	"google.golang.org/protobuf/types/known/structpb"
)

// signalEngine runs code/DSL/insight-based signal definitions against records.
type signalEngine struct {
	defs     []*signalpb.Definition
	insights *insightClient
}

// newSignalEngine constructs an engine for applicable definitions.
func newSignalEngine(defs []*signalpb.Definition) *signalEngine {
	return &signalEngine{defs: defs, insights: newInsightClient()}

thinking
**Investigating signal shape normalization**

I'm verifying if `normalizeSignalShape` expects structured records similar to `normalizeRecord` output to understand its input requirements.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'rg -n "normalizeSignalShape" -n platform/brain-core/internal/activities' in /Users/rishikeshkumar/Development/Nucleus succeeded in 98ms:
platform/brain-core/internal/activities/signal_engine.go:94:	norm := normalizeSignalShape(rec)
platform/brain-core/internal/activities/signal_engine.go:136:	norm := normalizeSignalShape(rec)
platform/brain-core/internal/activities/signal_engine.go:167:	norm := normalizeSignalShape(rec)
platform/brain-core/internal/activities/signal_engine.go:258:func normalizeSignalShape(rec map[string]any) normalizedSignal {
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '240,340p' platform/brain-core/internal/activities/signal_engine.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 93ms:
			}
		default:
			return false
		}
	}
	return true
}

type normalizedSignal struct {
	AgeMs      float64
	Status     string
	ProjectID  string
	SpaceID    string
	ViewCount  float64
	Summary    string
	CdmModelID string
}

func normalizeSignalShape(rec map[string]any) normalizedSignal {
	created := toTime(rec["createdAt"])

thinking
**Identifying missing vectorPayload in staging iterator**

The staging iterator's Value map lacks vectorPayload, causing empty normalized entries for the vector index; I'll fix this by modifying the iterator to expose vectorPayload alongside the original payload for fallback.

thinking
**Evaluating payload and metadata inclusion**

I'm considering adding vectorPayload and potentially rawPayload or envelope fields to the stage provider's RecordEnvelope to better support normalizers and data mapping, while noting that switching payload from normalized to raw risks breaking signals relying on normalized fields; I'll need to verify how signals consume these fields before changing payload structure.

thinking
**Planning vectorPayload inclusion**

The staging iterator currently lacks vectorPayload in its Value map, causing the indexer to fallback to vector normalization that can fail. Adding vectorPayload to the iterator’s value will let the indexer build vector entries correctly without altering payload mapping. Next, I’ll confirm the iterator includes actual vectorPayload from the environment and ensure this change doesn’t break other logic.

thinking
**Confirming tenant/project fields handling**

The RecordEnvelope struct includes TenantID and ProjectKey, but RunIngestionUnit doesn't set them since IngestionRequest lacks those fields. This explains why staged records miss tenant/project info, whereas indexing uses them from elsewhere, so no immediate change is needed.

thinking
**Planning payload normalization fix**

I'm outlining changes to preserve vectorPayload in non-staging streamDataset paths and refining the stagingIterator to expose both raw and normalized payload forms. This aims to fix lost vectorPayload in staging and improve fallback normalization functions. Next, I'll verify actual payload usage across brain-core code to avoid breaking expectations.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "rg \"\\[\\\"payload\\\"\\]\" platform/brain-core -g\"*.go\"" in /Users/rishikeshkumar/Development/Nucleus succeeded in 104ms:
platform/brain-core/internal/activities/insights.go:		payload, _ := rec["payload"].(map[string]any)
platform/brain-core/internal/activities/insights.go:			if payload, ok := rec["payload"]; ok {
platform/brain-core/internal/activities/normalize.go:	if payload, ok := rec["payload"].(map[string]any); ok {

thinking
**Inspecting usage of payload in insights**
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,200p' platform/brain-core/internal/activities/insights.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 96ms:
package activities

import (
	"context"
	"crypto/sha1"
	"encoding/json"
	"fmt"
	"strings"
	"time"

	"github.com/nucleus/ucl-core/pkg/endpoint"
	"github.com/nucleus/ucl-core/pkg/kgpb"
	"go.temporal.io/sdk/activity"
)

// ExtractInsights runs an LLM-based summarization per record (stubbed unless INSIGHT_PROVIDER is set).
func (a *Activities) ExtractInsights(ctx context.Context, req IndexArtifactRequest) error {
	if req.SinkEndpointID == "" || req.DatasetSlug == "" {
		return fmt.Errorf("sinkEndpointId and datasetSlug are required")
	}

thinking
**Planning vectorPayload propagation and checkpoint fix**

The key next step is adding vectorPayload to stage output so indexing writes vector entries correctly, as the connectors generate vectorPayload when supported. Additionally, fixing the checkpoint-save-failed error by aligning the KV gRPC address with vector gRPC (possibly defaulting to port 9099) is needed to prevent save failures and harden stability.

thinking
**Updating KV client defaults and canonical metadata**

I'm planning to fix the KV client to default to store-core's 9099 port or fall back on vector-related env vars to avoid checkpoint errors, and simultaneously update indexing to derive canonical project keys and tenant IDs from vectorPayload when present for better metadata normalization.

thinking
**Investigating tenant ID presence in connector records**

I'm assessing whether connector-generated records like from GitHub reliably include the `_tenantId` field used to set tenant IDs in vector payloads, since missing this could cause DB upsert failures due to non-null constraints. Next, I'll search the GitHub connector code for `_tenantId` usage to verify if it's set.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'rg "_tenantId" platform/ucl-core/internal/connector/github' in /Users/rishikeshkumar/Development/Nucleus succeeded in 102ms:
platform/ucl-core/internal/connector/github/github.go:			"_tenantId":   tenant,
platform/ucl-core/internal/connector/github/github.go:				"_tenantId":   tenant,
platform/ucl-core/internal/connector/github/github.go:		"_tenantId":        tenant,
platform/ucl-core/internal/connector/github/github.go:		TenantID:     asString(rec["_tenantId"]),
platform/ucl-core/internal/connector/github/github.go:		TenantID:     asString(rec["_tenantId"]),
platform/ucl-core/internal/connector/github/github.go:		TenantID:     asString(rec["_tenantId"]),
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1180,1225p' platform/ucl-core/internal/connector/github/github.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 78ms:
	Owner         ownerInfo   `json:"owner"`
	UpdatedAt     string      `json:"updated_at"`
	Files         []treeEntry `json:"tree,omitempty"`
}

type ownerInfo struct {
	Login string `json:"login"`
}

type treeResponse struct {
	Tree      []treeEntry `json:"tree"`
	Truncated bool        `json:"truncated"`
}

type treeEntry struct {
	Path string `json:"path"`
	Type string `json:"type"`
	SHA  string `json:"sha"`
	Size int64  `json:"size"`
}

thinking
**Analyzing payload normalization issues**

Noticed that staging stores normalized envelopes with nested raw payloads, causing fallback normalizers to fail when expecting raw data; considering adjusting the normalization function to unwrap nested payloads for compatibility without breaking other components.

thinking
**Planning nested payload unwrapping**

Planning to modify the staging iterator to include vectorPayload in the record and update normalization to unwrap nested payloads by copying the record and replacing the payload field with the inner map, preserving outer fields needed by the vector profile normalizer.

thinking
**Planning fallback normalization and tests**

Considering attaching rawNormalized to support fallback normalizers for nested payloads, while prioritizing vectorPayload when present; aiming for minimal code changes focused on stagingIterator, normalization, and getKVClient defaults. Planning targeted tests to verify vectorPayload preservation and nested payload normalization, with intent to check existing Go and TypeScript tests in brain-core and metadata-api first.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'ls apps/metadata-api/tests/brain' in /Users/rishikeshkumar/Development/Nucleus exited 1 in 81ms:
ls: apps/metadata-api/tests/brain: No such file or directory

thinking
**Checking test script for brain tests**

I'm searching the package.json of the metadata-api app to verify if the `test:brain` script exists and corresponds to tests under the brain directory.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'rg "test:brain"' in /Users/rishikeshkumar/Development/Nucleus succeeded in 189ms:
codex_output.log:2025-12-13T20:33:59Z | heartbeat | done: added vector_documents schema/store and seeded code.github.v1 profile; built code index runner with ingestionRunId filtering, deterministic embeddings, and MinIO JSONL.GZ normalization; added AC1–AC4 tests and reran pnpm --filter @apps/metadata-api test:brain (fails: METADATA_DATABASE_URL missing) | next: apply the new migration, configure METADATA_DATABASE_URL/pgvector Postgres, rerun brain tests, and validate index-run output against acceptance | risks: Postgres/pgvector env absent and migrations not yet applied; test suite blocked until METADATA_DATABASE_URL is set
codex_output.log:2025-12-14T05:07:40Z | heartbeat | done: consolidated code/doc/work onto vector_index_entries (doc_id + metadata), removed vector_documents; added in-memory profile/vector stores for offline tests; fixed code index runner to use upsertEntries and reran pnpm --filter @apps/metadata-api test:brain (all green with deterministic embeddings) | next: apply migration 20251213195500_code_vector_documents to pgvector DB, hook production embedding provider, and run db-backed validation when infra is available | risks: prod DB not yet migrated; embeddings still use fake provider in tests only
codex_output.log:2025-12-14T07:02:10Z | heartbeat | done: added materialized_artifacts registry + Prisma registry client; CodeIndexRunner now records run status (IN_PROGRESS/SUCCEEDED/FAILED) into registry (default no-op), fixed pg upsert placeholder, migrations applied and test:brain still green | next: wire ingestion sink completion to create registry entries and run indexer with production embeddings | risks: registry not yet fed by ingestion pipeline; embeddings still deterministic in local runs
codex_output.log:- Cover AC1–AC4 with tests (normalization, idempotent indexing/search filters, hardening) and run `pnpm --filter @apps/metadata-api test:brain`; validate against pgvector/MinIO once infra is available.
codex_output.log:- tests: pnpm --filter @apps/metadata-api test:brain (green)
codex_output.log:codex_output.log:2025-12-13T20:33:59Z | heartbeat | done: added vector_documents schema/store and seeded code.github.v1 profile; built code index runner with ingestionRunId filtering, deterministic embeddings, and MinIO JSONL.GZ normalization; added AC1–AC4 tests and reran pnpm --filter @apps/metadata-api test:brain (fails: METADATA_DATABASE_URL missing) | next: apply the new migration, configure METADATA_DATABASE_URL/pgvector Postgres, rerun brain tests, and validate index-run output against acceptance | risks: Postgres/pgvector env absent and migrations not yet applied; test suite blocked until METADATA_DATABASE_URL is set
codex_output.log:runs/code-vector-profile-and-indexing-v1/LOG.md:2025-12-13T20:33:59Z | heartbeat | done: added vector_documents schema/store and seeded code.github.v1 profile; built code index runner with ingestionRunId filtering, deterministic embeddings, and MinIO JSONL.GZ normalization; added AC1–AC4 tests and reran pnpm --filter @apps/metadata-api test:brain (fails: METADATA_DATABASE_URL missing) | next: apply the new migration, configure METADATA_DATABASE_URL/pgvector Postgres, rerun brain tests, and validate index-run output against acceptance | risks: Postgres/pgvector env absent and migrations not yet applied; test suite blocked until METADATA_DATABASE_URL is set
codex_output.log:codex_output.log:2025-12-14T05:07:40Z | heartbeat | done: consolidated code/doc/work onto vector_index_entries (doc_id + metadata), removed vector_documents; added in-memory profile/vector stores for offline tests; fixed code index runner to use upsertEntries and reran pnpm --filter @apps/metadata-api test:brain (all green with deterministic embeddings) | next: apply migration 20251213195500_code_vector_documents to pgvector DB, hook production embedding provider, and run db-backed validation when infra is available | risks: prod DB not yet migrated; embeddings still use fake provider in tests only
codex_output.log:sync/STATE.md:- 2025-12-14T05:07Z run in-progress (code-vector-profile-and-indexing-v1, unified vector_index_entries docId schema + in-memory stores; test:brain green with deterministic embeddings)
codex_output.log:stories/code-vector-profile-and-indexing-v1/STORY.md:- 2025-12-14: Pivoted to a single vector store (removed vector_documents), added in-memory profile/vector stores for offline runs, fixed the code index runner API, reran `pnpm --filter @apps/metadata-api test:brain` successfully with deterministic embeddings, applied migration `20251213195500_code_vector_documents` to the local metadata pgvector instance, ran a real GitHub→MinIO ingestion (octocat/Hello-World) with CodeIndexRunner persisting vectors to `metadata.vector_index_entries` (deterministic embeddings), and added a `materialized_artifacts` registry + registry-aware CodeIndexRunner to track index runs (registry not yet fed by ingestion events).
codex_output.log:runs/code-vector-profile-and-indexing-v1/LOG.md:2025-12-14T05:07:40Z | heartbeat | done: consolidated code/doc/work onto vector_index_entries (doc_id + metadata), removed vector_documents; added in-memory profile/vector stores for offline tests; fixed code index runner to use upsertEntries and reran pnpm --filter @apps/metadata-api test:brain (all green with deterministic embeddings) | next: apply migration 20251213195500_code_vector_documents to pgvector DB, hook production embedding provider, and run db-backed validation when infra is available | risks: prod DB not yet migrated; embeddings still use fake provider in tests only
codex_output.log:I'm searching the package.json of the metadata-api app to verify if the `test:brain` script exists and corresponds to tests under the brain directory.
codex_output.log:/bin/zsh -lc 'rg "test:brain"' in /Users/rishikeshkumar/Development/Nucleus
sync/STATE.md:- tests: pnpm --filter @apps/metadata-api test:brain (green)
sync/STATE.md:- 2025-12-14T12:59Z run success (materialized-registry-and-index-trigger-v1, registry/index trigger v1 landed; test:brain green)
sync/STATE.md:- 2025-12-14T12:48Z run success (materialized-registry-and-index-trigger-v1, registry/index trigger v1 landed; test:brain green)
sync/STATE.md:- 2025-12-14T07:02Z run in-progress (code-vector-profile-and-indexing-v1, materialized_artifacts registry + registry-aware CodeIndexRunner; test:brain still green)
sync/STATE.md:- 2025-12-14T05:07Z run in-progress (code-vector-profile-and-indexing-v1, unified vector_index_entries docId schema + in-memory stores; test:brain green with deterministic embeddings)
sync/STATE.md:- 2025-12-13T20:33Z run in-progress (code-vector-profile-and-indexing-v1, vector_documents schema/indexer/tests added; test:brain blocked by missing METADATA_DATABASE_URL)
sync/STATE.md:- 2025-12-12T16:37Z run in-progress (brain-search-graphrag-api-v1, brainSearch schema/service + prompt pack + deterministic tests added; test:brain blocked by missing metadata Postgres at localhost:5434)

thinking
**Scoping tests for vector payload fix**

I'm exploring existing TypeScript tests in metadata-api and brain-core to confirm coverage for the vector payload bridging bug but see that the core issue lies in the Go worker stage. I'll add new Go unit tests for streamFromStaging in brain-core to verify the fix, likely in a new test file alongside limited existing Go tests.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'fd --hidden --type f "_test.go" platform/brain-core' in /Users/rishikeshkumar/Development/Nucleus exited 127 in 97ms:
zsh:1: command not found: fd

thinking
**Switching search tool to find**
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'find platform/brain-core -name "*_test.go"' in /Users/rishikeshkumar/Development/Nucleus succeeded in 89ms:
platform/brain-core/internal/activities/activities_test.go

thinking
**Reviewing existing test setup**
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,240p' platform/brain-core/internal/activities/activities_test.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 118ms:
// Package activities provides tests for Temporal activities.
package activities

import (
	"context"
	"testing"

	"github.com/nucleus/ucl-core/pkg/endpoint"
)

// =============================================================================
// MOCK TYPES
// =============================================================================

// mockSourceEndpoint implements endpoint.SourceEndpoint for testing.
type mockSourceEndpoint struct {
	datasets []*endpoint.Dataset
	schema   *endpoint.Schema
	records  []map[string]any
}

thinking
**Evaluating test file and planning new tests**

The existing test file is placeholder-like and lacks real behavior checks, so adding new unit tests focused on `stagingIterator` and `streamFromStaging` with VectorPayload makes sense to validate the bug fix. I'll next check how `DefaultStagingRegistry` is defined to properly set up providers in these tests.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'rg -n "DefaultStagingRegistry" platform/ucl-core/pkg/orchestration' in /Users/rishikeshkumar/Development/Nucleus succeeded in 126ms:
platform/ucl-core/pkg/orchestration/staging_registry.go:17:// DefaultStagingRegistry returns the shared staging registry (memory, object-store, minio).
platform/ucl-core/pkg/orchestration/staging_registry.go:18:func DefaultStagingRegistry() *staging.Registry {
platform/ucl-core/pkg/orchestration/sink_runner.go:148:	reg := DefaultStagingRegistry()
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,200p' platform/ucl-core/pkg/orchestration/staging_registry.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 91ms:
package orchestration

import (
	"log"
	"os"
	"sync"

	minioProvider "github.com/nucleus/ucl-core/internal/connector/minio"
	"github.com/nucleus/ucl-core/pkg/staging"
)

var (
	defaultStagingRegistry     *staging.Registry
	defaultStagingRegistryOnce sync.Once
)

// DefaultStagingRegistry returns the shared staging registry (memory, object-store, minio).
func DefaultStagingRegistry() *staging.Registry {
	defaultStagingRegistryOnce.Do(func() {
		defaultStagingRegistry = BuildStagingRegistry()

thinking
**Examining memory provider usage for tests**

I'm verifying that the memory provider respects StageRef and stores data by writing a batch to it, ensuring staged data can be reliably created for tests using the default global registry. I'll confirm this by checking the memory provider implementation to see how it handles StageRef in PutBatch.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,200p' platform/ucl-core/pkg/staging/memory_provider.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 99ms:
package staging

import (
	"context"
	"fmt"
	"sort"
	"sync"
)

type memoryStage struct {
	batches    map[string][]RecordEnvelope
	totalBytes int64
}

// MemoryProvider stores staged data in process memory with a strict byte cap.
type MemoryProvider struct {
	maxBytes int64

	mu     sync.Mutex
	stages map[string]*memoryStage

thinking
**Planning test to verify staged stream**

The test will create a default memory provider and staging reference via PutBatch, then call streamFromStaging using that stageRef to ensure it returns a record with the expected vectorPayload. I'll verify the provider instance consistency by creating the stage within the default registry so streamFromStaging accesses the same data.

thinking
**Planning detailed fix and test steps**

I’m preparing a structured plan for fixing vector payload propagation, updating normalization logic, adjusting KV client fallback, and adding targeted tests, since these tasks are nontrivial and interdependent.

thinking
**Planning task status updates**

Deciding to validate and update TODO/LOG/STATE after completing steps, likely using an update_plan tool with initial pending statuses.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

Plan update
Plan to unblock IndexArtifact by carrying vector payloads from staging into normalization/upsert path and fixing KV endpoint default timeouts, then validate with tests and update run state.
  • Propagate vectorPayload + raw payload through staging iterator and normalization so IndexArtifact can upsert vectors
  • Harden checkpoint KV client defaults to hit store-core on 9099
  • Add/execute targeted tests to cover staging vector payload flow; update run artifacts (TODO/STATE/LOG)

thinking
**Planning payload field additions**

I'm deciding to add both `rawPayload` (original envelope payload) and `vectorPayload` to the streaming record map while keeping `payload` potentially mapped, to preserve access to both raw and normalized data. I'll update the normalization step to utilize `rawPayload` as needed without breaking existing code depending on the mapped `payload`.

thinking
**Planning payload and rawPayload handling**

Deciding to keep both mapped `payload` and original `rawPayload` in records to ensure normalization has access to raw data when mapping replaces or flattens payload, avoiding loss of original fields. Next, I'll adjust normalization to unwrap nested payloads and consider `rawPayload` for fallback.

thinking
**Planning fallback normalization and payload handling**

I'm outlining a two-phase normalization fallback using rawPayload when the main payload fails, plus handling tenant/project defaults and adding vectorPayload and rawPayload to the stream iterator. Next, I'll implement these adjustments and add corresponding tests.

thinking
**Preserving original payload**

I'm planning to save the original payload into a separate variable before modifications so it can be included unchanged later, ensuring the map contains the raw envelope data alongside the transformed payload.
file update:
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

file update
M /Users/rishikeshkumar/Development/Nucleus/platform/brain-core/internal/activities/stream.go
@@ -135,2 +135,3 @@
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
@@ -154,2 +155,4 @@
 				"payload":      payload,
+				"rawPayload":   rawPayload,
+				"vectorPayload": env.VectorPayload,
 				"source":       env.Source,
apply_patch(auto_approved=true) exited 0 in 36ms:
Success. Updated the following files:
M platform/brain-core/internal/activities/stream.go
file update:
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..2da02919802dd16192966f966fd0d1473d78ac96
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -152,6 +153,8 @@
 				"recordKind":   recordKind,
 				"entityKind":   entityKind,
 				"payload":      payload,
+				"rawPayload":   rawPayload,
+				"vectorPayload": env.VectorPayload,
 				"source":       env.Source,
 				"tenantId":     env.TenantID,
 				"projectKey":   env.ProjectKey,
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1


thinking
**Planning nested payload unwrapping**

Planning to modify normalization by unwrapping nested payloads with an inline clone of the record map, falling back to raw payload if needed, and considering a simple helper for map cloning to keep code clear.

thinking
**Refining normalization fallback logic**

I'm adjusting the normalization to retry using a nested raw payload without mutating the original record, ensuring `n` is resolved properly before calling Normalize again after reconstructing `normRec`.
Reconnecting... 1/5

thinking
**Enhancing normalization with nested and raw payload handling**

I'm modifying normalization to unwrap nested payloads inside "payload" fields and try fallback normalization using "rawPayload" if initial attempt fails, while ensuring tenant and project ID fallbacks are applied properly before returning the normalized entry.
file update:
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..2da02919802dd16192966f966fd0d1473d78ac96
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -152,6 +153,8 @@
 				"recordKind":   recordKind,
 				"entityKind":   entityKind,
 				"payload":      payload,
+				"rawPayload":   rawPayload,
+				"vectorPayload": env.VectorPayload,
 				"source":       env.Source,
 				"tenantId":     env.TenantID,
 				"projectKey":   env.ProjectKey,
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

file update
M /Users/rishikeshkumar/Development/Nucleus/platform/brain-core/internal/activities/normalize.go
@@ -13,4 +13,28 @@
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
apply_patch(auto_approved=true) exited 0 in 74ms:
Success. Updated the following files:
M platform/brain-core/internal/activities/normalize.go
file update:
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..2da02919802dd16192966f966fd0d1473d78ac96
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -152,6 +153,8 @@
 				"recordKind":   recordKind,
 				"entityKind":   entityKind,
 				"payload":      payload,
+				"rawPayload":   rawPayload,
+				"vectorPayload": env.VectorPayload,
 				"source":       env.Source,
 				"tenantId":     env.TenantID,
 				"projectKey":   env.ProjectKey,
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1


thinking
**Updating gRPC address fallback**

Planning to expand gRPC address fallbacks in KV client to include VECTOR_GRPC_ADDR and STORE_VECTOR_GRPC_ADDR, defaulting to localhost:9099 to align with store-core, while retaining backwards compatibility as needed. Next, I'll implement the updated fallback chain with possible helper for cleaner code.
file update:
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..2da02919802dd16192966f966fd0d1473d78ac96
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -152,6 +153,8 @@
 				"recordKind":   recordKind,
 				"entityKind":   entityKind,
 				"payload":      payload,
+				"rawPayload":   rawPayload,
+				"vectorPayload": env.VectorPayload,
 				"source":       env.Source,
 				"tenantId":     env.TenantID,
 				"projectKey":   env.ProjectKey,
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

file update
M /Users/rishikeshkumar/Development/Nucleus/platform/brain-core/internal/activities/kv_client.go
@@ -28,3 +28,9 @@
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
apply_patch(auto_approved=true) exited 0 in 55ms:
Success. Updated the following files:
M platform/brain-core/internal/activities/kv_client.go
file update:
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..2da02919802dd16192966f966fd0d1473d78ac96
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -152,6 +153,8 @@
 				"recordKind":   recordKind,
 				"entityKind":   entityKind,
 				"payload":      payload,
+				"rawPayload":   rawPayload,
+				"vectorPayload": env.VectorPayload,
 				"source":       env.Source,
 				"tenantId":     env.TenantID,
 				"projectKey":   env.ProjectKey,
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1


thinking
**Planning canonical project and tenant ID handling**

I'm deciding to update IndexArtifact to prefer projectKey and tenantId from vectorPayload with fallback to earlier variables for canonical metadata. I'll also add fallbacks for SourceFamily and ensure gofmt runs on modified Go files after these adjustments.

thinking
**Adjusting vector entry fields with fallbacks**

Planning to use the vector payload's project key as ProjectID with tenant fallback and similarly fallback SourceFamily to the request value. Considering adding RawPayload or RawMetadata if missing but noting it's likely unnecessary given canonical metadata. Preparing to patch accordingly.
file update:
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..2da02919802dd16192966f966fd0d1473d78ac96
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -152,6 +153,8 @@
 				"recordKind":   recordKind,
 				"entityKind":   entityKind,
 				"payload":      payload,
+				"rawPayload":   rawPayload,
+				"vectorPayload": env.VectorPayload,
 				"source":       env.Source,
 				"tenantId":     env.TenantID,
 				"projectKey":   env.ProjectKey,
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

file update
M /Users/rishikeshkumar/Development/Nucleus/platform/brain-core/internal/activities/activities.go
@@ -1042,2 +1042,3 @@
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
@@ -1045,3 +1046,3 @@
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
@@ -1057,2 +1058,11 @@
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
apply_patch(auto_approved=true) exited 0 in 60ms:
Success. Updated the following files:
M platform/brain-core/internal/activities/activities.go
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..2da02919802dd16192966f966fd0d1473d78ac96
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -152,6 +153,8 @@
 				"recordKind":   recordKind,
 				"entityKind":   entityKind,
 				"payload":      payload,
+				"rawPayload":   rawPayload,
+				"vectorPayload": env.VectorPayload,
 				"source":       env.Source,
 				"tenantId":     env.TenantID,
 				"projectKey":   env.ProjectKey,
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1


thinking
**Assessing normalization fallbacks and test planning**

Confirmed `tenantID` is in scope for normalization; noted `SourceFamily` fallback could remain empty if entry is empty, but no suitable alternative exists. Planning to add focused tests in `activities_test.go` to verify streaming returns vectorPayload and raw payload handling.

thinking
**Validating payload unwrapping and normalizer import**

I'm checking that the streamed record's payload contains the nested structure as expected and considering a focused test for the normalization function to ensure it unwraps nested payloads correctly. To enable this, I need to import the GitHub vector profile package for its initializer to register the normalizer.

thinking
**Planning controlled normalizer test**

I'm preparing to add a test for the normalizeVectorRecord function using a custom dummy normalizer registered under a unique profile ID to isolate test behavior without external dependencies. This approach avoids importing the full vectorprofile connector and focuses on verifying unwrap logic with controlled input and output.

thinking
**Designing normalizer to test fallback logic**

I'm planning a normalizer that sets only NodeID and ContentText, leaving TenantID and ProjectID empty to verify fallback from method arguments works correctly. The test uses nested payloads to ensure unwrapping is handled properly and fallback values are assigned as expected.

thinking
**Planning imports and tests**

I'm adding several new package imports to support vector handling and orchestration, plus two tests in activities_test.go to verify vector payload preservation and correct unwrapping behavior. I'll also create a helper normalizer type and ensure gofmt compliance.

thinking
**Drafting tests for vector record normalization**

I'm sketching tests to verify streaming from staging and that the normalization unwraps nested payloads correctly, planning to implement a test normalizer that sets NodeID based on payload text.

thinking
**Planning test setup and imports**

I noted the need to register the new vector profile globally in tests, confirmed it won't cause issues with reruns or concurrency, and planned the necessary import additions for staging, orchestration, vectorprofile, and vectorstore. I’m arranging to place new tests near the bottom to avoid interference and decided to let the provider assign the stage reference via PutBatch’s response instead of passing one upfront.

thinking
**Preparing staging tests and imports**

I'm planning to add new tests using standard error checks and importing necessary packages in activities_test.go, placing tests at the bottom while ensuring no naming conflicts and proper use of the provider ID with staging.ProviderMemory.
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..2da02919802dd16192966f966fd0d1473d78ac96
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -152,6 +153,8 @@
 				"recordKind":   recordKind,
 				"entityKind":   entityKind,
 				"payload":      payload,
+				"rawPayload":   rawPayload,
+				"vectorPayload": env.VectorPayload,
 				"source":       env.Source,
 				"tenantId":     env.TenantID,
 				"projectKey":   env.ProjectKey,
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

file update
M /Users/rishikeshkumar/Development/Nucleus/platform/brain-core/internal/activities/activities_test.go
@@ -8,2 +8,6 @@
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
apply_patch(auto_approved=true) exited 0 in 59ms:
Success. Updated the following files:
M platform/brain-core/internal/activities/activities_test.go
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..5482a22793b533773803bb8425e17325d507d198
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..2da02919802dd16192966f966fd0d1473d78ac96
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -152,6 +153,8 @@
 				"recordKind":   recordKind,
 				"entityKind":   entityKind,
 				"payload":      payload,
+				"rawPayload":   rawPayload,
+				"vectorPayload": env.VectorPayload,
 				"source":       env.Source,
 				"tenantId":     env.TenantID,
 				"projectKey":   env.ProjectKey,
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1


thinking
**Locating test placement**

I'm checking the bottom part of the test file to find a suitable spot after existing tests to add new ones.
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..5482a22793b533773803bb8425e17325d507d198
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..2da02919802dd16192966f966fd0d1473d78ac96
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -152,6 +153,8 @@
 				"recordKind":   recordKind,
 				"entityKind":   entityKind,
 				"payload":      payload,
+				"rawPayload":   rawPayload,
+				"vectorPayload": env.VectorPayload,
 				"source":       env.Source,
 				"tenantId":     env.TenantID,
 				"projectKey":   env.ProjectKey,
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '240,520p' platform/brain-core/internal/activities/activities_test.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 100ms:
		// Test dataMode reset behavior
		dataMode := "reset"

		shouldResetCheckpoint := dataMode == "reset" || dataMode == "full"
		if !shouldResetCheckpoint {
			t.Error("expected dataMode 'reset' to trigger checkpoint reset")
		}

		// Test filter passing
		filter := map[string]any{
			"project": "PROJ-1",
		}

		if filter["project"] != "PROJ-1" {
			t.Error("expected filter to be passed correctly")
		}
	})
}

thinking
**Confirming file end with line check**
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..5482a22793b533773803bb8425e17325d507d198
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..2da02919802dd16192966f966fd0d1473d78ac96
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -152,6 +153,8 @@
 				"recordKind":   recordKind,
 				"entityKind":   entityKind,
 				"payload":      payload,
+				"rawPayload":   rawPayload,
+				"vectorPayload": env.VectorPayload,
 				"source":       env.Source,
 				"tenantId":     env.TenantID,
 				"projectKey":   env.ProjectKey,
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '520,720p' platform/brain-core/internal/activities/activities_test.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 85ms:


thinking
**Adding detailed staging stream tests**

I'm preparing to add tests for streaming records from staging, verifying the presence and correctness of the vectorPayload and rawPayload fields using standard Go testing patterns.

thinking
**Confirming normalization and provider usage**

The test verifies successful normalization with correct fallback IDs and content extraction. The helper normalizer creates entries using a text-based NodeID as expected. The provider interface supports querying ID, and the stage reference handling in batch creation is noted as acceptable without explicit StageID.

thinking
**Adding memory provider initialization and tests**

I'm ensuring the memory provider is created by calling the default staging registry and adding tests with proper context setup, placing helper types and tests at the end of the file for clarity.
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..5482a22793b533773803bb8425e17325d507d198
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..2da02919802dd16192966f966fd0d1473d78ac96
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -152,6 +153,8 @@
 				"recordKind":   recordKind,
 				"entityKind":   entityKind,
 				"payload":      payload,
+				"rawPayload":   rawPayload,
+				"vectorPayload": env.VectorPayload,
 				"source":       env.Source,
 				"tenantId":     env.TenantID,
 				"projectKey":   env.ProjectKey,
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

file update
M /Users/rishikeshkumar/Development/Nucleus/platform/brain-core/internal/activities/activities_test.go
@@ -257 +257,113 @@
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:   "raw",
+			EntityKind:   "raw.code.file_chunk",
+			Payload:      map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
apply_patch(auto_approved=true) exited 0 in 65ms:
Success. Updated the following files:
M platform/brain-core/internal/activities/activities_test.go
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..71046ce51527cdb17e88d93f2d95862e53145c54
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:   "raw",
+			EntityKind:   "raw.code.file_chunk",
+			Payload:      map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..2da02919802dd16192966f966fd0d1473d78ac96
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -152,6 +153,8 @@
 				"recordKind":   recordKind,
 				"entityKind":   entityKind,
 				"payload":      payload,
+				"rawPayload":   rawPayload,
+				"vectorPayload": env.VectorPayload,
 				"source":       env.Source,
 				"tenantId":     env.TenantID,
 				"projectKey":   env.ProjectKey,
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1


thinking
**Ensuring struct alignment and formatting**

I’m verifying field order and capitalization for PutBatchRequest, confirming StageRef defaults to empty which is acceptable, and planning to run gofmt on modified Go files to fix spacing and formatting.
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..71046ce51527cdb17e88d93f2d95862e53145c54
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:   "raw",
+			EntityKind:   "raw.code.file_chunk",
+			Payload:      map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..2da02919802dd16192966f966fd0d1473d78ac96
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -152,6 +153,8 @@
 				"recordKind":   recordKind,
 				"entityKind":   entityKind,
 				"payload":      payload,
+				"rawPayload":   rawPayload,
+				"vectorPayload": env.VectorPayload,
 				"source":       env.Source,
 				"tenantId":     env.TenantID,
 				"projectKey":   env.ProjectKey,
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'gofmt -w platform/brain-core/internal/activities/activities.go platform/brain-core/internal/activities/stream.go platform/brain-core/internal/activities/normalize.go platform/brain-core/internal/activities/kv_client.go platform/brain-core/internal/activities/activities_test.go' in /Users/rishikeshkumar/Development/Nucleus succeeded in 227ms:


thinking
**Checking Go code formatting and field names**

I'm examining the `stream.go` file around lines 60–160 to verify map key usage and the spelling of the `rawPayload` field, ensuring they align properly with formatting expectations.
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..9bd44f2e2436b24b8aab842302ca0002a423d5dc
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:    "raw",
+			EntityKind:    "raw.code.file_chunk",
+			Payload:       map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..8ef359bc5acb56d4964366103fade2602527fac8
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -149,18 +150,20 @@
 				}
 			}
 			it.current = map[string]any{
-				"recordKind":   recordKind,
-				"entityKind":   entityKind,
-				"payload":      payload,
-				"source":       env.Source,
-				"tenantId":     env.TenantID,
-				"projectKey":   env.ProjectKey,
-				"observedAt":   env.ObservedAt,
-				"stageRef":     it.stageRef,
-				"sliceId":      it.sliceID,
-				"batchRef":     it.batchRefs[it.batchIdx-1],
-				"recordOffset": offset,
-				"mapperKey":    mapperKey,
+				"recordKind":    recordKind,
+				"entityKind":    entityKind,
+				"payload":       payload,
+				"rawPayload":    rawPayload,
+				"vectorPayload": env.VectorPayload,
+				"source":        env.Source,
+				"tenantId":      env.TenantID,
+				"projectKey":    env.ProjectKey,
+				"observedAt":    env.ObservedAt,
+				"stageRef":      it.stageRef,
+				"sliceId":       it.sliceID,
+				"batchRef":      it.batchRefs[it.batchIdx-1],
+				"recordOffset":  offset,
+				"mapperKey":     mapperKey,
 			}
 			return true
 		}
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '60,180p' platform/brain-core/internal/activities/stream.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 136ms:
	if stageRef == "" {
		stageRef = staging.MakeStageRef(providerID, staging.NewStageID())
	}

	batchRefs, err := provider.ListBatches(ctx, stageRef, sliceID)
	if err != nil {
		return nil, func() {}, fmt.Errorf("list batches: %w", err)
	}

	cpBatch := ""
	cpOffset := -1
	if checkpoint != nil {
		if v, ok := checkpoint["batchRef"].(string); ok {
			cpBatch = v
		}
		if v, ok := checkpoint["recordOffset"].(int); ok {
			cpOffset = v
		} else if v, ok := checkpoint["recordOffset"].(float64); ok { // JSON numbers
			cpOffset = int(v)
		}

thinking
**Reviewing normalize.go changes**
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..9bd44f2e2436b24b8aab842302ca0002a423d5dc
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:    "raw",
+			EntityKind:    "raw.code.file_chunk",
+			Payload:       map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..8ef359bc5acb56d4964366103fade2602527fac8
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -149,18 +150,20 @@
 				}
 			}
 			it.current = map[string]any{
-				"recordKind":   recordKind,
-				"entityKind":   entityKind,
-				"payload":      payload,
-				"source":       env.Source,
-				"tenantId":     env.TenantID,
-				"projectKey":   env.ProjectKey,
-				"observedAt":   env.ObservedAt,
-				"stageRef":     it.stageRef,
-				"sliceId":      it.sliceID,
-				"batchRef":     it.batchRefs[it.batchIdx-1],
-				"recordOffset": offset,
-				"mapperKey":    mapperKey,
+				"recordKind":    recordKind,
+				"entityKind":    entityKind,
+				"payload":       payload,
+				"rawPayload":    rawPayload,
+				"vectorPayload": env.VectorPayload,
+				"source":        env.Source,
+				"tenantId":      env.TenantID,
+				"projectKey":    env.ProjectKey,
+				"observedAt":    env.ObservedAt,
+				"stageRef":      it.stageRef,
+				"sliceId":       it.sliceID,
+				"batchRef":      it.batchRefs[it.batchIdx-1],
+				"recordOffset":  offset,
+				"mapperKey":     mapperKey,
 			}
 			return true
 		}
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,200p' platform/brain-core/internal/activities/normalize.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 165ms:
package activities

import (
	"fmt"
	"strings"

	"github.com/nucleus/ucl-core/pkg/vectorprofile"
	"github.com/nucleus/ucl-core/pkg/vectorstore"
)

// normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
// Returns (entry, content, ok).
func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
	normRec := rec
	if payload, ok := rec["payload"].(map[string]any); ok {
		if inner, ok := payload["payload"].(map[string]any); ok {
			clone := make(map[string]any, len(rec))
			for k, v := range rec {
				clone[k] = v

thinking
**Confirming fallback port usage**

I'm checking `kv_client.go` to verify if port 9099 is set as a fallback or used in connection logic.
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..9bd44f2e2436b24b8aab842302ca0002a423d5dc
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:    "raw",
+			EntityKind:    "raw.code.file_chunk",
+			Payload:       map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..8ef359bc5acb56d4964366103fade2602527fac8
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -149,18 +150,20 @@
 				}
 			}
 			it.current = map[string]any{
-				"recordKind":   recordKind,
-				"entityKind":   entityKind,
-				"payload":      payload,
-				"source":       env.Source,
-				"tenantId":     env.TenantID,
-				"projectKey":   env.ProjectKey,
-				"observedAt":   env.ObservedAt,
-				"stageRef":     it.stageRef,
-				"sliceId":      it.sliceID,
-				"batchRef":     it.batchRefs[it.batchIdx-1],
-				"recordOffset": offset,
-				"mapperKey":    mapperKey,
+				"recordKind":    recordKind,
+				"entityKind":    entityKind,
+				"payload":       payload,
+				"rawPayload":    rawPayload,
+				"vectorPayload": env.VectorPayload,
+				"source":        env.Source,
+				"tenantId":      env.TenantID,
+				"projectKey":    env.ProjectKey,
+				"observedAt":    env.ObservedAt,
+				"stageRef":      it.stageRef,
+				"sliceId":       it.sliceID,
+				"batchRef":      it.batchRefs[it.batchIdx-1],
+				"recordOffset":  offset,
+				"mapperKey":     mapperKey,
 			}
 			return true
 		}
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,120p' platform/brain-core/internal/activities/kv_client.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 151ms:
package activities

import (
	"context"
	"encoding/json"
	"fmt"
	"os"
	"sync"
	"time"

	"google.golang.org/grpc"

	kvpb "github.com/nucleus/ucl-core/gen/go/proto/github.com/nucleus/ucl-core/pkg/kvpb"
)

var (
	kvOnce   sync.Once
	kvClient kvpb.KVServiceClient
	kvErr    error
)

thinking
**Reviewing vectorPayload changes in activities.go**

I'm checking how vectorPayload modifications affected formatting and confirming that tenantID is properly recognized around line 1000.
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..9bd44f2e2436b24b8aab842302ca0002a423d5dc
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:    "raw",
+			EntityKind:    "raw.code.file_chunk",
+			Payload:       map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..8ef359bc5acb56d4964366103fade2602527fac8
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -149,18 +150,20 @@
 				}
 			}
 			it.current = map[string]any{
-				"recordKind":   recordKind,
-				"entityKind":   entityKind,
-				"payload":      payload,
-				"source":       env.Source,
-				"tenantId":     env.TenantID,
-				"projectKey":   env.ProjectKey,
-				"observedAt":   env.ObservedAt,
-				"stageRef":     it.stageRef,
-				"sliceId":      it.sliceID,
-				"batchRef":     it.batchRefs[it.batchIdx-1],
-				"recordOffset": offset,
-				"mapperKey":    mapperKey,
+				"recordKind":    recordKind,
+				"entityKind":    entityKind,
+				"payload":       payload,
+				"rawPayload":    rawPayload,
+				"vectorPayload": env.VectorPayload,
+				"source":        env.Source,
+				"tenantId":      env.TenantID,
+				"projectKey":    env.ProjectKey,
+				"observedAt":    env.ObservedAt,
+				"stageRef":      it.stageRef,
+				"sliceId":       it.sliceID,
+				"batchRef":      it.batchRefs[it.batchIdx-1],
+				"recordOffset":  offset,
+				"mapperKey":     mapperKey,
 			}
 			return true
 		}
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1000,1105p' platform/brain-core/internal/activities/activities.go" in /Users/rishikeshkumar/Development/Nucleus succeeded in 100ms:
	if err != nil {
		return nil, err
	}
	if closeFn == nil {
		closeFn = func() {}
	}
	defer closeFn()

	var recordsRead int64
	var lastKey, lastRun string
	var lastBatch string
	var lastOffset int
	var normalized []vectorstore.Entry
	var contents []string
	var kbEvents []kbEvent
	var kbSeq int64
	for iter.Next() {
		rec := iter.Value()
		recordsRead++
		if key, ok := rec["objectKey"].(string); ok {

thinking
**Preparing to run targeted Go tests**

I'm verifying that vectorPayload falls back properly using earlier tenant variables and ensuring code formatting hasn't disrupted test alignments. Next, I'll run tests in the core activities package to confirm all changes pass without environment issues.
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..9bd44f2e2436b24b8aab842302ca0002a423d5dc
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:    "raw",
+			EntityKind:    "raw.code.file_chunk",
+			Payload:       map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..8ef359bc5acb56d4964366103fade2602527fac8
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -149,18 +150,20 @@
 				}
 			}
 			it.current = map[string]any{
-				"recordKind":   recordKind,
-				"entityKind":   entityKind,
-				"payload":      payload,
-				"source":       env.Source,
-				"tenantId":     env.TenantID,
-				"projectKey":   env.ProjectKey,
-				"observedAt":   env.ObservedAt,
-				"stageRef":     it.stageRef,
-				"sliceId":      it.sliceID,
-				"batchRef":     it.batchRefs[it.batchIdx-1],
-				"recordOffset": offset,
-				"mapperKey":    mapperKey,
+				"recordKind":    recordKind,
+				"entityKind":    entityKind,
+				"payload":       payload,
+				"rawPayload":    rawPayload,
+				"vectorPayload": env.VectorPayload,
+				"source":        env.Source,
+				"tenantId":      env.TenantID,
+				"projectKey":    env.ProjectKey,
+				"observedAt":    env.ObservedAt,
+				"stageRef":      it.stageRef,
+				"sliceId":       it.sliceID,
+				"batchRef":      it.batchRefs[it.batchIdx-1],
+				"recordOffset":  offset,
+				"mapperKey":     mapperKey,
 			}
 			return true
 		}
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'go test ./platform/brain-core/internal/activities' in /Users/rishikeshkumar/Development/Nucleus exited 1 in 148ms:
go: cannot find main module, but found .git/config in /Users/rishikeshkumar/Development/Nucleus
	to create a module there, run:
	go mod init

thinking
**Identifying Go module root**

I'm checking for a `go.mod` file in subdirectories like `platform/brain-core` to determine the module root, since the project may use Go modules in a subdirectory rather than the root. Next, I'll locate `go.mod` to set the correct working directory for Go commands.
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..9bd44f2e2436b24b8aab842302ca0002a423d5dc
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:    "raw",
+			EntityKind:    "raw.code.file_chunk",
+			Payload:       map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..8ef359bc5acb56d4964366103fade2602527fac8
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -149,18 +150,20 @@
 				}
 			}
 			it.current = map[string]any{
-				"recordKind":   recordKind,
-				"entityKind":   entityKind,
-				"payload":      payload,
-				"source":       env.Source,
-				"tenantId":     env.TenantID,
-				"projectKey":   env.ProjectKey,
-				"observedAt":   env.ObservedAt,
-				"stageRef":     it.stageRef,
-				"sliceId":      it.sliceID,
-				"batchRef":     it.batchRefs[it.batchIdx-1],
-				"recordOffset": offset,
-				"mapperKey":    mapperKey,
+				"recordKind":    recordKind,
+				"entityKind":    entityKind,
+				"payload":       payload,
+				"rawPayload":    rawPayload,
+				"vectorPayload": env.VectorPayload,
+				"source":        env.Source,
+				"tenantId":      env.TenantID,
+				"projectKey":    env.ProjectKey,
+				"observedAt":    env.ObservedAt,
+				"stageRef":      it.stageRef,
+				"sliceId":       it.sliceID,
+				"batchRef":      it.batchRefs[it.batchIdx-1],
+				"recordOffset":  offset,
+				"mapperKey":     mapperKey,
 			}
 			return true
 		}
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'find .. -maxdepth 3 -name go.mod' in /Users/rishikeshkumar/Development/Nucleus succeeded in 452ms:
../Workspace/go-agent-service/go.mod

thinking
**Investigating missing go.mod**

I realized the initial limited-depth search missed go.mod files possibly deeper in the platform directory, so I'll run a full search at the repo root to locate any go.mod or go.work files.
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..9bd44f2e2436b24b8aab842302ca0002a423d5dc
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:    "raw",
+			EntityKind:    "raw.code.file_chunk",
+			Payload:       map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..8ef359bc5acb56d4964366103fade2602527fac8
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -149,18 +150,20 @@
 				}
 			}
 			it.current = map[string]any{
-				"recordKind":   recordKind,
-				"entityKind":   entityKind,
-				"payload":      payload,
-				"source":       env.Source,
-				"tenantId":     env.TenantID,
-				"projectKey":   env.ProjectKey,
-				"observedAt":   env.ObservedAt,
-				"stageRef":     it.stageRef,
-				"sliceId":      it.sliceID,
-				"batchRef":     it.batchRefs[it.batchIdx-1],
-				"recordOffset": offset,
-				"mapperKey":    mapperKey,
+				"recordKind":    recordKind,
+				"entityKind":    entityKind,
+				"payload":       payload,
+				"rawPayload":    rawPayload,
+				"vectorPayload": env.VectorPayload,
+				"source":        env.Source,
+				"tenantId":      env.TenantID,
+				"projectKey":    env.ProjectKey,
+				"observedAt":    env.ObservedAt,
+				"stageRef":      it.stageRef,
+				"sliceId":       it.sliceID,
+				"batchRef":      it.batchRefs[it.batchIdx-1],
+				"recordOffset":  offset,
+				"mapperKey":     mapperKey,
 			}
 			return true
 		}
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'find /Users/rishikeshkumar/Development/Nucleus -name go.mod' in /Users/rishikeshkumar/Development/Nucleus succeeded in 1.62s:
/Users/rishikeshkumar/Development/Nucleus/platform/brain-core/go.mod
/Users/rishikeshkumar/Development/Nucleus/platform/store-core/go.mod
/Users/rishikeshkumar/Development/Nucleus/platform/ucl-worker/go.mod
/Users/rishikeshkumar/Development/Nucleus/platform/ucl-core/go.mod
/Users/rishikeshkumar/Development/Nucleus/apps/___deprecated__/metadata-api-go/go.mod

thinking
**Adjusting test working directory**

Tests need to run from the module root under platform/brain-core to have proper Go environment and module context; I'll run the tests there to avoid environment issues.
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..9bd44f2e2436b24b8aab842302ca0002a423d5dc
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:    "raw",
+			EntityKind:    "raw.code.file_chunk",
+			Payload:       map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..8ef359bc5acb56d4964366103fade2602527fac8
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -149,18 +150,20 @@
 				}
 			}
 			it.current = map[string]any{
-				"recordKind":   recordKind,
-				"entityKind":   entityKind,
-				"payload":      payload,
-				"source":       env.Source,
-				"tenantId":     env.TenantID,
-				"projectKey":   env.ProjectKey,
-				"observedAt":   env.ObservedAt,
-				"stageRef":     it.stageRef,
-				"sliceId":      it.sliceID,
-				"batchRef":     it.batchRefs[it.batchIdx-1],
-				"recordOffset": offset,
-				"mapperKey":    mapperKey,
+				"recordKind":    recordKind,
+				"entityKind":    entityKind,
+				"payload":       payload,
+				"rawPayload":    rawPayload,
+				"vectorPayload": env.VectorPayload,
+				"source":        env.Source,
+				"tenantId":      env.TenantID,
+				"projectKey":    env.ProjectKey,
+				"observedAt":    env.ObservedAt,
+				"stageRef":      it.stageRef,
+				"sliceId":       it.sliceID,
+				"batchRef":      it.batchRefs[it.batchIdx-1],
+				"recordOffset":  offset,
+				"mapperKey":     mapperKey,
 			}
 			return true
 		}
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'go test ./internal/activities' in /Users/rishikeshkumar/Development/Nucleus/platform/brain-core exited 1 in 114ms:
# ./internal/activities
open /Users/rishikeshkumar/Library/Caches/go-build/6a/6a9615948c6a73c2a6d6c54689e612052ee73ad16fc79f03e6d4e2b1d959a12b-d: operation not permitted
FAIL	./internal/activities [setup failed]
FAIL

thinking
**Testing build cache permission issue**

Suspecting sandbox restrictions on Go build cache causing the error, planning to rerun tests with GOCACHE set to a writable temporary directory to avoid permission problems.
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..9bd44f2e2436b24b8aab842302ca0002a423d5dc
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:    "raw",
+			EntityKind:    "raw.code.file_chunk",
+			Payload:       map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..8ef359bc5acb56d4964366103fade2602527fac8
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -149,18 +150,20 @@
 				}
 			}
 			it.current = map[string]any{
-				"recordKind":   recordKind,
-				"entityKind":   entityKind,
-				"payload":      payload,
-				"source":       env.Source,
-				"tenantId":     env.TenantID,
-				"projectKey":   env.ProjectKey,
-				"observedAt":   env.ObservedAt,
-				"stageRef":     it.stageRef,
-				"sliceId":      it.sliceID,
-				"batchRef":     it.batchRefs[it.batchIdx-1],
-				"recordOffset": offset,
-				"mapperKey":    mapperKey,
+				"recordKind":    recordKind,
+				"entityKind":    entityKind,
+				"payload":       payload,
+				"rawPayload":    rawPayload,
+				"vectorPayload": env.VectorPayload,
+				"source":        env.Source,
+				"tenantId":      env.TenantID,
+				"projectKey":    env.ProjectKey,
+				"observedAt":    env.ObservedAt,
+				"stageRef":      it.stageRef,
+				"sliceId":       it.sliceID,
+				"batchRef":      it.batchRefs[it.batchIdx-1],
+				"recordOffset":  offset,
+				"mapperKey":     mapperKey,
 			}
 			return true
 		}
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'GOCACHE=/tmp/go-cache go test ./internal/activities' in /Users/rishikeshkumar/Development/Nucleus/platform/brain-core succeeded in 13.82s:
ok  	github.com/nucleus/brain-core/internal/activities	0.600s

thinking
**Updating plan statuses to reflect progress**

Steps 1 and 2 are complete, while Step 3 is in progress since tests ran but TODO/STATE/LOG updates remain; I'll mark steps accordingly via the update_plan tool.
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..9bd44f2e2436b24b8aab842302ca0002a423d5dc
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:    "raw",
+			EntityKind:    "raw.code.file_chunk",
+			Payload:       map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..8ef359bc5acb56d4964366103fade2602527fac8
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -149,18 +150,20 @@
 				}
 			}
 			it.current = map[string]any{
-				"recordKind":   recordKind,
-				"entityKind":   entityKind,
-				"payload":      payload,
-				"source":       env.Source,
-				"tenantId":     env.TenantID,
-				"projectKey":   env.ProjectKey,
-				"observedAt":   env.ObservedAt,
-				"stageRef":     it.stageRef,
-				"sliceId":      it.sliceID,
-				"batchRef":     it.batchRefs[it.batchIdx-1],
-				"recordOffset": offset,
-				"mapperKey":    mapperKey,
+				"recordKind":    recordKind,
+				"entityKind":    entityKind,
+				"payload":       payload,
+				"rawPayload":    rawPayload,
+				"vectorPayload": env.VectorPayload,
+				"source":        env.Source,
+				"tenantId":      env.TenantID,
+				"projectKey":    env.ProjectKey,
+				"observedAt":    env.ObservedAt,
+				"stageRef":      it.stageRef,
+				"sliceId":       it.sliceID,
+				"batchRef":      it.batchRefs[it.batchIdx-1],
+				"recordOffset":  offset,
+				"mapperKey":     mapperKey,
 			}
 			return true
 		}
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

Plan update
Implemented staging propagation and normalization fixes plus KV client fallback; tests added and running, wrapping up artifacts next.
  ✓ Propagate vectorPayload + raw payload through staging iterator and normalization so IndexArtifact can upsert vectors
  ✓ Harden checkpoint KV client defaults to hit store-core on 9099
  → Add/execute targeted tests to cover staging vector payload flow; update run artifacts (TODO/STATE/LOG)

thinking
**Reviewing fix impact and TODO updates**

The fix likely resolves the main vector_entries bug by ensuring normalized entries and tenant fallback usage, but full verification without running actual indexing remains uncertain. I'll plan to update the TODO list to mark the debug fix done after confirming the code changes via tests, while noting that additional broader tests (like TypeScript) may be unnecessary since only Go code changed.

thinking
**Planning final artifact update**

Need to update run artifacts and possibly the Story after finishing, following AGENT CODEX post-run instructions; I'll check git status to identify changed files before completing.
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..9bd44f2e2436b24b8aab842302ca0002a423d5dc
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:    "raw",
+			EntityKind:    "raw.code.file_chunk",
+			Payload:       map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..8ef359bc5acb56d4964366103fade2602527fac8
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -149,18 +150,20 @@
 				}
 			}
 			it.current = map[string]any{
-				"recordKind":   recordKind,
-				"entityKind":   entityKind,
-				"payload":      payload,
-				"source":       env.Source,
-				"tenantId":     env.TenantID,
-				"projectKey":   env.ProjectKey,
-				"observedAt":   env.ObservedAt,
-				"stageRef":     it.stageRef,
-				"sliceId":      it.sliceID,
-				"batchRef":     it.batchRefs[it.batchIdx-1],
-				"recordOffset": offset,
-				"mapperKey":    mapperKey,
+				"recordKind":    recordKind,
+				"entityKind":    entityKind,
+				"payload":       payload,
+				"rawPayload":    rawPayload,
+				"vectorPayload": env.VectorPayload,
+				"source":        env.Source,
+				"tenantId":      env.TenantID,
+				"projectKey":    env.ProjectKey,
+				"observedAt":    env.ObservedAt,
+				"stageRef":      it.stageRef,
+				"sliceId":       it.sliceID,
+				"batchRef":      it.batchRefs[it.batchIdx-1],
+				"recordOffset":  offset,
+				"mapperKey":     mapperKey,
 			}
 			return true
 		}
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'git status -sb' in /Users/rishikeshkumar/Development/Nucleus succeeded in 178ms:
## main...origin/main [ahead 7]
A  .agent/workflows/codex-feature.md
M  .env.example
 M .gitignore
M  .vscode/launch.json
M  README.md
M  apps/metadata-api/README.md
MM apps/metadata-api/metadata/kv-store.json
A  apps/metadata-api/metadata/preview/github.files_tenant-github_octocat_hello-world.json
A  apps/metadata-api/metadata/preview/github.issues_tenant-github_octocat_hello-world.json
M  apps/metadata-api/package.json
A  apps/metadata-api/prisma/migrations/20251213195500_code_vector_documents/migration.sql
A  apps/metadata-api/prisma/migrations/20251214061700_materialized_registry/migration.sql
A  apps/metadata-api/prisma/migrations/20251215160000_materialized_index_status/migration.sql
A  apps/metadata-api/prisma/migrations/20251217173000_graph_store/migration.sql
A  apps/metadata-api/prisma/migrations/20251221123500_materialized_artifact/migration.sql
A  apps/metadata-api/run-logs/checkpoint.json
D  apps/metadata-api/src/brain/brainSearch.test.ts
M  apps/metadata-api/src/brain/clusters.ts
A  apps/metadata-api/src/brain/embeddingProviderFactory.ts

thinking
**Clarifying file statuses and next updates**

I confirmed some vector_profile files are untracked despite earlier reads, indicating user changes outside our modifications; our edits show as modified-added, so no revert needed. Next, I'll focus on updating TODOs, tests, and logs per plan step 3, then add a heartbeat/log entry summarizing progress to ensure accurate state tracking.

thinking
**Planning post-run updates**

I'm confirming leftover tasks like marking debug issues done, updating TODOs, appending a progress heartbeat to the log, and recording recent design decisions about vectorPayload propagation and kv fallback in the DECISIONS file. I'll also update STATE and add a new run summary entry to the relevant STORY.md file, checking its existing format first.
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..9bd44f2e2436b24b8aab842302ca0002a423d5dc
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:    "raw",
+			EntityKind:    "raw.code.file_chunk",
+			Payload:       map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..8ef359bc5acb56d4964366103fade2602527fac8
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -149,18 +150,20 @@
 				}
 			}
 			it.current = map[string]any{
-				"recordKind":   recordKind,
-				"entityKind":   entityKind,
-				"payload":      payload,
-				"source":       env.Source,
-				"tenantId":     env.TenantID,
-				"projectKey":   env.ProjectKey,
-				"observedAt":   env.ObservedAt,
-				"stageRef":     it.stageRef,
-				"sliceId":      it.sliceID,
-				"batchRef":     it.batchRefs[it.batchIdx-1],
-				"recordOffset": offset,
-				"mapperKey":    mapperKey,
+				"recordKind":    recordKind,
+				"entityKind":    entityKind,
+				"payload":       payload,
+				"rawPayload":    rawPayload,
+				"vectorPayload": env.VectorPayload,
+				"source":        env.Source,
+				"tenantId":      env.TenantID,
+				"projectKey":    env.ProjectKey,
+				"observedAt":    env.ObservedAt,
+				"stageRef":      it.stageRef,
+				"sliceId":       it.sliceID,
+				"batchRef":      it.batchRefs[it.batchIdx-1],
+				"recordOffset":  offset,
+				"mapperKey":     mapperKey,
 			}
 			return true
 		}
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'tail -n 80 stories/code-vector-profile-and-indexing-v1/STORY.md' in /Users/rishikeshkumar/Development/Nucleus succeeded in 81ms:
# Story — code-vector-profile-and-indexing-v1

- 2025-12-13: Extended pgvector-backed `vector_index_entries` with docId/source metadata and seeded `code.github.v1`, implemented the code index runner/normalizer with deterministic embeddings, MinIO JSONL.GZ reading, and ingestionRunId filtering, and wrote AC1–AC4 tests; brain test suite currently blocked locally by missing METADATA_DATABASE_URL/Postgres.
- 2025-12-14: Pivoted to a single vector store (removed vector_documents), added in-memory profile/vector stores for offline runs, fixed the code index runner API, reran `pnpm --filter @apps/metadata-api test:brain` successfully with deterministic embeddings, applied migration `20251213195500_code_vector_documents` to the local metadata pgvector instance, ran a real GitHub→MinIO ingestion (octocat/Hello-World) with CodeIndexRunner persisting vectors to `metadata.vector_index_entries` (deterministic embeddings), and added a `materialized_artifacts` registry + registry-aware CodeIndexRunner to track index runs (registry not yet fed by ingestion events).

thinking
**Updating log heartbeat with timestamp**

I'm preparing to update the log heartbeat entry to summarize the recent work, capturing the current UTC timestamp using `date -u` for an accurate runtime record.
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..9bd44f2e2436b24b8aab842302ca0002a423d5dc
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:    "raw",
+			EntityKind:    "raw.code.file_chunk",
+			Payload:       map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..8ef359bc5acb56d4964366103fade2602527fac8
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -149,18 +150,20 @@
 				}
 			}
 			it.current = map[string]any{
-				"recordKind":   recordKind,
-				"entityKind":   entityKind,
-				"payload":      payload,
-				"source":       env.Source,
-				"tenantId":     env.TenantID,
-				"projectKey":   env.ProjectKey,
-				"observedAt":   env.ObservedAt,
-				"stageRef":     it.stageRef,
-				"sliceId":      it.sliceID,
-				"batchRef":     it.batchRefs[it.batchIdx-1],
-				"recordOffset": offset,
-				"mapperKey":    mapperKey,
+				"recordKind":    recordKind,
+				"entityKind":    entityKind,
+				"payload":       payload,
+				"rawPayload":    rawPayload,
+				"vectorPayload": env.VectorPayload,
+				"source":        env.Source,
+				"tenantId":      env.TenantID,
+				"projectKey":    env.ProjectKey,
+				"observedAt":    env.ObservedAt,
+				"stageRef":      it.stageRef,
+				"sliceId":       it.sliceID,
+				"batchRef":      it.batchRefs[it.batchIdx-1],
+				"recordOffset":  offset,
+				"mapperKey":     mapperKey,
 			}
 			return true
 		}
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc 'date -u +"%Y-%m-%dT%H:%M:%SZ"' in /Users/rishikeshkumar/Development/Nucleus succeeded in 81ms:
2025-12-22T14:06:27Z

thinking
**Adding heartbeat log entry**

I'll append a heartbeat entry to the run log with a summary of done work—brain-core staging updates, testing details—next steps focusing on TODO and possible reindexing, and risks noting pending validation, while using the actual current timestamp despite minor log order quirks.
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..9bd44f2e2436b24b8aab842302ca0002a423d5dc
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:    "raw",
+			EntityKind:    "raw.code.file_chunk",
+			Payload:       map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..8ef359bc5acb56d4964366103fade2602527fac8
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -149,18 +150,20 @@
 				}
 			}
 			it.current = map[string]any{
-				"recordKind":   recordKind,
-				"entityKind":   entityKind,
-				"payload":      payload,
-				"source":       env.Source,
-				"tenantId":     env.TenantID,
-				"projectKey":   env.ProjectKey,
-				"observedAt":   env.ObservedAt,
-				"stageRef":     it.stageRef,
-				"sliceId":      it.sliceID,
-				"batchRef":     it.batchRefs[it.batchIdx-1],
-				"recordOffset": offset,
-				"mapperKey":    mapperKey,
+				"recordKind":    recordKind,
+				"entityKind":    entityKind,
+				"payload":       payload,
+				"rawPayload":    rawPayload,
+				"vectorPayload": env.VectorPayload,
+				"source":        env.Source,
+				"tenantId":      env.TenantID,
+				"projectKey":    env.ProjectKey,
+				"observedAt":    env.ObservedAt,
+				"stageRef":      it.stageRef,
+				"sliceId":       it.sliceID,
+				"batchRef":      it.batchRefs[it.batchIdx-1],
+				"recordOffset":  offset,
+				"mapperKey":     mapperKey,
 			}
 			return true
 		}
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "printf '%s | heartbeat | done: propagated vectorPayload/rawPayload through staging iterator, normalized nested payload fallback, hardened KV gRPC default to 9099, and added Go tests (streamFromStaging, normalization); GOCACHE=/tmp/go-cache go test ./internal/activities passed | next: update TODO/STATE/story, then validate indexing on real env when available | risks: production indexing still unverified against live store-core/minio stack\\n' \"2025-12-22T14:06:27Z\" >> runs/code-vector-profile-and-indexing-v1/LOG.md" in /Users/rishikeshkumar/Development/Nucleus succeeded in 77ms:


thinking
**Updating and refining TODO checklist**

I'm preparing to mark the debug item as done in the TODO file while adding a note to verify staging environment indexing with the store-core provider, ensuring clear next steps for validation after fixes.
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..9bd44f2e2436b24b8aab842302ca0002a423d5dc
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:    "raw",
+			EntityKind:    "raw.code.file_chunk",
+			Payload:       map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..8ef359bc5acb56d4964366103fade2602527fac8
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -149,18 +150,20 @@
 				}
 			}
 			it.current = map[string]any{
-				"recordKind":   recordKind,
-				"entityKind":   entityKind,
-				"payload":      payload,
-				"source":       env.Source,
-				"tenantId":     env.TenantID,
-				"projectKey":   env.ProjectKey,
-				"observedAt":   env.ObservedAt,
-				"stageRef":     it.stageRef,
-				"sliceId":      it.sliceID,
-				"batchRef":     it.batchRefs[it.batchIdx-1],
-				"recordOffset": offset,
-				"mapperKey":    mapperKey,
+				"recordKind":    recordKind,
+				"entityKind":    entityKind,
+				"payload":       payload,
+				"rawPayload":    rawPayload,
+				"vectorPayload": env.VectorPayload,
+				"source":        env.Source,
+				"tenantId":      env.TenantID,
+				"projectKey":    env.ProjectKey,
+				"observedAt":    env.ObservedAt,
+				"stageRef":      it.stageRef,
+				"sliceId":       it.sliceID,
+				"batchRef":      it.batchRefs[it.batchIdx-1],
+				"recordOffset":  offset,
+				"mapperKey":     mapperKey,
 			}
 			return true
 		}
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

file update
M /Users/rishikeshkumar/Development/Nucleus/runs/code-vector-profile-and-indexing-v1/TODO.md
@@ -2,3 +2,3 @@
 - [x] Verify Staging (vectorPayload IS present in MinIO).
-- [ ] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
+- [x] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
 - [ ] Rerun code indexing with the production embedding provider once access is available.
apply_patch(auto_approved=true) exited 0 in 69ms:
Success. Updated the following files:
M runs/code-vector-profile-and-indexing-v1/TODO.md
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..9bd44f2e2436b24b8aab842302ca0002a423d5dc
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:    "raw",
+			EntityKind:    "raw.code.file_chunk",
+			Payload:       map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..8ef359bc5acb56d4964366103fade2602527fac8
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -149,18 +150,20 @@
 				}
 			}
 			it.current = map[string]any{
-				"recordKind":   recordKind,
-				"entityKind":   entityKind,
-				"payload":      payload,
-				"source":       env.Source,
-				"tenantId":     env.TenantID,
-				"projectKey":   env.ProjectKey,
-				"observedAt":   env.ObservedAt,
-				"stageRef":     it.stageRef,
-				"sliceId":      it.sliceID,
-				"batchRef":     it.batchRefs[it.batchIdx-1],
-				"recordOffset": offset,
-				"mapperKey":    mapperKey,
+				"recordKind":    recordKind,
+				"entityKind":    entityKind,
+				"payload":       payload,
+				"rawPayload":    rawPayload,
+				"vectorPayload": env.VectorPayload,
+				"source":        env.Source,
+				"tenantId":      env.TenantID,
+				"projectKey":    env.ProjectKey,
+				"observedAt":    env.ObservedAt,
+				"stageRef":      it.stageRef,
+				"sliceId":       it.sliceID,
+				"batchRef":      it.batchRefs[it.batchIdx-1],
+				"recordOffset":  offset,
+				"mapperKey":     mapperKey,
 			}
 			return true
 		}
diff --git a/runs/code-vector-profile-and-indexing-v1/TODO.md b/runs/code-vector-profile-and-indexing-v1/TODO.md
index 0bada439bd1de49e1a0913caf5f994a74d225413..c08b43dcc387e3fa061794cd5c7cfabb63562350
--- a/runs/code-vector-profile-and-indexing-v1/TODO.md
+++ b/runs/code-vector-profile-and-indexing-v1/TODO.md
@@ -1,5 +1,5 @@
 - [x] Implement Endpoint-Driven Vector Normalization (VectorProfileProvider, GitHub connector, Staging integration).
 - [x] Verify Staging (vectorPayload IS present in MinIO).
-- [ ] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
+- [x] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
 - [ ] Rerun code indexing with the production embedding provider once access is available.
 - [ ] Feed materialized_artifacts registry from sink completion events.
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1


thinking
**Updating decisions file**

I'll add a new bullet to the DECISIONS file summarizing streaming vectorPayload exposure, payload normalization, and KV client default address to document these key changes.
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..9bd44f2e2436b24b8aab842302ca0002a423d5dc
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:    "raw",
+			EntityKind:    "raw.code.file_chunk",
+			Payload:       map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..8ef359bc5acb56d4964366103fade2602527fac8
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -149,18 +150,20 @@
 				}
 			}
 			it.current = map[string]any{
-				"recordKind":   recordKind,
-				"entityKind":   entityKind,
-				"payload":      payload,
-				"source":       env.Source,
-				"tenantId":     env.TenantID,
-				"projectKey":   env.ProjectKey,
-				"observedAt":   env.ObservedAt,
-				"stageRef":     it.stageRef,
-				"sliceId":      it.sliceID,
-				"batchRef":     it.batchRefs[it.batchIdx-1],
-				"recordOffset": offset,
-				"mapperKey":    mapperKey,
+				"recordKind":    recordKind,
+				"entityKind":    entityKind,
+				"payload":       payload,
+				"rawPayload":    rawPayload,
+				"vectorPayload": env.VectorPayload,
+				"source":        env.Source,
+				"tenantId":      env.TenantID,
+				"projectKey":    env.ProjectKey,
+				"observedAt":    env.ObservedAt,
+				"stageRef":      it.stageRef,
+				"sliceId":       it.sliceID,
+				"batchRef":      it.batchRefs[it.batchIdx-1],
+				"recordOffset":  offset,
+				"mapperKey":     mapperKey,
 			}
 			return true
 		}
diff --git a/runs/code-vector-profile-and-indexing-v1/TODO.md b/runs/code-vector-profile-and-indexing-v1/TODO.md
index 0bada439bd1de49e1a0913caf5f994a74d225413..c08b43dcc387e3fa061794cd5c7cfabb63562350
--- a/runs/code-vector-profile-and-indexing-v1/TODO.md
+++ b/runs/code-vector-profile-and-indexing-v1/TODO.md
@@ -1,5 +1,5 @@
 - [x] Implement Endpoint-Driven Vector Normalization (VectorProfileProvider, GitHub connector, Staging integration).
 - [x] Verify Staging (vectorPayload IS present in MinIO).
-- [ ] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
+- [x] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
 - [ ] Rerun code indexing with the production embedding provider once access is available.
 - [ ] Feed materialized_artifacts registry from sink completion events.
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

file update
M /Users/rishikeshkumar/Development/Nucleus/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
@@ -4 +4,2 @@
 - Index runs read MinIO-style JSONL.GZ envelopes from a dataset prefix (supports `minio://bucket/prefix`) via filesystem, streaming/decompressing lines; optional `ingestionRunId` filters to `run=<id>` segments (basePrefix defaults to `sink`). Errors classify as E_SOURCE_READ_FAILED, E_INVALID_RECORD, E_MISSING_CANONICAL_KEYS, E_EMBEDDING_FAILED, or E_VECTOR_DB_FAILED. Oversized text is truncated; malformed/missing-key records are skipped and surfaced as errors, with run status set to SUCCEEDED_WITH_ERRORS when any non-fatal errors occur.
+- Staging replay now surfaces `vectorPayload` and `rawPayload` to IndexArtifact; normalization unwraps nested payloads so VectorProfile normalizers see original records, and KV checkpoint client defaults to VECTOR_GRPC_ADDR/9099 (falling back from KV/UCL) to avoid checkpoint timeouts when store-core runs on 9099.
apply_patch(auto_approved=true) exited 0 in 91ms:
Success. Updated the following files:
M runs/code-vector-profile-and-indexing-v1/DECISIONS.md
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..9bd44f2e2436b24b8aab842302ca0002a423d5dc
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:    "raw",
+			EntityKind:    "raw.code.file_chunk",
+			Payload:       map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..8ef359bc5acb56d4964366103fade2602527fac8
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -149,18 +150,20 @@
 				}
 			}
 			it.current = map[string]any{
-				"recordKind":   recordKind,
-				"entityKind":   entityKind,
-				"payload":      payload,
-				"source":       env.Source,
-				"tenantId":     env.TenantID,
-				"projectKey":   env.ProjectKey,
-				"observedAt":   env.ObservedAt,
-				"stageRef":     it.stageRef,
-				"sliceId":      it.sliceID,
-				"batchRef":     it.batchRefs[it.batchIdx-1],
-				"recordOffset": offset,
-				"mapperKey":    mapperKey,
+				"recordKind":    recordKind,
+				"entityKind":    entityKind,
+				"payload":       payload,
+				"rawPayload":    rawPayload,
+				"vectorPayload": env.VectorPayload,
+				"source":        env.Source,
+				"tenantId":      env.TenantID,
+				"projectKey":    env.ProjectKey,
+				"observedAt":    env.ObservedAt,
+				"stageRef":      it.stageRef,
+				"sliceId":       it.sliceID,
+				"batchRef":      it.batchRefs[it.batchIdx-1],
+				"recordOffset":  offset,
+				"mapperKey":     mapperKey,
 			}
 			return true
 		}
diff --git a/runs/code-vector-profile-and-indexing-v1/DECISIONS.md b/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
index 29c1ccbf79cbaa94d2475978d1fdb5b8fec28620..2cca30268af052a903fa1c11ac33ee194f6fd081
--- a/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
+++ b/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
@@ -2,3 +2,4 @@
 - Canonical code profile `code.github.v1` normalizes projectKey to lower-case `{owner}/{repo}`, entityKind=`code.file_chunk`, profileKind=`code`, sourceFamily=`github`, docId=`code:github:{tenantId}:{projectKey}:{path}:{sha}:{chunkIndex}`. Content comes from `payload.text` truncated to 20k chars; attributes capture path/sha/chunkIndex/language/truncated flag. Missing tenantId/projectKey/profileKind fail closed with E_MISSING_CANONICAL_KEYS. Profile is also seeded into `vector_index_profiles` with text_source field hints.
 - Deterministic embeddings use a fake provider that hashes text to a one-hot 1536-dim vector (no external calls) for tests and default offline runs.
 - Index runs read MinIO-style JSONL.GZ envelopes from a dataset prefix (supports `minio://bucket/prefix`) via filesystem, streaming/decompressing lines; optional `ingestionRunId` filters to `run=<id>` segments (basePrefix defaults to `sink`). Errors classify as E_SOURCE_READ_FAILED, E_INVALID_RECORD, E_MISSING_CANONICAL_KEYS, E_EMBEDDING_FAILED, or E_VECTOR_DB_FAILED. Oversized text is truncated; malformed/missing-key records are skipped and surfaced as errors, with run status set to SUCCEEDED_WITH_ERRORS when any non-fatal errors occur.
+- Staging replay now surfaces `vectorPayload` and `rawPayload` to IndexArtifact; normalization unwraps nested payloads so VectorProfile normalizers see original records, and KV checkpoint client defaults to VECTOR_GRPC_ADDR/9099 (falling back from KV/UCL) to avoid checkpoint timeouts when store-core runs on 9099.
diff --git a/runs/code-vector-profile-and-indexing-v1/TODO.md b/runs/code-vector-profile-and-indexing-v1/TODO.md
index 0bada439bd1de49e1a0913caf5f994a74d225413..c08b43dcc387e3fa061794cd5c7cfabb63562350
--- a/runs/code-vector-profile-and-indexing-v1/TODO.md
+++ b/runs/code-vector-profile-and-indexing-v1/TODO.md
@@ -1,5 +1,5 @@
 - [x] Implement Endpoint-Driven Vector Normalization (VectorProfileProvider, GitHub connector, Staging integration).
 - [x] Verify Staging (vectorPayload IS present in MinIO).
-- [ ] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
+- [x] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
 - [ ] Rerun code indexing with the production embedding provider once access is available.
 - [ ] Feed materialized_artifacts registry from sink completion events.
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1


thinking
**Adding story update and final state sync**

I'm preparing to add a dated bullet to the story summarizing recent fixes and test additions, then I'll update sync/STATE.md with the latest run info and finalize state changes to mark plan completion.
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..9bd44f2e2436b24b8aab842302ca0002a423d5dc
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:    "raw",
+			EntityKind:    "raw.code.file_chunk",
+			Payload:       map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..8ef359bc5acb56d4964366103fade2602527fac8
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -149,18 +150,20 @@
 				}
 			}
 			it.current = map[string]any{
-				"recordKind":   recordKind,
-				"entityKind":   entityKind,
-				"payload":      payload,
-				"source":       env.Source,
-				"tenantId":     env.TenantID,
-				"projectKey":   env.ProjectKey,
-				"observedAt":   env.ObservedAt,
-				"stageRef":     it.stageRef,
-				"sliceId":      it.sliceID,
-				"batchRef":     it.batchRefs[it.batchIdx-1],
-				"recordOffset": offset,
-				"mapperKey":    mapperKey,
+				"recordKind":    recordKind,
+				"entityKind":    entityKind,
+				"payload":       payload,
+				"rawPayload":    rawPayload,
+				"vectorPayload": env.VectorPayload,
+				"source":        env.Source,
+				"tenantId":      env.TenantID,
+				"projectKey":    env.ProjectKey,
+				"observedAt":    env.ObservedAt,
+				"stageRef":      it.stageRef,
+				"sliceId":       it.sliceID,
+				"batchRef":      it.batchRefs[it.batchIdx-1],
+				"recordOffset":  offset,
+				"mapperKey":     mapperKey,
 			}
 			return true
 		}
diff --git a/runs/code-vector-profile-and-indexing-v1/DECISIONS.md b/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
index 29c1ccbf79cbaa94d2475978d1fdb5b8fec28620..2cca30268af052a903fa1c11ac33ee194f6fd081
--- a/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
+++ b/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
@@ -2,3 +2,4 @@
 - Canonical code profile `code.github.v1` normalizes projectKey to lower-case `{owner}/{repo}`, entityKind=`code.file_chunk`, profileKind=`code`, sourceFamily=`github`, docId=`code:github:{tenantId}:{projectKey}:{path}:{sha}:{chunkIndex}`. Content comes from `payload.text` truncated to 20k chars; attributes capture path/sha/chunkIndex/language/truncated flag. Missing tenantId/projectKey/profileKind fail closed with E_MISSING_CANONICAL_KEYS. Profile is also seeded into `vector_index_profiles` with text_source field hints.
 - Deterministic embeddings use a fake provider that hashes text to a one-hot 1536-dim vector (no external calls) for tests and default offline runs.
 - Index runs read MinIO-style JSONL.GZ envelopes from a dataset prefix (supports `minio://bucket/prefix`) via filesystem, streaming/decompressing lines; optional `ingestionRunId` filters to `run=<id>` segments (basePrefix defaults to `sink`). Errors classify as E_SOURCE_READ_FAILED, E_INVALID_RECORD, E_MISSING_CANONICAL_KEYS, E_EMBEDDING_FAILED, or E_VECTOR_DB_FAILED. Oversized text is truncated; malformed/missing-key records are skipped and surfaced as errors, with run status set to SUCCEEDED_WITH_ERRORS when any non-fatal errors occur.
+- Staging replay now surfaces `vectorPayload` and `rawPayload` to IndexArtifact; normalization unwraps nested payloads so VectorProfile normalizers see original records, and KV checkpoint client defaults to VECTOR_GRPC_ADDR/9099 (falling back from KV/UCL) to avoid checkpoint timeouts when store-core runs on 9099.
diff --git a/runs/code-vector-profile-and-indexing-v1/TODO.md b/runs/code-vector-profile-and-indexing-v1/TODO.md
index 0bada439bd1de49e1a0913caf5f994a74d225413..c08b43dcc387e3fa061794cd5c7cfabb63562350
--- a/runs/code-vector-profile-and-indexing-v1/TODO.md
+++ b/runs/code-vector-profile-and-indexing-v1/TODO.md
@@ -1,5 +1,5 @@
 - [x] Implement Endpoint-Driven Vector Normalization (VectorProfileProvider, GitHub connector, Staging integration).
 - [x] Verify Staging (vectorPayload IS present in MinIO).
-- [ ] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
+- [x] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
 - [ ] Rerun code indexing with the production embedding provider once access is available.
 - [ ] Feed materialized_artifacts registry from sink completion events.
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

file update
M /Users/rishikeshkumar/Development/Nucleus/stories/code-vector-profile-and-indexing-v1/STORY.md
@@ -4 +4,2 @@
 - 2025-12-14: Pivoted to a single vector store (removed vector_documents), added in-memory profile/vector stores for offline runs, fixed the code index runner API, reran `pnpm --filter @apps/metadata-api test:brain` successfully with deterministic embeddings, applied migration `20251213195500_code_vector_documents` to the local metadata pgvector instance, ran a real GitHub→MinIO ingestion (octocat/Hello-World) with CodeIndexRunner persisting vectors to `metadata.vector_index_entries` (deterministic embeddings), and added a `materialized_artifacts` registry + registry-aware CodeIndexRunner to track index runs (registry not yet fed by ingestion events).
+- 2025-12-22: Unblocked IndexArtifact by replaying staged envelopes with `vectorPayload`/`rawPayload`, unwrapping nested payloads for VectorProfile normalizers, defaulting KV checkpoints to store-core on 9099, and adding Go tests for staging replay + normalization (GOCACHE=/tmp/go-cache go test ./internal/activities).
apply_patch(auto_approved=true) exited 0 in 144ms:
Success. Updated the following files:
M stories/code-vector-profile-and-indexing-v1/STORY.md
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..9bd44f2e2436b24b8aab842302ca0002a423d5dc
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:    "raw",
+			EntityKind:    "raw.code.file_chunk",
+			Payload:       map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..8ef359bc5acb56d4964366103fade2602527fac8
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -149,18 +150,20 @@
 				}
 			}
 			it.current = map[string]any{
-				"recordKind":   recordKind,
-				"entityKind":   entityKind,
-				"payload":      payload,
-				"source":       env.Source,
-				"tenantId":     env.TenantID,
-				"projectKey":   env.ProjectKey,
-				"observedAt":   env.ObservedAt,
-				"stageRef":     it.stageRef,
-				"sliceId":      it.sliceID,
-				"batchRef":     it.batchRefs[it.batchIdx-1],
-				"recordOffset": offset,
-				"mapperKey":    mapperKey,
+				"recordKind":    recordKind,
+				"entityKind":    entityKind,
+				"payload":       payload,
+				"rawPayload":    rawPayload,
+				"vectorPayload": env.VectorPayload,
+				"source":        env.Source,
+				"tenantId":      env.TenantID,
+				"projectKey":    env.ProjectKey,
+				"observedAt":    env.ObservedAt,
+				"stageRef":      it.stageRef,
+				"sliceId":       it.sliceID,
+				"batchRef":      it.batchRefs[it.batchIdx-1],
+				"recordOffset":  offset,
+				"mapperKey":     mapperKey,
 			}
 			return true
 		}
diff --git a/runs/code-vector-profile-and-indexing-v1/DECISIONS.md b/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
index 29c1ccbf79cbaa94d2475978d1fdb5b8fec28620..2cca30268af052a903fa1c11ac33ee194f6fd081
--- a/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
+++ b/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
@@ -2,3 +2,4 @@
 - Canonical code profile `code.github.v1` normalizes projectKey to lower-case `{owner}/{repo}`, entityKind=`code.file_chunk`, profileKind=`code`, sourceFamily=`github`, docId=`code:github:{tenantId}:{projectKey}:{path}:{sha}:{chunkIndex}`. Content comes from `payload.text` truncated to 20k chars; attributes capture path/sha/chunkIndex/language/truncated flag. Missing tenantId/projectKey/profileKind fail closed with E_MISSING_CANONICAL_KEYS. Profile is also seeded into `vector_index_profiles` with text_source field hints.
 - Deterministic embeddings use a fake provider that hashes text to a one-hot 1536-dim vector (no external calls) for tests and default offline runs.
 - Index runs read MinIO-style JSONL.GZ envelopes from a dataset prefix (supports `minio://bucket/prefix`) via filesystem, streaming/decompressing lines; optional `ingestionRunId` filters to `run=<id>` segments (basePrefix defaults to `sink`). Errors classify as E_SOURCE_READ_FAILED, E_INVALID_RECORD, E_MISSING_CANONICAL_KEYS, E_EMBEDDING_FAILED, or E_VECTOR_DB_FAILED. Oversized text is truncated; malformed/missing-key records are skipped and surfaced as errors, with run status set to SUCCEEDED_WITH_ERRORS when any non-fatal errors occur.
+- Staging replay now surfaces `vectorPayload` and `rawPayload` to IndexArtifact; normalization unwraps nested payloads so VectorProfile normalizers see original records, and KV checkpoint client defaults to VECTOR_GRPC_ADDR/9099 (falling back from KV/UCL) to avoid checkpoint timeouts when store-core runs on 9099.
diff --git a/runs/code-vector-profile-and-indexing-v1/TODO.md b/runs/code-vector-profile-and-indexing-v1/TODO.md
index 0bada439bd1de49e1a0913caf5f994a74d225413..c08b43dcc387e3fa061794cd5c7cfabb63562350
--- a/runs/code-vector-profile-and-indexing-v1/TODO.md
+++ b/runs/code-vector-profile-and-indexing-v1/TODO.md
@@ -1,5 +1,5 @@
 - [x] Implement Endpoint-Driven Vector Normalization (VectorProfileProvider, GitHub connector, Staging integration).
 - [x] Verify Staging (vectorPayload IS present in MinIO).
-- [ ] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
+- [x] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
 - [ ] Rerun code indexing with the production embedding provider once access is available.
 - [ ] Feed materialized_artifacts registry from sink completion events.
diff --git a/stories/code-vector-profile-and-indexing-v1/STORY.md b/stories/code-vector-profile-and-indexing-v1/STORY.md
index 83311a64dc7bd20787734b76ff509d51483b20bd..a62438295091ba6b53d561e5e97616b4627f29bc
--- a/stories/code-vector-profile-and-indexing-v1/STORY.md
+++ b/stories/code-vector-profile-and-indexing-v1/STORY.md
@@ -2,3 +2,4 @@
 
 - 2025-12-13: Extended pgvector-backed `vector_index_entries` with docId/source metadata and seeded `code.github.v1`, implemented the code index runner/normalizer with deterministic embeddings, MinIO JSONL.GZ reading, and ingestionRunId filtering, and wrote AC1–AC4 tests; brain test suite currently blocked locally by missing METADATA_DATABASE_URL/Postgres.
 - 2025-12-14: Pivoted to a single vector store (removed vector_documents), added in-memory profile/vector stores for offline runs, fixed the code index runner API, reran `pnpm --filter @apps/metadata-api test:brain` successfully with deterministic embeddings, applied migration `20251213195500_code_vector_documents` to the local metadata pgvector instance, ran a real GitHub→MinIO ingestion (octocat/Hello-World) with CodeIndexRunner persisting vectors to `metadata.vector_index_entries` (deterministic embeddings), and added a `materialized_artifacts` registry + registry-aware CodeIndexRunner to track index runs (registry not yet fed by ingestion events).
+- 2025-12-22: Unblocked IndexArtifact by replaying staged envelopes with `vectorPayload`/`rawPayload`, unwrapping nested payloads for VectorProfile normalizers, defaulting KV checkpoints to store-core on 9099, and adding Go tests for staging replay + normalization (GOCACHE=/tmp/go-cache go test ./internal/activities).
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1


thinking
**Deciding run status as in-progress**

Due to unresolved TODOs and lack of production verification, the run status should be marked as in-progress rather than success. Next, I'll update the Last Run block with the current slug and status in-progress, add an event entry with a timestamp, and include a test summary with approximate duration.

thinking
**Planning state file updates**

I'm preparing to increment the decisions count by one, add a new event line with the latest timestamp, and update the Last Run block in `sync/STATE.md` while keeping format intact; I'll confirm current content near the top before patching.
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..9bd44f2e2436b24b8aab842302ca0002a423d5dc
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:    "raw",
+			EntityKind:    "raw.code.file_chunk",
+			Payload:       map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..8ef359bc5acb56d4964366103fade2602527fac8
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -149,18 +150,20 @@
 				}
 			}
 			it.current = map[string]any{
-				"recordKind":   recordKind,
-				"entityKind":   entityKind,
-				"payload":      payload,
-				"source":       env.Source,
-				"tenantId":     env.TenantID,
-				"projectKey":   env.ProjectKey,
-				"observedAt":   env.ObservedAt,
-				"stageRef":     it.stageRef,
-				"sliceId":      it.sliceID,
-				"batchRef":     it.batchRefs[it.batchIdx-1],
-				"recordOffset": offset,
-				"mapperKey":    mapperKey,
+				"recordKind":    recordKind,
+				"entityKind":    entityKind,
+				"payload":       payload,
+				"rawPayload":    rawPayload,
+				"vectorPayload": env.VectorPayload,
+				"source":        env.Source,
+				"tenantId":      env.TenantID,
+				"projectKey":    env.ProjectKey,
+				"observedAt":    env.ObservedAt,
+				"stageRef":      it.stageRef,
+				"sliceId":       it.sliceID,
+				"batchRef":      it.batchRefs[it.batchIdx-1],
+				"recordOffset":  offset,
+				"mapperKey":     mapperKey,
 			}
 			return true
 		}
diff --git a/runs/code-vector-profile-and-indexing-v1/DECISIONS.md b/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
index 29c1ccbf79cbaa94d2475978d1fdb5b8fec28620..2cca30268af052a903fa1c11ac33ee194f6fd081
--- a/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
+++ b/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
@@ -2,3 +2,4 @@
 - Canonical code profile `code.github.v1` normalizes projectKey to lower-case `{owner}/{repo}`, entityKind=`code.file_chunk`, profileKind=`code`, sourceFamily=`github`, docId=`code:github:{tenantId}:{projectKey}:{path}:{sha}:{chunkIndex}`. Content comes from `payload.text` truncated to 20k chars; attributes capture path/sha/chunkIndex/language/truncated flag. Missing tenantId/projectKey/profileKind fail closed with E_MISSING_CANONICAL_KEYS. Profile is also seeded into `vector_index_profiles` with text_source field hints.
 - Deterministic embeddings use a fake provider that hashes text to a one-hot 1536-dim vector (no external calls) for tests and default offline runs.
 - Index runs read MinIO-style JSONL.GZ envelopes from a dataset prefix (supports `minio://bucket/prefix`) via filesystem, streaming/decompressing lines; optional `ingestionRunId` filters to `run=<id>` segments (basePrefix defaults to `sink`). Errors classify as E_SOURCE_READ_FAILED, E_INVALID_RECORD, E_MISSING_CANONICAL_KEYS, E_EMBEDDING_FAILED, or E_VECTOR_DB_FAILED. Oversized text is truncated; malformed/missing-key records are skipped and surfaced as errors, with run status set to SUCCEEDED_WITH_ERRORS when any non-fatal errors occur.
+- Staging replay now surfaces `vectorPayload` and `rawPayload` to IndexArtifact; normalization unwraps nested payloads so VectorProfile normalizers see original records, and KV checkpoint client defaults to VECTOR_GRPC_ADDR/9099 (falling back from KV/UCL) to avoid checkpoint timeouts when store-core runs on 9099.
diff --git a/runs/code-vector-profile-and-indexing-v1/TODO.md b/runs/code-vector-profile-and-indexing-v1/TODO.md
index 0bada439bd1de49e1a0913caf5f994a74d225413..c08b43dcc387e3fa061794cd5c7cfabb63562350
--- a/runs/code-vector-profile-and-indexing-v1/TODO.md
+++ b/runs/code-vector-profile-and-indexing-v1/TODO.md
@@ -1,5 +1,5 @@
 - [x] Implement Endpoint-Driven Vector Normalization (VectorProfileProvider, GitHub connector, Staging integration).
 - [x] Verify Staging (vectorPayload IS present in MinIO).
-- [ ] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
+- [x] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
 - [ ] Rerun code indexing with the production embedding provider once access is available.
 - [ ] Feed materialized_artifacts registry from sink completion events.
diff --git a/stories/code-vector-profile-and-indexing-v1/STORY.md b/stories/code-vector-profile-and-indexing-v1/STORY.md
index 83311a64dc7bd20787734b76ff509d51483b20bd..a62438295091ba6b53d561e5e97616b4627f29bc
--- a/stories/code-vector-profile-and-indexing-v1/STORY.md
+++ b/stories/code-vector-profile-and-indexing-v1/STORY.md
@@ -2,3 +2,4 @@
 
 - 2025-12-13: Extended pgvector-backed `vector_index_entries` with docId/source metadata and seeded `code.github.v1`, implemented the code index runner/normalizer with deterministic embeddings, MinIO JSONL.GZ reading, and ingestionRunId filtering, and wrote AC1–AC4 tests; brain test suite currently blocked locally by missing METADATA_DATABASE_URL/Postgres.
 - 2025-12-14: Pivoted to a single vector store (removed vector_documents), added in-memory profile/vector stores for offline runs, fixed the code index runner API, reran `pnpm --filter @apps/metadata-api test:brain` successfully with deterministic embeddings, applied migration `20251213195500_code_vector_documents` to the local metadata pgvector instance, ran a real GitHub→MinIO ingestion (octocat/Hello-World) with CodeIndexRunner persisting vectors to `metadata.vector_index_entries` (deterministic embeddings), and added a `materialized_artifacts` registry + registry-aware CodeIndexRunner to track index runs (registry not yet fed by ingestion events).
+- 2025-12-22: Unblocked IndexArtifact by replaying staged envelopes with `vectorPayload`/`rawPayload`, unwrapping nested payloads for VectorProfile normalizers, defaulting KV checkpoints to store-core on 9099, and adding Go tests for staging replay + normalization (GOCACHE=/tmp/go-cache go test ./internal/activities).
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

exec
/bin/zsh -lc "sed -n '1,120p' sync/STATE.md" in /Users/rishikeshkumar/Development/Nucleus succeeded in 99ms:
# STATE SYNC (auto-updated)

## Focus Feature
code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)

## Last Run
- slug: materialized-registry-and-index-trigger-v1
- status: success
- duration: ~0.9h
- tests: pnpm --filter @apps/metadata-api test:brain (green)
- commits: none
- decisions: 2
- next_step: apply the materialized_artifacts migration/client to environments and extend profile mappings as new source families adopt the registry

## Global Queue
TODAY:
- 
NEXT:
- 
LATER:

thinking
**Preparing patch for Focus and Last Run updates**
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..9bd44f2e2436b24b8aab842302ca0002a423d5dc
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:    "raw",
+			EntityKind:    "raw.code.file_chunk",
+			Payload:       map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..8ef359bc5acb56d4964366103fade2602527fac8
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -149,18 +150,20 @@
 				}
 			}
 			it.current = map[string]any{
-				"recordKind":   recordKind,
-				"entityKind":   entityKind,
-				"payload":      payload,
-				"source":       env.Source,
-				"tenantId":     env.TenantID,
-				"projectKey":   env.ProjectKey,
-				"observedAt":   env.ObservedAt,
-				"stageRef":     it.stageRef,
-				"sliceId":      it.sliceID,
-				"batchRef":     it.batchRefs[it.batchIdx-1],
-				"recordOffset": offset,
-				"mapperKey":    mapperKey,
+				"recordKind":    recordKind,
+				"entityKind":    entityKind,
+				"payload":       payload,
+				"rawPayload":    rawPayload,
+				"vectorPayload": env.VectorPayload,
+				"source":        env.Source,
+				"tenantId":      env.TenantID,
+				"projectKey":    env.ProjectKey,
+				"observedAt":    env.ObservedAt,
+				"stageRef":      it.stageRef,
+				"sliceId":       it.sliceID,
+				"batchRef":      it.batchRefs[it.batchIdx-1],
+				"recordOffset":  offset,
+				"mapperKey":     mapperKey,
 			}
 			return true
 		}
diff --git a/runs/code-vector-profile-and-indexing-v1/DECISIONS.md b/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
index 29c1ccbf79cbaa94d2475978d1fdb5b8fec28620..2cca30268af052a903fa1c11ac33ee194f6fd081
--- a/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
+++ b/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
@@ -2,3 +2,4 @@
 - Canonical code profile `code.github.v1` normalizes projectKey to lower-case `{owner}/{repo}`, entityKind=`code.file_chunk`, profileKind=`code`, sourceFamily=`github`, docId=`code:github:{tenantId}:{projectKey}:{path}:{sha}:{chunkIndex}`. Content comes from `payload.text` truncated to 20k chars; attributes capture path/sha/chunkIndex/language/truncated flag. Missing tenantId/projectKey/profileKind fail closed with E_MISSING_CANONICAL_KEYS. Profile is also seeded into `vector_index_profiles` with text_source field hints.
 - Deterministic embeddings use a fake provider that hashes text to a one-hot 1536-dim vector (no external calls) for tests and default offline runs.
 - Index runs read MinIO-style JSONL.GZ envelopes from a dataset prefix (supports `minio://bucket/prefix`) via filesystem, streaming/decompressing lines; optional `ingestionRunId` filters to `run=<id>` segments (basePrefix defaults to `sink`). Errors classify as E_SOURCE_READ_FAILED, E_INVALID_RECORD, E_MISSING_CANONICAL_KEYS, E_EMBEDDING_FAILED, or E_VECTOR_DB_FAILED. Oversized text is truncated; malformed/missing-key records are skipped and surfaced as errors, with run status set to SUCCEEDED_WITH_ERRORS when any non-fatal errors occur.
+- Staging replay now surfaces `vectorPayload` and `rawPayload` to IndexArtifact; normalization unwraps nested payloads so VectorProfile normalizers see original records, and KV checkpoint client defaults to VECTOR_GRPC_ADDR/9099 (falling back from KV/UCL) to avoid checkpoint timeouts when store-core runs on 9099.
diff --git a/runs/code-vector-profile-and-indexing-v1/TODO.md b/runs/code-vector-profile-and-indexing-v1/TODO.md
index 0bada439bd1de49e1a0913caf5f994a74d225413..c08b43dcc387e3fa061794cd5c7cfabb63562350
--- a/runs/code-vector-profile-and-indexing-v1/TODO.md
+++ b/runs/code-vector-profile-and-indexing-v1/TODO.md
@@ -1,5 +1,5 @@
 - [x] Implement Endpoint-Driven Vector Normalization (VectorProfileProvider, GitHub connector, Staging integration).
 - [x] Verify Staging (vectorPayload IS present in MinIO).
-- [ ] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
+- [x] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
 - [ ] Rerun code indexing with the production embedding provider once access is available.
 - [ ] Feed materialized_artifacts registry from sink completion events.
diff --git a/stories/code-vector-profile-and-indexing-v1/STORY.md b/stories/code-vector-profile-and-indexing-v1/STORY.md
index 83311a64dc7bd20787734b76ff509d51483b20bd..a62438295091ba6b53d561e5e97616b4627f29bc
--- a/stories/code-vector-profile-and-indexing-v1/STORY.md
+++ b/stories/code-vector-profile-and-indexing-v1/STORY.md
@@ -2,3 +2,4 @@
 
 - 2025-12-13: Extended pgvector-backed `vector_index_entries` with docId/source metadata and seeded `code.github.v1`, implemented the code index runner/normalizer with deterministic embeddings, MinIO JSONL.GZ reading, and ingestionRunId filtering, and wrote AC1–AC4 tests; brain test suite currently blocked locally by missing METADATA_DATABASE_URL/Postgres.
 - 2025-12-14: Pivoted to a single vector store (removed vector_documents), added in-memory profile/vector stores for offline runs, fixed the code index runner API, reran `pnpm --filter @apps/metadata-api test:brain` successfully with deterministic embeddings, applied migration `20251213195500_code_vector_documents` to the local metadata pgvector instance, ran a real GitHub→MinIO ingestion (octocat/Hello-World) with CodeIndexRunner persisting vectors to `metadata.vector_index_entries` (deterministic embeddings), and added a `materialized_artifacts` registry + registry-aware CodeIndexRunner to track index runs (registry not yet fed by ingestion events).
+- 2025-12-22: Unblocked IndexArtifact by replaying staged envelopes with `vectorPayload`/`rawPayload`, unwrapping nested payloads for VectorProfile normalizers, defaulting KV checkpoints to store-core on 9099, and adding Go tests for staging replay + normalization (GOCACHE=/tmp/go-cache go test ./internal/activities).
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..010db1d0f9cc51ba8975e7407368e20cea86327e
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,7 +1,7 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
 
 ## Last Run
 - slug: materialized-registry-and-index-trigger-v1

file update
M /Users/rishikeshkumar/Development/Nucleus/sync/STATE.md
@@ -3,12 +3,12 @@
 ## Focus Feature
-code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T13:23Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T14:06Z)
 
 ## Last Run
-- slug: materialized-registry-and-index-trigger-v1
-- status: success
-- duration: ~0.9h
-- tests: pnpm --filter @apps/metadata-api test:brain (green)
+- slug: code-vector-profile-and-indexing-v1
+- status: in-progress
+- duration: ~0.8h
+- tests: GOCACHE=/tmp/go-cache go test ./internal/activities (green)
 - commits: none
-- decisions: 2
-- next_step: apply the materialized_artifacts migration/client to environments and extend profile mappings as new source families adopt the registry
+- decisions: 1
+- next_step: validate IndexArtifact against store-core/minio with production embeddings and rerun indexing if needed
 
apply_patch(auto_approved=true) exited 0 in 83ms:
Success. Updated the following files:
M sync/STATE.md
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..9bd44f2e2436b24b8aab842302ca0002a423d5dc
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:    "raw",
+			EntityKind:    "raw.code.file_chunk",
+			Payload:       map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..8ef359bc5acb56d4964366103fade2602527fac8
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -149,18 +150,20 @@
 				}
 			}
 			it.current = map[string]any{
-				"recordKind":   recordKind,
-				"entityKind":   entityKind,
-				"payload":      payload,
-				"source":       env.Source,
-				"tenantId":     env.TenantID,
-				"projectKey":   env.ProjectKey,
-				"observedAt":   env.ObservedAt,
-				"stageRef":     it.stageRef,
-				"sliceId":      it.sliceID,
-				"batchRef":     it.batchRefs[it.batchIdx-1],
-				"recordOffset": offset,
-				"mapperKey":    mapperKey,
+				"recordKind":    recordKind,
+				"entityKind":    entityKind,
+				"payload":       payload,
+				"rawPayload":    rawPayload,
+				"vectorPayload": env.VectorPayload,
+				"source":        env.Source,
+				"tenantId":      env.TenantID,
+				"projectKey":    env.ProjectKey,
+				"observedAt":    env.ObservedAt,
+				"stageRef":      it.stageRef,
+				"sliceId":       it.sliceID,
+				"batchRef":      it.batchRefs[it.batchIdx-1],
+				"recordOffset":  offset,
+				"mapperKey":     mapperKey,
 			}
 			return true
 		}
diff --git a/runs/code-vector-profile-and-indexing-v1/DECISIONS.md b/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
index 29c1ccbf79cbaa94d2475978d1fdb5b8fec28620..2cca30268af052a903fa1c11ac33ee194f6fd081
--- a/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
+++ b/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
@@ -2,3 +2,4 @@
 - Canonical code profile `code.github.v1` normalizes projectKey to lower-case `{owner}/{repo}`, entityKind=`code.file_chunk`, profileKind=`code`, sourceFamily=`github`, docId=`code:github:{tenantId}:{projectKey}:{path}:{sha}:{chunkIndex}`. Content comes from `payload.text` truncated to 20k chars; attributes capture path/sha/chunkIndex/language/truncated flag. Missing tenantId/projectKey/profileKind fail closed with E_MISSING_CANONICAL_KEYS. Profile is also seeded into `vector_index_profiles` with text_source field hints.
 - Deterministic embeddings use a fake provider that hashes text to a one-hot 1536-dim vector (no external calls) for tests and default offline runs.
 - Index runs read MinIO-style JSONL.GZ envelopes from a dataset prefix (supports `minio://bucket/prefix`) via filesystem, streaming/decompressing lines; optional `ingestionRunId` filters to `run=<id>` segments (basePrefix defaults to `sink`). Errors classify as E_SOURCE_READ_FAILED, E_INVALID_RECORD, E_MISSING_CANONICAL_KEYS, E_EMBEDDING_FAILED, or E_VECTOR_DB_FAILED. Oversized text is truncated; malformed/missing-key records are skipped and surfaced as errors, with run status set to SUCCEEDED_WITH_ERRORS when any non-fatal errors occur.
+- Staging replay now surfaces `vectorPayload` and `rawPayload` to IndexArtifact; normalization unwraps nested payloads so VectorProfile normalizers see original records, and KV checkpoint client defaults to VECTOR_GRPC_ADDR/9099 (falling back from KV/UCL) to avoid checkpoint timeouts when store-core runs on 9099.
diff --git a/runs/code-vector-profile-and-indexing-v1/TODO.md b/runs/code-vector-profile-and-indexing-v1/TODO.md
index 0bada439bd1de49e1a0913caf5f994a74d225413..c08b43dcc387e3fa061794cd5c7cfabb63562350
--- a/runs/code-vector-profile-and-indexing-v1/TODO.md
+++ b/runs/code-vector-profile-and-indexing-v1/TODO.md
@@ -1,5 +1,5 @@
 - [x] Implement Endpoint-Driven Vector Normalization (VectorProfileProvider, GitHub connector, Staging integration).
 - [x] Verify Staging (vectorPayload IS present in MinIO).
-- [ ] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
+- [x] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
 - [ ] Rerun code indexing with the production embedding provider once access is available.
 - [ ] Feed materialized_artifacts registry from sink completion events.
diff --git a/stories/code-vector-profile-and-indexing-v1/STORY.md b/stories/code-vector-profile-and-indexing-v1/STORY.md
index 83311a64dc7bd20787734b76ff509d51483b20bd..a62438295091ba6b53d561e5e97616b4627f29bc
--- a/stories/code-vector-profile-and-indexing-v1/STORY.md
+++ b/stories/code-vector-profile-and-indexing-v1/STORY.md
@@ -2,3 +2,4 @@
 
 - 2025-12-13: Extended pgvector-backed `vector_index_entries` with docId/source metadata and seeded `code.github.v1`, implemented the code index runner/normalizer with deterministic embeddings, MinIO JSONL.GZ reading, and ingestionRunId filtering, and wrote AC1–AC4 tests; brain test suite currently blocked locally by missing METADATA_DATABASE_URL/Postgres.
 - 2025-12-14: Pivoted to a single vector store (removed vector_documents), added in-memory profile/vector stores for offline runs, fixed the code index runner API, reran `pnpm --filter @apps/metadata-api test:brain` successfully with deterministic embeddings, applied migration `20251213195500_code_vector_documents` to the local metadata pgvector instance, ran a real GitHub→MinIO ingestion (octocat/Hello-World) with CodeIndexRunner persisting vectors to `metadata.vector_index_entries` (deterministic embeddings), and added a `materialized_artifacts` registry + registry-aware CodeIndexRunner to track index runs (registry not yet fed by ingestion events).
+- 2025-12-22: Unblocked IndexArtifact by replaying staged envelopes with `vectorPayload`/`rawPayload`, unwrapping nested payloads for VectorProfile normalizers, defaulting KV checkpoints to store-core on 9099, and adding Go tests for staging replay + normalization (GOCACHE=/tmp/go-cache go test ./internal/activities).
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..85282f05bff35865de190b1e339df7cb4e7c012d
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,16 +1,16 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T14:06Z)
 
 ## Last Run
-- slug: materialized-registry-and-index-trigger-v1
-- status: success
-- duration: ~0.9h
-- tests: pnpm --filter @apps/metadata-api test:brain (green)
+- slug: code-vector-profile-and-indexing-v1
+- status: in-progress
+- duration: ~0.8h
+- tests: GOCACHE=/tmp/go-cache go test ./internal/activities (green)
 - commits: none
-- decisions: 2
-- next_step: apply the materialized_artifacts migration/client to environments and extend profile mappings as new source families adopt the registry
+- decisions: 1
+- next_step: validate IndexArtifact against store-core/minio with production embeddings and rerun indexing if needed
 
 ## Global Queue
 TODAY:


thinking
**Inserting new event near top**

I'm adding the new event line right after the blank line following the Global Queue section to appear at the top of the existing event list.
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..9bd44f2e2436b24b8aab842302ca0002a423d5dc
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:    "raw",
+			EntityKind:    "raw.code.file_chunk",
+			Payload:       map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..8ef359bc5acb56d4964366103fade2602527fac8
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -149,18 +150,20 @@
 				}
 			}
 			it.current = map[string]any{
-				"recordKind":   recordKind,
-				"entityKind":   entityKind,
-				"payload":      payload,
-				"source":       env.Source,
-				"tenantId":     env.TenantID,
-				"projectKey":   env.ProjectKey,
-				"observedAt":   env.ObservedAt,
-				"stageRef":     it.stageRef,
-				"sliceId":      it.sliceID,
-				"batchRef":     it.batchRefs[it.batchIdx-1],
-				"recordOffset": offset,
-				"mapperKey":    mapperKey,
+				"recordKind":    recordKind,
+				"entityKind":    entityKind,
+				"payload":       payload,
+				"rawPayload":    rawPayload,
+				"vectorPayload": env.VectorPayload,
+				"source":        env.Source,
+				"tenantId":      env.TenantID,
+				"projectKey":    env.ProjectKey,
+				"observedAt":    env.ObservedAt,
+				"stageRef":      it.stageRef,
+				"sliceId":       it.sliceID,
+				"batchRef":      it.batchRefs[it.batchIdx-1],
+				"recordOffset":  offset,
+				"mapperKey":     mapperKey,
 			}
 			return true
 		}
diff --git a/runs/code-vector-profile-and-indexing-v1/DECISIONS.md b/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
index 29c1ccbf79cbaa94d2475978d1fdb5b8fec28620..2cca30268af052a903fa1c11ac33ee194f6fd081
--- a/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
+++ b/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
@@ -2,3 +2,4 @@
 - Canonical code profile `code.github.v1` normalizes projectKey to lower-case `{owner}/{repo}`, entityKind=`code.file_chunk`, profileKind=`code`, sourceFamily=`github`, docId=`code:github:{tenantId}:{projectKey}:{path}:{sha}:{chunkIndex}`. Content comes from `payload.text` truncated to 20k chars; attributes capture path/sha/chunkIndex/language/truncated flag. Missing tenantId/projectKey/profileKind fail closed with E_MISSING_CANONICAL_KEYS. Profile is also seeded into `vector_index_profiles` with text_source field hints.
 - Deterministic embeddings use a fake provider that hashes text to a one-hot 1536-dim vector (no external calls) for tests and default offline runs.
 - Index runs read MinIO-style JSONL.GZ envelopes from a dataset prefix (supports `minio://bucket/prefix`) via filesystem, streaming/decompressing lines; optional `ingestionRunId` filters to `run=<id>` segments (basePrefix defaults to `sink`). Errors classify as E_SOURCE_READ_FAILED, E_INVALID_RECORD, E_MISSING_CANONICAL_KEYS, E_EMBEDDING_FAILED, or E_VECTOR_DB_FAILED. Oversized text is truncated; malformed/missing-key records are skipped and surfaced as errors, with run status set to SUCCEEDED_WITH_ERRORS when any non-fatal errors occur.
+- Staging replay now surfaces `vectorPayload` and `rawPayload` to IndexArtifact; normalization unwraps nested payloads so VectorProfile normalizers see original records, and KV checkpoint client defaults to VECTOR_GRPC_ADDR/9099 (falling back from KV/UCL) to avoid checkpoint timeouts when store-core runs on 9099.
diff --git a/runs/code-vector-profile-and-indexing-v1/TODO.md b/runs/code-vector-profile-and-indexing-v1/TODO.md
index 0bada439bd1de49e1a0913caf5f994a74d225413..c08b43dcc387e3fa061794cd5c7cfabb63562350
--- a/runs/code-vector-profile-and-indexing-v1/TODO.md
+++ b/runs/code-vector-profile-and-indexing-v1/TODO.md
@@ -1,5 +1,5 @@
 - [x] Implement Endpoint-Driven Vector Normalization (VectorProfileProvider, GitHub connector, Staging integration).
 - [x] Verify Staging (vectorPayload IS present in MinIO).
-- [ ] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
+- [x] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
 - [ ] Rerun code indexing with the production embedding provider once access is available.
 - [ ] Feed materialized_artifacts registry from sink completion events.
diff --git a/stories/code-vector-profile-and-indexing-v1/STORY.md b/stories/code-vector-profile-and-indexing-v1/STORY.md
index 83311a64dc7bd20787734b76ff509d51483b20bd..a62438295091ba6b53d561e5e97616b4627f29bc
--- a/stories/code-vector-profile-and-indexing-v1/STORY.md
+++ b/stories/code-vector-profile-and-indexing-v1/STORY.md
@@ -2,3 +2,4 @@
 
 - 2025-12-13: Extended pgvector-backed `vector_index_entries` with docId/source metadata and seeded `code.github.v1`, implemented the code index runner/normalizer with deterministic embeddings, MinIO JSONL.GZ reading, and ingestionRunId filtering, and wrote AC1–AC4 tests; brain test suite currently blocked locally by missing METADATA_DATABASE_URL/Postgres.
 - 2025-12-14: Pivoted to a single vector store (removed vector_documents), added in-memory profile/vector stores for offline runs, fixed the code index runner API, reran `pnpm --filter @apps/metadata-api test:brain` successfully with deterministic embeddings, applied migration `20251213195500_code_vector_documents` to the local metadata pgvector instance, ran a real GitHub→MinIO ingestion (octocat/Hello-World) with CodeIndexRunner persisting vectors to `metadata.vector_index_entries` (deterministic embeddings), and added a `materialized_artifacts` registry + registry-aware CodeIndexRunner to track index runs (registry not yet fed by ingestion events).
+- 2025-12-22: Unblocked IndexArtifact by replaying staged envelopes with `vectorPayload`/`rawPayload`, unwrapping nested payloads for VectorProfile normalizers, defaulting KV checkpoints to store-core on 9099, and adding Go tests for staging replay + normalization (GOCACHE=/tmp/go-cache go test ./internal/activities).
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..85282f05bff35865de190b1e339df7cb4e7c012d
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,16 +1,16 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T14:06Z)
 
 ## Last Run
-- slug: materialized-registry-and-index-trigger-v1
-- status: success
-- duration: ~0.9h
-- tests: pnpm --filter @apps/metadata-api test:brain (green)
+- slug: code-vector-profile-and-indexing-v1
+- status: in-progress
+- duration: ~0.8h
+- tests: GOCACHE=/tmp/go-cache go test ./internal/activities (green)
 - commits: none
-- decisions: 2
-- next_step: apply the materialized_artifacts migration/client to environments and extend profile mappings as new source families adopt the registry
+- decisions: 1
+- next_step: validate IndexArtifact against store-core/minio with production embeddings and rerun indexing if needed
 
 ## Global Queue
 TODAY:

file update
M /Users/rishikeshkumar/Development/Nucleus/sync/STATE.md
@@ -22,2 +22,3 @@
 
+- 2025-12-22T14:06Z run in-progress (code-vector-profile-and-indexing-v1, staging replay exposes vectorPayload/rawPayload; go activities tests green)
 - 2025-12-14T12:59Z run success (materialized-registry-and-index-trigger-v1, registry/index trigger v1 landed; test:brain green)
apply_patch(auto_approved=true) exited 0 in 94ms:
Success. Updated the following files:
M sync/STATE.md
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..9bd44f2e2436b24b8aab842302ca0002a423d5dc
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:    "raw",
+			EntityKind:    "raw.code.file_chunk",
+			Payload:       map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..8ef359bc5acb56d4964366103fade2602527fac8
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -149,18 +150,20 @@
 				}
 			}
 			it.current = map[string]any{
-				"recordKind":   recordKind,
-				"entityKind":   entityKind,
-				"payload":      payload,
-				"source":       env.Source,
-				"tenantId":     env.TenantID,
-				"projectKey":   env.ProjectKey,
-				"observedAt":   env.ObservedAt,
-				"stageRef":     it.stageRef,
-				"sliceId":      it.sliceID,
-				"batchRef":     it.batchRefs[it.batchIdx-1],
-				"recordOffset": offset,
-				"mapperKey":    mapperKey,
+				"recordKind":    recordKind,
+				"entityKind":    entityKind,
+				"payload":       payload,
+				"rawPayload":    rawPayload,
+				"vectorPayload": env.VectorPayload,
+				"source":        env.Source,
+				"tenantId":      env.TenantID,
+				"projectKey":    env.ProjectKey,
+				"observedAt":    env.ObservedAt,
+				"stageRef":      it.stageRef,
+				"sliceId":       it.sliceID,
+				"batchRef":      it.batchRefs[it.batchIdx-1],
+				"recordOffset":  offset,
+				"mapperKey":     mapperKey,
 			}
 			return true
 		}
diff --git a/runs/code-vector-profile-and-indexing-v1/DECISIONS.md b/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
index 29c1ccbf79cbaa94d2475978d1fdb5b8fec28620..2cca30268af052a903fa1c11ac33ee194f6fd081
--- a/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
+++ b/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
@@ -2,3 +2,4 @@
 - Canonical code profile `code.github.v1` normalizes projectKey to lower-case `{owner}/{repo}`, entityKind=`code.file_chunk`, profileKind=`code`, sourceFamily=`github`, docId=`code:github:{tenantId}:{projectKey}:{path}:{sha}:{chunkIndex}`. Content comes from `payload.text` truncated to 20k chars; attributes capture path/sha/chunkIndex/language/truncated flag. Missing tenantId/projectKey/profileKind fail closed with E_MISSING_CANONICAL_KEYS. Profile is also seeded into `vector_index_profiles` with text_source field hints.
 - Deterministic embeddings use a fake provider that hashes text to a one-hot 1536-dim vector (no external calls) for tests and default offline runs.
 - Index runs read MinIO-style JSONL.GZ envelopes from a dataset prefix (supports `minio://bucket/prefix`) via filesystem, streaming/decompressing lines; optional `ingestionRunId` filters to `run=<id>` segments (basePrefix defaults to `sink`). Errors classify as E_SOURCE_READ_FAILED, E_INVALID_RECORD, E_MISSING_CANONICAL_KEYS, E_EMBEDDING_FAILED, or E_VECTOR_DB_FAILED. Oversized text is truncated; malformed/missing-key records are skipped and surfaced as errors, with run status set to SUCCEEDED_WITH_ERRORS when any non-fatal errors occur.
+- Staging replay now surfaces `vectorPayload` and `rawPayload` to IndexArtifact; normalization unwraps nested payloads so VectorProfile normalizers see original records, and KV checkpoint client defaults to VECTOR_GRPC_ADDR/9099 (falling back from KV/UCL) to avoid checkpoint timeouts when store-core runs on 9099.
diff --git a/runs/code-vector-profile-and-indexing-v1/TODO.md b/runs/code-vector-profile-and-indexing-v1/TODO.md
index 0bada439bd1de49e1a0913caf5f994a74d225413..c08b43dcc387e3fa061794cd5c7cfabb63562350
--- a/runs/code-vector-profile-and-indexing-v1/TODO.md
+++ b/runs/code-vector-profile-and-indexing-v1/TODO.md
@@ -1,5 +1,5 @@
 - [x] Implement Endpoint-Driven Vector Normalization (VectorProfileProvider, GitHub connector, Staging integration).
 - [x] Verify Staging (vectorPayload IS present in MinIO).
-- [ ] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
+- [x] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
 - [ ] Rerun code indexing with the production embedding provider once access is available.
 - [ ] Feed materialized_artifacts registry from sink completion events.
diff --git a/stories/code-vector-profile-and-indexing-v1/STORY.md b/stories/code-vector-profile-and-indexing-v1/STORY.md
index 83311a64dc7bd20787734b76ff509d51483b20bd..a62438295091ba6b53d561e5e97616b4627f29bc
--- a/stories/code-vector-profile-and-indexing-v1/STORY.md
+++ b/stories/code-vector-profile-and-indexing-v1/STORY.md
@@ -2,3 +2,4 @@
 
 - 2025-12-13: Extended pgvector-backed `vector_index_entries` with docId/source metadata and seeded `code.github.v1`, implemented the code index runner/normalizer with deterministic embeddings, MinIO JSONL.GZ reading, and ingestionRunId filtering, and wrote AC1–AC4 tests; brain test suite currently blocked locally by missing METADATA_DATABASE_URL/Postgres.
 - 2025-12-14: Pivoted to a single vector store (removed vector_documents), added in-memory profile/vector stores for offline runs, fixed the code index runner API, reran `pnpm --filter @apps/metadata-api test:brain` successfully with deterministic embeddings, applied migration `20251213195500_code_vector_documents` to the local metadata pgvector instance, ran a real GitHub→MinIO ingestion (octocat/Hello-World) with CodeIndexRunner persisting vectors to `metadata.vector_index_entries` (deterministic embeddings), and added a `materialized_artifacts` registry + registry-aware CodeIndexRunner to track index runs (registry not yet fed by ingestion events).
+- 2025-12-22: Unblocked IndexArtifact by replaying staged envelopes with `vectorPayload`/`rawPayload`, unwrapping nested payloads for VectorProfile normalizers, defaulting KV checkpoints to store-core on 9099, and adding Go tests for staging replay + normalization (GOCACHE=/tmp/go-cache go test ./internal/activities).
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..425bda9090e00f0426384cefeca60e1179cda01c
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,16 +1,16 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T14:06Z)
 
 ## Last Run
-- slug: materialized-registry-and-index-trigger-v1
-- status: success
-- duration: ~0.9h
-- tests: pnpm --filter @apps/metadata-api test:brain (green)
+- slug: code-vector-profile-and-indexing-v1
+- status: in-progress
+- duration: ~0.8h
+- tests: GOCACHE=/tmp/go-cache go test ./internal/activities (green)
 - commits: none
-- decisions: 2
-- next_step: apply the materialized_artifacts migration/client to environments and extend profile mappings as new source families adopt the registry
+- decisions: 1
+- next_step: validate IndexArtifact against store-core/minio with production embeddings and rerun indexing if needed
 
 ## Global Queue
 TODAY:
@@ -20,6 +20,7 @@
 LATER:
 - 
 
+- 2025-12-22T14:06Z run in-progress (code-vector-profile-and-indexing-v1, staging replay exposes vectorPayload/rawPayload; go activities tests green)
 - 2025-12-14T12:59Z run success (materialized-registry-and-index-trigger-v1, registry/index trigger v1 landed; test:brain green)
 - 2025-12-14T12:48Z run success (materialized-registry-and-index-trigger-v1, registry/index trigger v1 landed; test:brain green)
 - 2025-12-14T07:02Z run in-progress (code-vector-profile-and-indexing-v1, materialized_artifacts registry + registry-aware CodeIndexRunner; test:brain still green)


thinking
**Updating plan statuses**

I'll finalize the plan by marking the last step as completed using the update_plan function.
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..9bd44f2e2436b24b8aab842302ca0002a423d5dc
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:    "raw",
+			EntityKind:    "raw.code.file_chunk",
+			Payload:       map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..8ef359bc5acb56d4964366103fade2602527fac8
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -149,18 +150,20 @@
 				}
 			}
 			it.current = map[string]any{
-				"recordKind":   recordKind,
-				"entityKind":   entityKind,
-				"payload":      payload,
-				"source":       env.Source,
-				"tenantId":     env.TenantID,
-				"projectKey":   env.ProjectKey,
-				"observedAt":   env.ObservedAt,
-				"stageRef":     it.stageRef,
-				"sliceId":      it.sliceID,
-				"batchRef":     it.batchRefs[it.batchIdx-1],
-				"recordOffset": offset,
-				"mapperKey":    mapperKey,
+				"recordKind":    recordKind,
+				"entityKind":    entityKind,
+				"payload":       payload,
+				"rawPayload":    rawPayload,
+				"vectorPayload": env.VectorPayload,
+				"source":        env.Source,
+				"tenantId":      env.TenantID,
+				"projectKey":    env.ProjectKey,
+				"observedAt":    env.ObservedAt,
+				"stageRef":      it.stageRef,
+				"sliceId":       it.sliceID,
+				"batchRef":      it.batchRefs[it.batchIdx-1],
+				"recordOffset":  offset,
+				"mapperKey":     mapperKey,
 			}
 			return true
 		}
diff --git a/runs/code-vector-profile-and-indexing-v1/DECISIONS.md b/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
index 29c1ccbf79cbaa94d2475978d1fdb5b8fec28620..2cca30268af052a903fa1c11ac33ee194f6fd081
--- a/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
+++ b/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
@@ -2,3 +2,4 @@
 - Canonical code profile `code.github.v1` normalizes projectKey to lower-case `{owner}/{repo}`, entityKind=`code.file_chunk`, profileKind=`code`, sourceFamily=`github`, docId=`code:github:{tenantId}:{projectKey}:{path}:{sha}:{chunkIndex}`. Content comes from `payload.text` truncated to 20k chars; attributes capture path/sha/chunkIndex/language/truncated flag. Missing tenantId/projectKey/profileKind fail closed with E_MISSING_CANONICAL_KEYS. Profile is also seeded into `vector_index_profiles` with text_source field hints.
 - Deterministic embeddings use a fake provider that hashes text to a one-hot 1536-dim vector (no external calls) for tests and default offline runs.
 - Index runs read MinIO-style JSONL.GZ envelopes from a dataset prefix (supports `minio://bucket/prefix`) via filesystem, streaming/decompressing lines; optional `ingestionRunId` filters to `run=<id>` segments (basePrefix defaults to `sink`). Errors classify as E_SOURCE_READ_FAILED, E_INVALID_RECORD, E_MISSING_CANONICAL_KEYS, E_EMBEDDING_FAILED, or E_VECTOR_DB_FAILED. Oversized text is truncated; malformed/missing-key records are skipped and surfaced as errors, with run status set to SUCCEEDED_WITH_ERRORS when any non-fatal errors occur.
+- Staging replay now surfaces `vectorPayload` and `rawPayload` to IndexArtifact; normalization unwraps nested payloads so VectorProfile normalizers see original records, and KV checkpoint client defaults to VECTOR_GRPC_ADDR/9099 (falling back from KV/UCL) to avoid checkpoint timeouts when store-core runs on 9099.
diff --git a/runs/code-vector-profile-and-indexing-v1/TODO.md b/runs/code-vector-profile-and-indexing-v1/TODO.md
index 0bada439bd1de49e1a0913caf5f994a74d225413..c08b43dcc387e3fa061794cd5c7cfabb63562350
--- a/runs/code-vector-profile-and-indexing-v1/TODO.md
+++ b/runs/code-vector-profile-and-indexing-v1/TODO.md
@@ -1,5 +1,5 @@
 - [x] Implement Endpoint-Driven Vector Normalization (VectorProfileProvider, GitHub connector, Staging integration).
 - [x] Verify Staging (vectorPayload IS present in MinIO).
-- [ ] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
+- [x] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
 - [ ] Rerun code indexing with the production embedding provider once access is available.
 - [ ] Feed materialized_artifacts registry from sink completion events.
diff --git a/stories/code-vector-profile-and-indexing-v1/STORY.md b/stories/code-vector-profile-and-indexing-v1/STORY.md
index 83311a64dc7bd20787734b76ff509d51483b20bd..a62438295091ba6b53d561e5e97616b4627f29bc
--- a/stories/code-vector-profile-and-indexing-v1/STORY.md
+++ b/stories/code-vector-profile-and-indexing-v1/STORY.md
@@ -2,3 +2,4 @@
 
 - 2025-12-13: Extended pgvector-backed `vector_index_entries` with docId/source metadata and seeded `code.github.v1`, implemented the code index runner/normalizer with deterministic embeddings, MinIO JSONL.GZ reading, and ingestionRunId filtering, and wrote AC1–AC4 tests; brain test suite currently blocked locally by missing METADATA_DATABASE_URL/Postgres.
 - 2025-12-14: Pivoted to a single vector store (removed vector_documents), added in-memory profile/vector stores for offline runs, fixed the code index runner API, reran `pnpm --filter @apps/metadata-api test:brain` successfully with deterministic embeddings, applied migration `20251213195500_code_vector_documents` to the local metadata pgvector instance, ran a real GitHub→MinIO ingestion (octocat/Hello-World) with CodeIndexRunner persisting vectors to `metadata.vector_index_entries` (deterministic embeddings), and added a `materialized_artifacts` registry + registry-aware CodeIndexRunner to track index runs (registry not yet fed by ingestion events).
+- 2025-12-22: Unblocked IndexArtifact by replaying staged envelopes with `vectorPayload`/`rawPayload`, unwrapping nested payloads for VectorProfile normalizers, defaulting KV checkpoints to store-core on 9099, and adding Go tests for staging replay + normalization (GOCACHE=/tmp/go-cache go test ./internal/activities).
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..425bda9090e00f0426384cefeca60e1179cda01c
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,16 +1,16 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T14:06Z)
 
 ## Last Run
-- slug: materialized-registry-and-index-trigger-v1
-- status: success
-- duration: ~0.9h
-- tests: pnpm --filter @apps/metadata-api test:brain (green)
+- slug: code-vector-profile-and-indexing-v1
+- status: in-progress
+- duration: ~0.8h
+- tests: GOCACHE=/tmp/go-cache go test ./internal/activities (green)
 - commits: none
-- decisions: 2
-- next_step: apply the materialized_artifacts migration/client to environments and extend profile mappings as new source families adopt the registry
+- decisions: 1
+- next_step: validate IndexArtifact against store-core/minio with production embeddings and rerun indexing if needed
 
 ## Global Queue
 TODAY:
@@ -20,6 +20,7 @@
 LATER:
 - 
 
+- 2025-12-22T14:06Z run in-progress (code-vector-profile-and-indexing-v1, staging replay exposes vectorPayload/rawPayload; go activities tests green)
 - 2025-12-14T12:59Z run success (materialized-registry-and-index-trigger-v1, registry/index trigger v1 landed; test:brain green)
 - 2025-12-14T12:48Z run success (materialized-registry-and-index-trigger-v1, registry/index trigger v1 landed; test:brain green)
 - 2025-12-14T07:02Z run in-progress (code-vector-profile-and-indexing-v1, materialized_artifacts registry + registry-aware CodeIndexRunner; test:brain still green)

Plan update
Artifacts updated (LOG/TODO/STATE/STORY) after adding tests and running go test.
  ✓ Propagate vectorPayload + raw payload through staging iterator and normalization so IndexArtifact can upsert vectors
  ✓ Harden checkpoint KV client defaults to hit store-core on 9099
  ✓ Add/execute targeted tests to cover staging vector payload flow; update run artifacts (TODO/STATE/LOG)

thinking
**Verifying log append correctness**
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..9bd44f2e2436b24b8aab842302ca0002a423d5dc
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:    "raw",
+			EntityKind:    "raw.code.file_chunk",
+			Payload:       map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..8ef359bc5acb56d4964366103fade2602527fac8
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -149,18 +150,20 @@
 				}
 			}
 			it.current = map[string]any{
-				"recordKind":   recordKind,
-				"entityKind":   entityKind,
-				"payload":      payload,
-				"source":       env.Source,
-				"tenantId":     env.TenantID,
-				"projectKey":   env.ProjectKey,
-				"observedAt":   env.ObservedAt,
-				"stageRef":     it.stageRef,
-				"sliceId":      it.sliceID,
-				"batchRef":     it.batchRefs[it.batchIdx-1],
-				"recordOffset": offset,
-				"mapperKey":    mapperKey,
+				"recordKind":    recordKind,
+				"entityKind":    entityKind,
+				"payload":       payload,
+				"rawPayload":    rawPayload,
+				"vectorPayload": env.VectorPayload,
+				"source":        env.Source,
+				"tenantId":      env.TenantID,
+				"projectKey":    env.ProjectKey,
+				"observedAt":    env.ObservedAt,
+				"stageRef":      it.stageRef,
+				"sliceId":       it.sliceID,
+				"batchRef":      it.batchRefs[it.batchIdx-1],
+				"recordOffset":  offset,
+				"mapperKey":     mapperKey,
 			}
 			return true
 		}
diff --git a/runs/code-vector-profile-and-indexing-v1/DECISIONS.md b/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
index 29c1ccbf79cbaa94d2475978d1fdb5b8fec28620..2cca30268af052a903fa1c11ac33ee194f6fd081
--- a/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
+++ b/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
@@ -2,3 +2,4 @@
 - Canonical code profile `code.github.v1` normalizes projectKey to lower-case `{owner}/{repo}`, entityKind=`code.file_chunk`, profileKind=`code`, sourceFamily=`github`, docId=`code:github:{tenantId}:{projectKey}:{path}:{sha}:{chunkIndex}`. Content comes from `payload.text` truncated to 20k chars; attributes capture path/sha/chunkIndex/language/truncated flag. Missing tenantId/projectKey/profileKind fail closed with E_MISSING_CANONICAL_KEYS. Profile is also seeded into `vector_index_profiles` with text_source field hints.
 - Deterministic embeddings use a fake provider that hashes text to a one-hot 1536-dim vector (no external calls) for tests and default offline runs.
 - Index runs read MinIO-style JSONL.GZ envelopes from a dataset prefix (supports `minio://bucket/prefix`) via filesystem, streaming/decompressing lines; optional `ingestionRunId` filters to `run=<id>` segments (basePrefix defaults to `sink`). Errors classify as E_SOURCE_READ_FAILED, E_INVALID_RECORD, E_MISSING_CANONICAL_KEYS, E_EMBEDDING_FAILED, or E_VECTOR_DB_FAILED. Oversized text is truncated; malformed/missing-key records are skipped and surfaced as errors, with run status set to SUCCEEDED_WITH_ERRORS when any non-fatal errors occur.
+- Staging replay now surfaces `vectorPayload` and `rawPayload` to IndexArtifact; normalization unwraps nested payloads so VectorProfile normalizers see original records, and KV checkpoint client defaults to VECTOR_GRPC_ADDR/9099 (falling back from KV/UCL) to avoid checkpoint timeouts when store-core runs on 9099.
diff --git a/runs/code-vector-profile-and-indexing-v1/TODO.md b/runs/code-vector-profile-and-indexing-v1/TODO.md
index 0bada439bd1de49e1a0913caf5f994a74d225413..c08b43dcc387e3fa061794cd5c7cfabb63562350
--- a/runs/code-vector-profile-and-indexing-v1/TODO.md
+++ b/runs/code-vector-profile-and-indexing-v1/TODO.md
@@ -1,5 +1,5 @@
 - [x] Implement Endpoint-Driven Vector Normalization (VectorProfileProvider, GitHub connector, Staging integration).
 - [x] Verify Staging (vectorPayload IS present in MinIO).
-- [ ] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
+- [x] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
 - [ ] Rerun code indexing with the production embedding provider once access is available.
 - [ ] Feed materialized_artifacts registry from sink completion events.
diff --git a/stories/code-vector-profile-and-indexing-v1/STORY.md b/stories/code-vector-profile-and-indexing-v1/STORY.md
index 83311a64dc7bd20787734b76ff509d51483b20bd..a62438295091ba6b53d561e5e97616b4627f29bc
--- a/stories/code-vector-profile-and-indexing-v1/STORY.md
+++ b/stories/code-vector-profile-and-indexing-v1/STORY.md
@@ -2,3 +2,4 @@
 
 - 2025-12-13: Extended pgvector-backed `vector_index_entries` with docId/source metadata and seeded `code.github.v1`, implemented the code index runner/normalizer with deterministic embeddings, MinIO JSONL.GZ reading, and ingestionRunId filtering, and wrote AC1–AC4 tests; brain test suite currently blocked locally by missing METADATA_DATABASE_URL/Postgres.
 - 2025-12-14: Pivoted to a single vector store (removed vector_documents), added in-memory profile/vector stores for offline runs, fixed the code index runner API, reran `pnpm --filter @apps/metadata-api test:brain` successfully with deterministic embeddings, applied migration `20251213195500_code_vector_documents` to the local metadata pgvector instance, ran a real GitHub→MinIO ingestion (octocat/Hello-World) with CodeIndexRunner persisting vectors to `metadata.vector_index_entries` (deterministic embeddings), and added a `materialized_artifacts` registry + registry-aware CodeIndexRunner to track index runs (registry not yet fed by ingestion events).
+- 2025-12-22: Unblocked IndexArtifact by replaying staged envelopes with `vectorPayload`/`rawPayload`, unwrapping nested payloads for VectorProfile normalizers, defaulting KV checkpoints to store-core on 9099, and adding Go tests for staging replay + normalization (GOCACHE=/tmp/go-cache go test ./internal/activities).
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..425bda9090e00f0426384cefeca60e1179cda01c
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,16 +1,16 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T14:06Z)
 
 ## Last Run
-- slug: materialized-registry-and-index-trigger-v1
-- status: success
-- duration: ~0.9h
-- tests: pnpm --filter @apps/metadata-api test:brain (green)
+- slug: code-vector-profile-and-indexing-v1
+- status: in-progress
+- duration: ~0.8h
+- tests: GOCACHE=/tmp/go-cache go test ./internal/activities (green)
 - commits: none
-- decisions: 2
-- next_step: apply the materialized_artifacts migration/client to environments and extend profile mappings as new source families adopt the registry
+- decisions: 1
+- next_step: validate IndexArtifact against store-core/minio with production embeddings and rerun indexing if needed
 
 ## Global Queue
 TODAY:
@@ -20,6 +20,7 @@
 LATER:
 - 
 
+- 2025-12-22T14:06Z run in-progress (code-vector-profile-and-indexing-v1, staging replay exposes vectorPayload/rawPayload; go activities tests green)
 - 2025-12-14T12:59Z run success (materialized-registry-and-index-trigger-v1, registry/index trigger v1 landed; test:brain green)
 - 2025-12-14T12:48Z run success (materialized-registry-and-index-trigger-v1, registry/index trigger v1 landed; test:brain green)
 - 2025-12-14T07:02Z run in-progress (code-vector-profile-and-indexing-v1, materialized_artifacts registry + registry-aware CodeIndexRunner; test:brain still green)

exec
/bin/zsh -lc 'tail -n 10 runs/code-vector-profile-and-indexing-v1/LOG.md' in /Users/rishikeshkumar/Development/Nucleus succeeded in 99ms:
2025-12-13T19:53:14Z | start | done: booted code-vector-profile-and-indexing-v1 run, confirmed intent/spec/acceptance files exist | next: read INTENT/SPEC/ACCEPTANCE plus PLAN/TODO/LOG tail for resume context | risks: none yet
2025-12-13T20:33:59Z | heartbeat | done: added vector_documents schema/store and seeded code.github.v1 profile; built code index runner with ingestionRunId filtering, deterministic embeddings, and MinIO JSONL.GZ normalization; added AC1–AC4 tests and reran pnpm --filter @apps/metadata-api test:brain (fails: METADATA_DATABASE_URL missing) | next: apply the new migration, configure METADATA_DATABASE_URL/pgvector Postgres, rerun brain tests, and validate index-run output against acceptance | risks: Postgres/pgvector env absent and migrations not yet applied; test suite blocked until METADATA_DATABASE_URL is set
2025-12-14T05:07:40Z | heartbeat | done: consolidated code/doc/work onto vector_index_entries (doc_id + metadata), removed vector_documents; added in-memory profile/vector stores for offline tests; fixed code index runner to use upsertEntries and reran pnpm --filter @apps/metadata-api test:brain (all green with deterministic embeddings) | next: apply migration 20251213195500_code_vector_documents to pgvector DB, hook production embedding provider, and run db-backed validation when infra is available | risks: prod DB not yet migrated; embeddings still use fake provider in tests only
2025-12-14T05:27:43Z | heartbeat | done: ran prisma migrate deploy with METADATA_DATABASE_URL (schema metadata @ localhost:5434) and applied 20251213195500_code_vector_documents | next: validate code indexer against real MinIO + production embeddings, then update TODO/state once prod DBs are migrated | risks: none locally; prod envs still need migration + real embedding provider
2025-12-14T06:17:45Z | heartbeat | done: ingested public repo (octocat/Hello-World) via real GitHub connector into MinIO localhost:9000 (ucl-staging) and ran CodeIndexRunner against sink raw.code.file_chunk using deterministic embeddings; vectors landed in metadata.vector_index_entries (docId/code profile) | next: rerun indexing with production embedding provider when available; keep MinIO artifacts for reuse | risks: current vectors use deterministic embeddings, not production model
2025-12-14T07:02:10Z | heartbeat | done: added materialized_artifacts registry + Prisma registry client; CodeIndexRunner now records run status (IN_PROGRESS/SUCCEEDED/FAILED) into registry (default no-op), fixed pg upsert placeholder, migrations applied and test:brain still green | next: wire ingestion sink completion to create registry entries and run indexer with production embeddings | risks: registry not yet fed by ingestion pipeline; embeddings still deterministic in local runs
2025-12-22T18:55:00Z | handoff | done: Implemented endpoint-driven vector normalization (ucl-core: VectorProfileProvider, GitHub connector, staging); brain-core: IndexArtifact reads pre-normalized vectorPayload. Verified staging contains valid vectorPayload. | next: DEBUG why vector_entries remains empty despite IndexArtifact running (logs: checkpoint-save-failed/deadline exceeded). | risks: persistent timeout or silent failure in brain block db persistence.
2025-12-22T13:23:29Z | resume | done: resumed code-vector-profile-and-indexing-v1 run; reviewed INTENT/SPEC/ACCEPTANCE, PLAN, TODO, LOG tail | next: investigate IndexArtifact pipeline/vector_entries empty issue; inspect code + logs and run targeted brain tests | risks: db persistence path or long-running checkpoint failures could block indexing
2025-12-22T14:06:27Z | heartbeat | done: propagated vectorPayload/rawPayload through staging iterator, normalized nested payload fallback, hardened KV gRPC default to 9099, and added Go tests (streamFromStaging, normalization); GOCACHE=/tmp/go-cache go test ./internal/activities passed | next: update TODO/STATE/story, then validate indexing on real env when available | risks: production indexing still unverified against live store-core/minio stack

thinking
**Summarizing code changes and next steps**

I'm preparing a final summary that highlights key modifications across multiple files, including enhanced payload handling in the staging iterator and normalization, fallback port updates, and added tests. The summary will reference specific file paths like `platform/brain-core/internal/activities/stream.go` and test commands such as `GOCACHE=/tmp/go-cache go test ./internal/activities`. It will also mention outstanding risks like unvalidated production indexing and propose clear next steps.

thinking
**Adding fallback to prevent checkpoint timeouts**

Tests verify staging replay vectorPayload and normalization with fallback to the default vector gRPC address; ran targeted tests confirming stability. Next, I'll suggest running IndexArtifact on staging with production embeddings and mention updating TODO, STATE, and STORY accordingly.
codex
Staging replay now preserves the pre-normalized vector payload and unwraps nested payloads so IndexArtifact can actually upsert vectors; KV checkpoint client defaults to the store-core address to avoid timeouts, and tests cover the new flow.

- `platform/brain-core/internal/activities/stream.go`: staging iterator emits `vectorPayload` and `rawPayload` for each record so the indexer can use pre-normalized vectors.
- `platform/brain-core/internal/activities/normalize.go`: unwraps nested `payload.payload` shapes and falls back to `rawPayload`, keeping tenant/project/profile fallbacks intact.
- `platform/brain-core/internal/activities/activities.go`: vectorPayload path now applies project/tenant/source fallbacks for canonical metadata.
- `platform/brain-core/internal/activities/kv_client.go`: KV client now falls back to VECTOR_GRPC_ADDR/STORE_VECTOR_GRPC_ADDR/9099 to match store-core, preventing checkpoint save deadlines.
- `platform/brain-core/internal/activities/activities_test.go`: added coverage for staged vectorPayload replay and normalization unwrapping.

Tests: `GOCACHE=/tmp/go-cache go test ./internal/activities` (from `platform/brain-core`).

Next steps: 1) run IndexArtifact end-to-end against the live store-core/minio stack with the production embedding provider, 2) keep an eye on vector_entries after the next indexing pass.
file update:
diff --git a/platform/brain-core/internal/activities/activities.go b/platform/brain-core/internal/activities/activities.go
index 6c71a0c1df4a0d34126a90d3fc20313f93fddb70..56eadbe0728f5c202f0ffe53898c81cc28fbdca1
--- a/platform/brain-core/internal/activities/activities.go
+++ b/platform/brain-core/internal/activities/activities.go
@@ -1040,10 +1040,11 @@
 			// Use pre-normalized vector record from staging
 			nodeID := asString(vp["nodeId"])
 			text := asString(vp["text"])
+			vpProject := asString(vp["projectKey"])
 			if nodeID != "" && text != "" {
 				entry = vectorstore.Entry{
 					TenantID:       asString(vp["tenantId"]),
-					ProjectID:      projectID,
+					ProjectID:      vpProject,
 					ProfileID:      asString(vp["profileId"]),
 					NodeID:         nodeID,
 					SourceFamily:   asString(vp["sourceFamily"]),
@@ -1055,6 +1056,15 @@
 				if meta, ok := vp["metadata"].(map[string]any); ok {
 					entry.Metadata = meta
 				}
+				if entry.TenantID == "" {
+					entry.TenantID = tenantID
+				}
+				if entry.ProjectID == "" {
+					entry.ProjectID = projectID
+				}
+				if entry.SourceFamily == "" {
+					entry.SourceFamily = req.SourceFamily
+				}
 				content = text
 				ok = true
 			}
diff --git a/platform/brain-core/internal/activities/activities_test.go b/platform/brain-core/internal/activities/activities_test.go
index 82a4e856e1d2588a9814fb755df5acf9253017a4..9bd44f2e2436b24b8aab842302ca0002a423d5dc
--- a/platform/brain-core/internal/activities/activities_test.go
+++ b/platform/brain-core/internal/activities/activities_test.go
@@ -6,6 +6,10 @@
 	"testing"
 
 	"github.com/nucleus/ucl-core/pkg/endpoint"
+	"github.com/nucleus/ucl-core/pkg/orchestration"
+	"github.com/nucleus/ucl-core/pkg/staging"
+	"github.com/nucleus/ucl-core/pkg/vectorprofile"
+	"github.com/nucleus/ucl-core/pkg/vectorstore"
 )
 
 // =============================================================================
@@ -251,3 +255,115 @@
 		}
 	})
 }
+
+func TestStreamFromStagingIncludesVectorPayload(t *testing.T) {
+	ctx := context.Background()
+	reg := orchestration.DefaultStagingRegistry()
+	provider, ok := reg.Get(staging.ProviderMemory)
+	if !ok || provider == nil {
+		t.Fatalf("memory staging provider not registered")
+	}
+
+	vectorPayload := map[string]any{
+		"nodeId":       "node-1",
+		"profileId":    "source.github.code.v1",
+		"entityKind":   "code.file_chunk",
+		"text":         "chunk content",
+		"tenantId":     "tenant-1",
+		"projectKey":   "org/repo",
+		"sourceFamily": "github",
+		"metadata": map[string]any{
+			"path": "README.md",
+		},
+	}
+	rawRecord := map[string]any{
+		"repo":       "org/repo",
+		"path":       "README.md",
+		"sha":        "abc123",
+		"text":       "chunk content",
+		"chunkIndex": 0,
+	}
+	res, err := provider.PutBatch(ctx, &staging.PutBatchRequest{
+		Records: []staging.RecordEnvelope{{
+			RecordKind:    "raw",
+			EntityKind:    "raw.code.file_chunk",
+			Payload:       map[string]any{"payload": rawRecord},
+			VectorPayload: vectorPayload,
+		}},
+	})
+	if err != nil {
+		t.Fatalf("put batch: %v", err)
+	}
+
+	iter, closeFn, err := streamFromStaging(ctx, provider.ID(), res.StageRef, "", "raw.code.file_chunk", nil, 0)
+	if err != nil {
+		t.Fatalf("streamFromStaging: %v", err)
+	}
+	defer closeFn()
+
+	if !iter.Next() {
+		t.Fatalf("expected staged record")
+	}
+	rec := iter.Value()
+	vp, ok := rec["vectorPayload"].(map[string]any)
+	if !ok || vp == nil {
+		t.Fatalf("expected vectorPayload in record")
+	}
+	if vp["nodeId"] != vectorPayload["nodeId"] {
+		t.Fatalf("expected nodeId %v, got %v", vectorPayload["nodeId"], vp["nodeId"])
+	}
+	if _, ok := rec["rawPayload"]; !ok {
+		t.Fatalf("expected rawPayload to be preserved")
+	}
+	if iter.Err() != nil {
+		t.Fatalf("unexpected iterator error: %v", iter.Err())
+	}
+}
+
+func TestNormalizeVectorRecordUnwrapsNestedPayload(t *testing.T) {
+	const profileID = "test.unwrap"
+	vectorprofile.Register(profileID, &testUnwrapNormalizer{})
+
+	rec := map[string]any{
+		"payload": map[string]any{
+			"payload": map[string]any{
+				"text": "hello",
+			},
+		},
+		"rawPayload": map[string]any{
+			"text": "hello",
+		},
+	}
+
+	entry, content, ok := normalizeVectorRecord(rec, profileID, "tenant-x", "project-x", "raw.code.file_chunk", "sink")
+	if !ok {
+		t.Fatalf("expected normalization to succeed")
+	}
+	if content != "hello" {
+		t.Fatalf("expected content 'hello', got %s", content)
+	}
+	if entry.TenantID != "tenant-x" {
+		t.Fatalf("expected tenant fallback, got %s", entry.TenantID)
+	}
+	if entry.ProjectID != "project-x" {
+		t.Fatalf("expected project fallback, got %s", entry.ProjectID)
+	}
+	if entry.NodeID != "node-hello" {
+		t.Fatalf("expected nodeId 'node-hello', got %s", entry.NodeID)
+	}
+}
+
+type testUnwrapNormalizer struct{}
+
+func (n *testUnwrapNormalizer) Normalize(record map[string]any) (vectorstore.Entry, string, bool) {
+	payload, _ := record["payload"].(map[string]any)
+	text, _ := payload["text"].(string)
+	if text == "" {
+		return vectorstore.Entry{}, "", false
+	}
+	return vectorstore.Entry{
+		ProfileID:   "test.unwrap",
+		NodeID:      "node-" + text,
+		ContentText: text,
+	}, text, true
+}
diff --git a/platform/brain-core/internal/activities/kv_client.go b/platform/brain-core/internal/activities/kv_client.go
index 2868190648ebf3ab73a7d28c18a89cbea3864795..2677392e8a9545b875114bcce24dda834d2a1714
--- a/platform/brain-core/internal/activities/kv_client.go
+++ b/platform/brain-core/internal/activities/kv_client.go
@@ -26,7 +26,13 @@
 			addr = os.Getenv("UCL_GRPC_ADDR")
 		}
 		if addr == "" {
-			addr = "localhost:50051"
+			addr = os.Getenv("VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = os.Getenv("STORE_VECTOR_GRPC_ADDR")
+		}
+		if addr == "" {
+			addr = "localhost:9099"
 		}
 		conn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(5*time.Second))
 		if err != nil {
diff --git a/platform/brain-core/internal/activities/normalize.go b/platform/brain-core/internal/activities/normalize.go
index 9e368ba2b48982589479541f2504d8b56f267d8f..08ac51753ad1d7988cf469dc54e1b9f83a31a09c
--- a/platform/brain-core/internal/activities/normalize.go
+++ b/platform/brain-core/internal/activities/normalize.go
@@ -11,8 +11,32 @@
 // normalizeVectorRecord builds a vector entry and content text from a raw record map based on profile.
 // Returns (entry, content, ok).
 func normalizeVectorRecord(rec map[string]any, profileID, tenantID, projectID, datasetSlug, sinkEndpointID string) (vectorstore.Entry, string, bool) {
+	// Unwrap nested payloads when staging envelopes wrap the original record under payload.payload.
+	normRec := rec
+	if payload, ok := rec["payload"].(map[string]any); ok {
+		if inner, ok := payload["payload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = inner
+			normRec = clone
+		}
+	}
+
 	n := vectorprofile.Resolve(profileID)
-	entry, content, ok := n.Normalize(rec)
+	entry, content, ok := n.Normalize(normRec)
+	if !ok {
+		// Fallback to rawPayload (pre-mapped) when available.
+		if raw, ok := rec["rawPayload"].(map[string]any); ok {
+			clone := make(map[string]any, len(rec))
+			for k, v := range rec {
+				clone[k] = v
+			}
+			clone["payload"] = raw
+			entry, content, ok = n.Normalize(clone)
+		}
+	}
 	if !ok {
 		return vectorstore.Entry{}, "", false
 	}
diff --git a/platform/brain-core/internal/activities/stream.go b/platform/brain-core/internal/activities/stream.go
index c9f71841968672ea48ef14da90d2f10821bc9962..8ef359bc5acb56d4964366103fade2602527fac8
--- a/platform/brain-core/internal/activities/stream.go
+++ b/platform/brain-core/internal/activities/stream.go
@@ -133,6 +133,7 @@
 			recordKind := env.RecordKind
 			entityKind := env.EntityKind
 			payload := env.Payload
+			rawPayload := env.Payload
 			mapperKey := entityKind
 			if mapperKey == "" {
 				mapperKey = it.datasetID
@@ -149,18 +150,20 @@
 				}
 			}
 			it.current = map[string]any{
-				"recordKind":   recordKind,
-				"entityKind":   entityKind,
-				"payload":      payload,
-				"source":       env.Source,
-				"tenantId":     env.TenantID,
-				"projectKey":   env.ProjectKey,
-				"observedAt":   env.ObservedAt,
-				"stageRef":     it.stageRef,
-				"sliceId":      it.sliceID,
-				"batchRef":     it.batchRefs[it.batchIdx-1],
-				"recordOffset": offset,
-				"mapperKey":    mapperKey,
+				"recordKind":    recordKind,
+				"entityKind":    entityKind,
+				"payload":       payload,
+				"rawPayload":    rawPayload,
+				"vectorPayload": env.VectorPayload,
+				"source":        env.Source,
+				"tenantId":      env.TenantID,
+				"projectKey":    env.ProjectKey,
+				"observedAt":    env.ObservedAt,
+				"stageRef":      it.stageRef,
+				"sliceId":       it.sliceID,
+				"batchRef":      it.batchRefs[it.batchIdx-1],
+				"recordOffset":  offset,
+				"mapperKey":     mapperKey,
 			}
 			return true
 		}
diff --git a/runs/code-vector-profile-and-indexing-v1/DECISIONS.md b/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
index 29c1ccbf79cbaa94d2475978d1fdb5b8fec28620..2cca30268af052a903fa1c11ac33ee194f6fd081
--- a/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
+++ b/runs/code-vector-profile-and-indexing-v1/DECISIONS.md
@@ -2,3 +2,4 @@
 - Canonical code profile `code.github.v1` normalizes projectKey to lower-case `{owner}/{repo}`, entityKind=`code.file_chunk`, profileKind=`code`, sourceFamily=`github`, docId=`code:github:{tenantId}:{projectKey}:{path}:{sha}:{chunkIndex}`. Content comes from `payload.text` truncated to 20k chars; attributes capture path/sha/chunkIndex/language/truncated flag. Missing tenantId/projectKey/profileKind fail closed with E_MISSING_CANONICAL_KEYS. Profile is also seeded into `vector_index_profiles` with text_source field hints.
 - Deterministic embeddings use a fake provider that hashes text to a one-hot 1536-dim vector (no external calls) for tests and default offline runs.
 - Index runs read MinIO-style JSONL.GZ envelopes from a dataset prefix (supports `minio://bucket/prefix`) via filesystem, streaming/decompressing lines; optional `ingestionRunId` filters to `run=<id>` segments (basePrefix defaults to `sink`). Errors classify as E_SOURCE_READ_FAILED, E_INVALID_RECORD, E_MISSING_CANONICAL_KEYS, E_EMBEDDING_FAILED, or E_VECTOR_DB_FAILED. Oversized text is truncated; malformed/missing-key records are skipped and surfaced as errors, with run status set to SUCCEEDED_WITH_ERRORS when any non-fatal errors occur.
+- Staging replay now surfaces `vectorPayload` and `rawPayload` to IndexArtifact; normalization unwraps nested payloads so VectorProfile normalizers see original records, and KV checkpoint client defaults to VECTOR_GRPC_ADDR/9099 (falling back from KV/UCL) to avoid checkpoint timeouts when store-core runs on 9099.
diff --git a/runs/code-vector-profile-and-indexing-v1/TODO.md b/runs/code-vector-profile-and-indexing-v1/TODO.md
index 0bada439bd1de49e1a0913caf5f994a74d225413..c08b43dcc387e3fa061794cd5c7cfabb63562350
--- a/runs/code-vector-profile-and-indexing-v1/TODO.md
+++ b/runs/code-vector-profile-and-indexing-v1/TODO.md
@@ -1,5 +1,5 @@
 - [x] Implement Endpoint-Driven Vector Normalization (VectorProfileProvider, GitHub connector, Staging integration).
 - [x] Verify Staging (vectorPayload IS present in MinIO).
-- [ ] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
+- [x] DEBUG: Fix issue where `IndexArtifact` completes but `vector_entries` table is empty. Logs show `checkpoint-save-failed ... err context deadline exceeded`.
 - [ ] Rerun code indexing with the production embedding provider once access is available.
 - [ ] Feed materialized_artifacts registry from sink completion events.
diff --git a/stories/code-vector-profile-and-indexing-v1/STORY.md b/stories/code-vector-profile-and-indexing-v1/STORY.md
index 83311a64dc7bd20787734b76ff509d51483b20bd..a62438295091ba6b53d561e5e97616b4627f29bc
--- a/stories/code-vector-profile-and-indexing-v1/STORY.md
+++ b/stories/code-vector-profile-and-indexing-v1/STORY.md
@@ -2,3 +2,4 @@
 
 - 2025-12-13: Extended pgvector-backed `vector_index_entries` with docId/source metadata and seeded `code.github.v1`, implemented the code index runner/normalizer with deterministic embeddings, MinIO JSONL.GZ reading, and ingestionRunId filtering, and wrote AC1–AC4 tests; brain test suite currently blocked locally by missing METADATA_DATABASE_URL/Postgres.
 - 2025-12-14: Pivoted to a single vector store (removed vector_documents), added in-memory profile/vector stores for offline runs, fixed the code index runner API, reran `pnpm --filter @apps/metadata-api test:brain` successfully with deterministic embeddings, applied migration `20251213195500_code_vector_documents` to the local metadata pgvector instance, ran a real GitHub→MinIO ingestion (octocat/Hello-World) with CodeIndexRunner persisting vectors to `metadata.vector_index_entries` (deterministic embeddings), and added a `materialized_artifacts` registry + registry-aware CodeIndexRunner to track index runs (registry not yet fed by ingestion events).
+- 2025-12-22: Unblocked IndexArtifact by replaying staged envelopes with `vectorPayload`/`rawPayload`, unwrapping nested payloads for VectorProfile normalizers, defaulting KV checkpoints to store-core on 9099, and adding Go tests for staging replay + normalization (GOCACHE=/tmp/go-cache go test ./internal/activities).
diff --git a/sync/STATE.md b/sync/STATE.md
index 36057abb485d5b149593770a6f9daad8c6705681..425bda9090e00f0426384cefeca60e1179cda01c
--- a/sync/STATE.md
+++ b/sync/STATE.md
@@ -1,16 +1,16 @@
 # STATE SYNC (auto-updated)
 
 ## Focus Feature
-materialized-registry-and-index-trigger-v1 (success @ 2025-12-14T12:59Z)
+code-vector-profile-and-indexing-v1 (in-progress @ 2025-12-22T14:06Z)
 
 ## Last Run
-- slug: materialized-registry-and-index-trigger-v1
-- status: success
-- duration: ~0.9h
-- tests: pnpm --filter @apps/metadata-api test:brain (green)
+- slug: code-vector-profile-and-indexing-v1
+- status: in-progress
+- duration: ~0.8h
+- tests: GOCACHE=/tmp/go-cache go test ./internal/activities (green)
 - commits: none
-- decisions: 2
-- next_step: apply the materialized_artifacts migration/client to environments and extend profile mappings as new source families adopt the registry
+- decisions: 1
+- next_step: validate IndexArtifact against store-core/minio with production embeddings and rerun indexing if needed
 
 ## Global Queue
 TODAY:
@@ -20,6 +20,7 @@
 LATER:
 - 
 
+- 2025-12-22T14:06Z run in-progress (code-vector-profile-and-indexing-v1, staging replay exposes vectorPayload/rawPayload; go activities tests green)
 - 2025-12-14T12:59Z run success (materialized-registry-and-index-trigger-v1, registry/index trigger v1 landed; test:brain green)
 - 2025-12-14T12:48Z run success (materialized-registry-and-index-trigger-v1, registry/index trigger v1 landed; test:brain green)
 - 2025-12-14T07:02Z run in-progress (code-vector-profile-and-indexing-v1, materialized_artifacts registry + registry-aware CodeIndexRunner; test:brain still green)

tokens used
2,79,562
Staging replay now preserves the pre-normalized vector payload and unwraps nested payloads so IndexArtifact can actually upsert vectors; KV checkpoint client defaults to the store-core address to avoid timeouts, and tests cover the new flow.

- `platform/brain-core/internal/activities/stream.go`: staging iterator emits `vectorPayload` and `rawPayload` for each record so the indexer can use pre-normalized vectors.
- `platform/brain-core/internal/activities/normalize.go`: unwraps nested `payload.payload` shapes and falls back to `rawPayload`, keeping tenant/project/profile fallbacks intact.
- `platform/brain-core/internal/activities/activities.go`: vectorPayload path now applies project/tenant/source fallbacks for canonical metadata.
- `platform/brain-core/internal/activities/kv_client.go`: KV client now falls back to VECTOR_GRPC_ADDR/STORE_VECTOR_GRPC_ADDR/9099 to match store-core, preventing checkpoint save deadlines.
- `platform/brain-core/internal/activities/activities_test.go`: added coverage for staged vectorPayload replay and normalization unwrapping.

Tests: `GOCACHE=/tmp/go-cache go test ./internal/activities` (from `platform/brain-core`).

Next steps: 1) run IndexArtifact end-to-end against the live store-core/minio stack with the production embedding provider, 2) keep an eye on vector_entries after the next indexing pass.
